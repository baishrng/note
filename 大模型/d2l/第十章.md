# 注意力提示

## 注意力的可视化

平均汇聚层可以被视为输入的加权平均值，其中各输入的权重是一样的。实际上，注意力汇聚得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的。

```python
import torch
from d2l import torch as d2l
```

为了可视化注意力权重，需要定义一个`show_heatmaps`函数。其输入`matrices`的形状是（要显示的行数，要显示的列数，查询的数目，键的数目）。

```python
#@save
def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
                  cmap='Reds'):
    """显示矩阵热图"""
    d2l.use_svg_display()  # 使用SVG格式显示图像
    num_rows, num_cols = matrices.shape[0], matrices.shape[1]
    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
                                 sharex=True, sharey=True, squeeze=False)
    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)  # 绘制热图
            if i == num_rows - 1:
                ax.set_xlabel(xlabel)  # 设置x轴标签
            if j == 0:
                ax.set_ylabel(ylabel)  # 设置y轴标签
            if titles:
                ax.set_title(titles[j])  # 设置子图标题
    fig.colorbar(pcm, ax=axes, shrink=0.6);  # 添加颜色条
```

下面使用一个简单的例子进行演示。在本例子中，仅当查询和键相同时，注意力权重为1，否则为0。

```python
attention_weights = torch.eye(10).reshape((1, 1, 10, 10))  # 创建一个10x10的单位矩阵，并将其形状调整为(1, 1, 10, 10)
show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')  # 显示注意力权重的热图，横坐标为Keys，纵坐标为Queries
```

# 注意力汇聚：Nadaraya-Watson 核回归
Nadaraya-Watson核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。

```python
import torch
from torch import nn
from d2l import torch as d2l
```

## **生成数据集**

根据下面的非线性函数生成一个人工数据集，其中加入的噪声项为$\epsilon$：

$$y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon,$$

其中$\epsilon$服从均值为$0$和标准差为$0.5$的正态分布。在这里生成了$50$个训练样本和$50$个测试样本。为了更好地可视化之后的注意力模式，需要将训练样本进行排序。

```python
n_train = 50  # 训练样本数
x_train, _ = torch.sort(torch.rand(n_train) * 5)   # 排序后的训练样本
```

```python
def f(x):
    return 2 * torch.sin(x) + x**0.8

y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # 训练样本的输出
x_test = torch.arange(0, 5, 0.1)  # 测试样本
y_truth = f(x_test)  # 测试样本的真实输出
n_test = len(x_test)  # 测试样本数
n_test
```

下面的函数将绘制所有的训练样本（样本由圆圈表示），不带噪声项的真实数据生成函数$f$（标记为“Truth”），以及学习得到的预测函数（标记为“Pred”）。

```python
def plot_kernel_reg(y_hat):
    d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'Pred'],
             xlim=[0, 5], ylim=[-1, 5])  # 绘制测试数据的真实值和预测值的图像，横坐标为x，纵坐标为y，包含图例Truth和Pred，横坐标范围[0, 5]，纵坐标范围[-1, 5]
    d2l.plt.plot(x_train, y_train, 'o', alpha=0.5);  # 绘制训练数据的散点图，透明度为0.5
```

## 平均汇聚

先使用最简单的估计器来解决回归问题。基于平均汇聚来计算所有训练样本输出值的平均值：

$$f(x) = \frac{1}{n}\sum_{i=1}^n y_i,$$

如下图所示，这个估计器确实不够聪明。真实函数$f$（“Truth”）和预测函数（“Pred”）相差很大。

```python
y_hat = torch.repeat_interleave(y_train.mean(), n_test)  # 使用训练数据标签的平均值来生成预测值
plot_kernel_reg(y_hat)  # 调用plot_kernel_reg函数，绘制使用平均预测值的核回归图
```

## **非参数注意力汇聚**
值得注意的是，Nadaraya-Watson核回归是一个非参数模型。接下来，我们将基于这个非参数的注意力汇聚模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。

```python
# X_repeat的形状:(n_test,n_train),
# 每一行都包含着相同的测试输入（例如：同样的查询）
X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))
# x_train包含着键。attention_weights的形状：(n_test,n_train),
# 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重
attention_weights = nn.functional.softmax(-(X_repeat - x_train)**2 / 2, dim=1)
# y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重
y_hat = torch.matmul(attention_weights, y_train)
plot_kernel_reg(y_hat)
```

这里测试数据的输入相当于查询，而训练数据的输入相当于键。因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近，注意力汇聚的[**注意力权重**]就越高。

```python
d2l.show_heatmaps(attention_weights.unsqueeze(0).unsqueeze(0),
                  xlabel='Sorted training inputs',  # 设置横坐标标签为排序后的训练输入
                  ylabel='Sorted testing inputs')  # 设置纵坐标标签为排序后的测试输入
```

## **带参数注意力汇聚**
### 批量矩阵乘法
为了更有效地计算小批量数据的注意力，我们可以利用深度学习开发框架中提供的批量矩阵乘法。假设第一个小批量数据包含$n$个矩阵$\mathbf{X}_1,\ldots, \mathbf{X}_n$，形状为$a\times b$，第二个小批量包含$n$个矩阵$\mathbf{Y}_1, \ldots, \mathbf{Y}_n$，形状为$b\times c$。它们的批量矩阵乘法得到$n$个矩阵$\mathbf{X}_1\mathbf{Y}_1, \ldots, \mathbf{X}_n\mathbf{Y}_n$，形状为$a\times c$。因此，[**假定两个张量的形状分别是$(n,a,b)$和$(n,b,c)$，它们的批量矩阵乘法输出的形状为$(n,a,c)$**]。

```python
X = torch.ones((2, 1, 4))  # 创建一个形状为(2, 1, 4)的张量X，元素都为1
Y = torch.ones((2, 4, 6))  # 创建一个形状为(2, 4, 6)的张量Y，元素都为1
torch.bmm(X, Y).shape  # 对X和Y进行批量矩阵乘法，返回结果的形状
```

在注意力机制的背景中，我们可以[**使用小批量矩阵乘法来计算小批量数据中的加权平均值**]。

```python
weights = torch.ones((2, 10)) * 0.1  # 创建一个形状为(2, 10)的权重张量，每个元素为0.1
values = torch.arange(20.0).reshape((2, 10))  # 创建一个形状为(2, 10)的值张量，从0到19
torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))  # 对权重张量和值张量进行批量矩阵乘法
```

### 定义模型
使用小批量矩阵乘法，定义Nadaraya-Watson核回归的带参数版本为：

```python
class NWKernelRegression(nn.Module):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))
        # 初始化一个可学习的参数w

    def forward(self, queries, keys, values):
        # queries和attention_weights的形状为(查询个数，“键－值”对个数)
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))
        self.attention_weights = nn.functional.softmax(
            -((queries - keys) * self.w)**2 / 2, dim=1)
        # 计算注意力权重，使用softmax将加权后的结果转换为注意力权重

        # values的形状为(查询个数，“键－值”对个数)
        return torch.bmm(self.attention_weights.unsqueeze(1),
                         values.unsqueeze(-1)).reshape(-1)
        # 返回加权后的values，形状为(查询个数,)
```

### 训练

接下来，[**将训练数据集变换为键和值**]用于训练注意力模型。在带参数的注意力汇聚模型中，任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算，从而得到其对应的预测输出。

```python
# X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入
X_tile = x_train.repeat((n_train, 1))

# Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出
Y_tile = y_train.repeat((n_train, 1))

# keys的形状:(n_train，n_train-1)，创建一个矩阵，每一行是X_tile的一个副本，但不包括对角线上的元素
keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))

# values的形状:(n_train，n_train-1)，创建一个矩阵，每一行是Y_tile的一个副本，但不包括对角线上的元素
values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))
```

[**训练带参数的注意力汇聚模型**]时，使用平方损失函数和随机梯度下降。

```python
net = NWKernelRegression()  # 创建一个NWKernelRegression模型实例

loss = nn.MSELoss(reduction='none')  # 定义平方误差损失函数，不进行降维

trainer = torch.optim.SGD(net.parameters(), lr=0.5)  # 使用随机梯度下降优化器，学习率为0.5

animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])  # 设置动画器用于实时可视化训练过程中的损失变化，横轴为epoch，纵轴为loss，x轴范围为1到5

for epoch in range(5):
    trainer.zero_grad()  # 清除优化器梯度
    l = loss(net(x_train, keys, values), y_train)  # 计算模型在训练数据上的预测值并计算损失
    l.sum().backward()  # 反向传播，计算梯度
    trainer.step()  # 根据梯度更新模型参数
    print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')  # 打印当前epoch的损失值
    animator.add(epoch + 1, float(l.sum()))  # 将当前epoch的损失值添加到动画器中，用于实时可视化
```

如下所示，训练完带参数的注意力汇聚模型后可以发现：在尝试拟合带噪声的训练数据时，[**预测结果绘制**]的线不如之前非参数模型的平滑。

```python
keys = x_train.repeat((n_test, 1))  # keys的形状:(n_test，n_train)，每一行包含相同的训练输入（例如，相同的键）

values = y_train.repeat((n_test, 1))  # values的形状:(n_test，n_train)，每一行包含相同的训练输出（例如，相同的值）

y_hat = net(x_test, keys, values).unsqueeze(1).detach()  # 对测试数据进行预测，将维度调整为(n_test, 1)，并且断开梯度追踪

plot_kernel_reg(y_hat)  # 绘制核回归的预测结果
```

为什么新的模型更不平滑了呢？下面看一下输出结果的绘制图：与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，[**曲线在注意力权重较大的区域变得更不平滑**]。

```python
d2l.show_heatmaps(net.attention_weights.unsqueeze(0).unsqueeze(0),  # 展示注意力权重的热图，unsqueeze是为了适应函数要求的输入维度
                  xlabel='Sorted training inputs',  # 设置横轴标签为“排序后的训练输入”
                  ylabel='Sorted testing inputs')  # 设置纵轴标签为“排序后的测试输入”
```

# 注意力评分函数
本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。

```python
import math  # 导入数学库，用于数学运算
import torch  # 导入PyTorch库，用于张量操作和深度学习模型构建
from torch import nn  # 从PyTorch中导入神经网络模块，用于定义神经网络层和损失函数
from d2l import torch as d2l  # 从d2l.torch模块导入d2l库，这是Dive into Deep Learning书籍的PyTorch实现
```

## [**掩蔽softmax操作**]

正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度（即词元的个数），以便在计算softmax时过滤掉超出指定范围的位置。下面的`masked_softmax`函数实现了这样的*掩蔽softmax操作*，其中任何超出有效长度的位置都被掩蔽并置为0。

```python
#@save
def masked_softmax(X, valid_lens):
    """通过在最后一个轴上掩蔽元素来执行softmax操作"""
    # X: 3D张量，valid_lens: 1D或2D张量
    
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)  # 如果没有有效长度，直接使用标准softmax
    
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])  # 将有效长度广播到与X相同的形状
        else:
            valid_lens = valid_lens.reshape(-1)  # 将有效长度张量展平为1维
        
        # 在最后一轴上掩蔽元素，将不需要参与softmax计算的位置设置为一个很大的负数
        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,
                              value=-1e6)
        
        return nn.functional.softmax(X.reshape(shape), dim=-1)  # 对调整后的张量进行softmax操作，保持形状不变

```

为了[**演示此函数是如何工作**]的，考虑由两个$2 \times 4$矩阵表示的样本，这两个样本的有效长度分别为$2$和$3$。经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。

```python
masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))  # 对一个形状为 (2, 2, 4) 的随机张量执行 masked_softmax 操作，有效长度为 [2, 3]
```

同样，也可以使用二维张量，为矩阵样本中的每一行指定有效长度。

```python
masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))  # 对一个形状为 (2, 2, 4) 的随机张量执行 masked_softmax 操作，有效长度张量为二维张量 [[1, 3], [2, 4]]

```

## [**加性注意力**]

一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。给定查询$\mathbf{q} \in \mathbb{R}^q$和键$\mathbf{k} \in \mathbb{R}^k$，*加性注意力*（additive attention）的评分函数为

$$a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},$$
		其中可学习的参数是$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$和$\mathbf w_v\in\mathbb R^{h}$。如 :eqref:`eq_additive-attn`所示，将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$。
通过使用$\tanh$作为激活函数，并且禁用偏置项。下面来实现加性注意力。

```python
#@save
class AdditiveAttention(nn.Module):
    """加性注意力模块"""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)  # 线性变换W_k
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)  # 线性变换W_q
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)  # 线性变换w_v，输出一个值
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
         # 在维度扩展后，
        # queries的形状：(batch_size，查询的个数，1，num_hidden)
        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)
        # 使用广播方式进行求和
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。
        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)
        scores = self.w_v(features).squeeze(-1)
        # scores的形状：(batch_size, 查询个数, "键-值"对个数)
        
        self.attention_weights = masked_softmax(scores, valid_lens)
        # 使用掩码softmax计算注意力权重
        # attention_weights的形状：(batch_size, 查询个数, "键-值"对个数)
        
        # values的形状：(batch_size，“键－值”对的个数，值的维度)
        return torch.bmm(self.dropout(self.attention_weights), values)
        # 返回加权后的values
```

用一个小例子来[**演示上面的`AdditiveAttention`类**]，其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小），实际输出为$(2,1,20)$、$(2,10,2)$和$(2,10,4)$。注意力汇聚输出的形状为（批量大小，查询的步数，值的维度）。

```python
queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))
# queries的形状：(2, 1, 20)，两个样本，每个样本一个查询，每个查询有20个特征
# keys的形状：(2, 10, 2)，两个样本，每个样本有10个键，每个键有2个特征

# values的小批量，两个值矩阵是相同的
values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(
    2, 1, 1)
# values的形状：(2, 10, 4)，两个样本，每个样本有10个值，每个值有4个特征

valid_lens = torch.tensor([2, 6])
# 有效长度：两个样本中分别有2和6个有效的键-值对

attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,
                              dropout=0.1)
# 创建加性注意力模型实例，key_size=2，query_size=20，隐藏单元数为8，dropout率为0.1

attention.eval()
# 将模型切换为评估模式，这意味着在推断阶段不会执行 Dropout 操作

attention(queries, keys, values, valid_lens)
# 调用加性注意力模型的forward方法，传入queries, keys, values, 和 valid_lens作为参数
# 返回加权后的values，形状为(2, 1, 4)，表示两个样本中每个样本的一个加权后的值向量，每个值向量有4个特征
```

尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的，所以[**注意力权重**]是均匀的，由指定的有效长度决定。

```python
d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
# 调用d2l.show_heatmaps函数显示注意力权重的热图，重塑注意力权重张量的形状为(1, 1, 2, 10)
# xlabel设置为'Keys'，ylabel设置为'Queries'
```

## **缩放点积注意力**
下面的缩放点积注意力的实现使用了暂退法进行模型正则化。

```python
#@save
class DotProductAttention(nn.Module):
    """缩放点积注意力"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # queries的形状：(batch_size，查询的个数，d)
    # keys的形状：(batch_size，“键－值”对的个数，d)
    # values的形状：(batch_size，“键－值”对的个数，值的维度)
    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]  # 获取查询向量的维度
        # 设置transpose_b=True为了交换keys的最后两个维度
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)  # 计算未缩放的注意力分数
        self.attention_weights = masked_softmax(scores, valid_lens)  # 计算注意力权重并保留在self.attention_weights中
        return torch.bmm(self.dropout(self.attention_weights), values)  # 使用注意力权重加权求和values并返回
```

为了[**演示上述的`DotProductAttention`类**]，我们使用与先前加性注意力例子中相同的键、值和有效长度。对于点积操作，我们令查询的特征维度与键的特征维度大小相同。

```python
queries = torch.normal(0, 1, (2, 1, 2))  # 创建正态分布的查询张量，形状为(2, 1, 2)

attention = DotProductAttention(dropout=0.5)  # 创建缩放点积注意力对象，设定dropout率为0.5
attention.eval()  # 切换模型为评估模式，关闭dropout

# 调用注意力对象进行前向传播
attention(queries, keys, values, valid_lens)
```

与加性注意力演示相同，由于键包含的是相同的元素，而这些元素无法通过任何查询进行区分，因此获得了[**均匀的注意力权重**]。

```python
d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')  # 显示注意力权重的热图，调整形状为(1, 1, 2, 10)，横轴标签为Keys，纵轴标签为Queries
```

# Bahdanau 注意力

```python
import torch
from torch import nn
from d2l import torch as d2l
```

## 定义注意力解码器

下面看看如何定义Bahdanau注意力，实现循环神经网络编码器-解码器。其实，我们只需重新定义解码器即可。为了更方便地显示学习的注意力权重，以下`AttentionDecoder`类定义了[**带有注意力机制解码器的基本接口**]。

```python
#@save
class AttentionDecoder(d2l.Decoder):
    """带有注意力机制解码器的基本接口"""
    def __init__(self, **kwargs):
        super(AttentionDecoder, self).__init__(**kwargs)

    @property
    def attention_weights(self):
        raise NotImplementedError  # 如果子类未实现attention_weights属性，抛出未实现错误
```

接下来，让我们在接下来的`Seq2SeqAttentionDecoder`类中[**实现带有Bahdanau注意力的循环神经网络解码器**]。首先，初始化解码器的状态，需要下面的输入：

1. 编码器在所有时间步的最终层隐状态，将作为注意力的键和值；
1. 上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；
1. 编码器有效长度（排除在注意力池中填充词元）。
在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。

```python
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)
        self.attention = d2l.AdditiveAttention(
            num_hiddens, num_hiddens, num_hiddens, dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(
            embed_size + num_hiddens, num_hiddens, num_layers,
            dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        # outputs的形状为(batch_size，num_steps，num_hiddens).
        # hidden_state的形状为(num_layers，batch_size，num_hiddens)
        outputs, hidden_state = enc_outputs
        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)

    def forward(self, X, state):
        # enc_outputs的形状为(batch_size,num_steps,num_hiddens).
        # hidden_state的形状为(num_layers,batch_size,num_hiddens)
        enc_outputs, hidden_state, enc_valid_lens = state
        # 输出X的形状为(num_steps,batch_size,embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        outputs, self._attention_weights = [], []
        for x in X:
            # query的形状为(batch_size,1,num_hiddens)
            query = torch.unsqueeze(hidden_state[-1], dim=1)
            # context的形状为(batch_size,1,num_hiddens)
            context = self.attention(
                query, enc_outputs, enc_outputs, enc_valid_lens)
            # 在特征维度上连结
            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)
            # 将x变形为(1,batch_size,embed_size+num_hiddens)
            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)
            outputs.append(out)
            self._attention_weights.append(self.attention.attention_weights)
        # 全连接层变换后，outputs的形状为(num_steps,batch_size,vocab_size)
        outputs = self.dense(torch.cat(outputs, dim=0))
        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,
                                          enc_valid_lens]

    @property
    def attention_weights(self):
        return self._attention_weights
```

接下来，使用包含7个时间步的4个序列输入的小批量[**测试Bahdanau注意力解码器**]。

```python
encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
                             num_layers=2)
encoder.eval()  # 设置编码器为评估模式

decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                                  num_layers=2)
decoder.eval()  # 设置解码器为评估模式

X = torch.zeros((4, 7), dtype=torch.long)  # (batch_size,num_steps) 创建一个全零张量作为输入X，形状为(batch_size, num_steps)
state = decoder.init_state(encoder(X), None)  # 初始化解码器的状态，使用编码器的输出作为输入
output, state = decoder(X, state)  # 对解码器进行前向传播计算输出和新状态
output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape
# 输出output的形状，状态state的长度，state中第一个元素的形状，state中第二个元素的长度，state中第二个元素的第一个元素的形状
```

## **训练**

我们在这里指定超参数，实例化一个带有Bahdanau注意力的编码器和解码器，并对这个模型进行机器翻译训练。由于新增的注意力机制，训练要比没有注意力机制的慢得多。

```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1  # 嵌入层大小，隐藏单元个数，层数，dropout概率
batch_size, num_steps = 64, 10  # 批量大小，时间步数

lr, num_epochs, device = 0.005, 250, d2l.try_gpu()  # 学习率，训练轮数，设备选择

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)  # 加载训练数据迭代器，源语言词汇表，目标语言词汇表

encoder = d2l.Seq2SeqEncoder(  # 定义编码器
    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)

decoder = Seq2SeqAttentionDecoder(  # 定义带注意力机制的解码器
    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)

net = d2l.EncoderDecoder(encoder, decoder)  # 定义编码器-解码器模型

d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)  # 训练序列到序列模型
```

模型训练后，我们用它[**将几个英语句子翻译成法语**]并计算它们的BLEU分数。

```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']  # 英文句子列表
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']  # 法文句子列表

for eng, fra in zip(engs, fras):
    translation, dec_attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, True)  # 使用模型进行序列到序列的预测
    print(f'{eng} => {translation}, ',  # 打印英文句子及其翻译结果
          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')  # 计算并打印BLEU分数
```

```python
attention_weights = torch.cat([step[0][0][0] for step in dec_attention_weight_seq], 0).reshape((
    1, 1, -1, num_steps))  # 拼接解码器注意力权重并重塑张量形状
```

训练结束后，下面通过[**可视化注意力权重**]会发现，每个查询都会在键值对上分配不同的权重，这说明在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中。

```python
# 加上一个包含序列结束词元
d2l.show_heatmaps(
    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),  # 显示注意力热图，截取包含序列结束词元的部分
    xlabel='Key positions', ylabel='Query positions')  # 设置x轴和y轴标签
```

# 多头注意力

```python
import math
import torch
from torch import nn
from d2l import torch as d2l
```

## 实现

在实现过程中通常[**选择缩放点积注意力作为每一个注意力头**]。为了避免计算代价和参数代价的大幅增长，我们设定$p_q = p_k = p_v = p_o / h$。值得注意的是，如果将查询、键和值的线性变换的输出数量设置为$p_q h = p_k h = p_v h = p_o$，则可以并行计算$h$个头。在下面的实现中，$p_o$是通过参数`num_hiddens`指定的。

```python
#@save
class MultiHeadAttention(nn.Module):
    """多头注意力"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)  # 线性变换查询向量
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)  # 线性变换键向量
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)  # 线性变换值向量
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)  # 线性变换输出向量

    def forward(self, queries, keys, values, valid_lens):
        # queries，keys，values的形状:
        # (batch_size，查询或者“键－值”对的个数，num_hiddens)
        # valid_lens　的形状:
        # (batch_size，)或(batch_size，查询的个数)
        # 经过变换后，输出的queries，keys，values　的形状:
        # (batch_size*num_heads，查询或者“键－值”对的个数，
        # num_hiddens/num_heads)
        queries = transpose_qkv(self.W_q(queries), self.num_heads)  # 变换并转置查询向量
        keys = transpose_qkv(self.W_k(keys), self.num_heads)  # 变换并转置键向量
        values = transpose_qkv(self.W_v(values), self.num_heads)  # 变换并转置值向量

        if valid_lens is not None:
            # 在轴0，将第一项（标量或者矢量）复制num_heads次，
            # 然后如此复制第二项，然后诸如此类。
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        # output的形状:(batch_size*num_heads，查询的个数，
        # num_hiddens/num_heads)
        output = self.attention(queries, keys, values, valid_lens)

        # output_concat的形状:(batch_size，查询的个数，num_hiddens)
        output_concat = transpose_output(output, self.num_heads)  # 转置并连接输出向量
        return self.W_o(output_concat)  # 线性变换输出连接后的向量
```

为了能够[**使多个头并行计算**]，上面的`MultiHeadAttention`类将使用下面定义的两个转置函数。具体来说，`transpose_output`函数反转了`transpose_qkv`函数的操作。

```python
#@save
def transpose_qkv(X, num_heads):
    """为了多头注意力机制的并行计算而变换形状"""
    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，num_hiddens/num_heads)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,num_hiddens/num_heads)
    X = X.permute(0, 2, 1, 3)

    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,num_hiddens/num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])


#@save
def transpose_output(X, num_heads):
    """逆转transpose_qkv函数的操作"""
    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)
```

下面使用键和值相同的小例子来[**测试**]我们编写的`MultiHeadAttention`类。多头注意力输出的形状是（`batch_size`，`num_queries`，`num_hiddens`）。

```python
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
                               num_hiddens, num_heads, 0.5)
attention.eval()  # 设置注意力模型为评估模式
```

```python
batch_size, num_queries = 2, 4
num_kvpairs, valid_lens =  6, torch.tensor([3, 2])
X = torch.ones((batch_size, num_queries, num_hiddens))  # 创建形状为(batch_size, num_queries, num_hiddens)的张量X，填充为1
Y = torch.ones((batch_size, num_kvpairs, num_hiddens))  # 创建形状为(batch_size, num_kvpairs, num_hiddens)的张量Y，填充为1
attention(X, Y, Y, valid_lens).shape  # 使用注意力机制计算输出张量的形状
```

# 自注意力和位置编码
本节将使用自注意力进行序列编码，以及如何使用序列的顺序作为补充信息。

```python
import math
import torch
from torch import nn
from d2l import torch as d2l
```

## **自注意力**
给定一个由词元组成的输入序列$\mathbf{x}_1, \ldots, \mathbf{x}_n$，其中任意$\mathbf{x}_i \in \mathbb{R}^d$（$1 \leq i \leq n$）。该序列的自注意力输出为一个长度相同的序列$\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中：

$$\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d$$
下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为（批量大小，时间步的数目或词元序列的长度，$d$）。输出与输入的张量形状相同。

```python
num_hiddens, num_heads = 100, 5
# 定义一个多头注意力机制模型，输入和输出维度均为 num_hiddens，使用 num_heads 个注意力头，
# dropout 概率为 0.5
attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
                                   num_hiddens, num_heads, 0.5)
attention.eval()  # 将模型设置为评估模式
```

```python
batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])  # 定义批量大小、查询数量和有效长度
X = torch.ones((batch_size, num_queries, num_hiddens))  # 创建形状为(batch_size, num_queries, num_hiddens)的全1张量X
attention(X, X, X, valid_lens).shape  # 调用attention函数并输出其形状
```

## **位置编码**
在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加*位置编码*（positional encoding）来注入绝对的或相对的位置信息。位置编码可以通过学习得到也可以直接固定得到。接下来描述的是基于正弦函数和余弦函数的固定位置编码

```python
#@save
class PositionalEncoding(nn.Module):
    """位置编码"""

    def __init__(self, num_hiddens, dropout, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        # 创建一个足够长的P
        self.P = torch.zeros((1, max_len, num_hiddens))  # 初始化位置编码矩阵P，形状为(1, max_len, num_hiddens)
        X = torch.arange(max_len, dtype=torch.float32).reshape(
            -1, 1) / torch.pow(10000, torch.arange(
            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)  # 计算位置编码公式中的X
        self.P[:, :, 0::2] = torch.sin(X)  # 将sin部分填充到P的偶数列
        self.P[:, :, 1::2] = torch.cos(X)  # 将cos部分填充到P的奇数列

    def forward(self, X):
        X = X + self.P[:, :X.shape[1], :].to(X.device)  # 将位置编码矩阵P加到输入张量X上
        return self.dropout(X)  # 对加完位置编码后的张量X进行dropout操作
```

在位置嵌入矩阵$\mathbf{P}$中，[**行代表词元在序列中的位置，列代表位置编码的不同维度**]。从下面的例子中可以看到位置嵌入矩阵的第$6$列和第$7$列的频率高于第$8$列和第$9$列。第$6$列和第$7$列之间的偏移量（第$8$列和第$9$列相同）是由于正弦函数和余弦函数的交替。

```python
encoding_dim, num_steps = 32, 60  # 设置编码维度为32，步数为60

# 创建一个PositionalEncoding实例
pos_encoding = PositionalEncoding(encoding_dim, 0)

# 将PositionalEncoding设置为评估模式（禁用dropout）
pos_encoding.eval()

# 创建一个形状为(1, num_steps, encoding_dim)的全零张量，并应用位置编码
X = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))

# 提取位置编码矩阵P，长度为X的长度（num_steps）
P = pos_encoding.P[:, :X.shape[1], :]

# 绘制选定列的位置编码值
d2l.plot(torch.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',
         figsize=(6, 2.5), legend=["Col %d" % d for d in torch.arange(6, 10)])
```

### 绝对位置信息

为了明白沿着编码维度单调降低的频率与绝对位置信息的关系，让我们打印出$0, 1, \ldots, 7$的[**二进制表示**]形式。正如所看到的，每个数字、每两个数字和每四个数字上的比特值在第一个最低位、第二个最低位和第三个最低位上分别交替。

```python
for i in range(8):
    print(f'{i}的二进制是：{i:>03b}')  # 打印每个数字的二进制表示，使用三位填充并右对齐
```

在二进制表示中，较高比特位的交替频率低于较低比特位，与下面的热图所示相似，只是位置编码通过使用三角函数[**在编码维度上降低频率**]。由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。

```python
P = P[0, :, :].unsqueeze(0).unsqueeze(0)  # 从P中选择第一个通道的数据，并添加两个维度使其成为四维张量
d2l.show_heatmaps(P, xlabel='Column (encoding dimension)',
                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')  # 显示热图，横轴表示编码维度，纵轴表示位置，颜色映射为蓝色调色板
```

### 相对位置信息

除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息。这是因为对于任何确定的位置偏移$\delta$，位置$i + \delta$处的位置编码可以线性投影位置$i$处的位置编码来表示。

# Transformer
在此之前已经描述并实现了基于缩放点积多头注意力和位置编码。接下来将实现Transformer模型的剩余部分。

```python
import math
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l
```

## **基于位置的前馈网络**

基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是*基于位置的*（positionwise）的原因。在下面的实现中，输入`X`的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，`ffn_num_outputs`）的输出张量。

```python
#@save
class PositionWiseFFN(nn.Module):
    """基于位置的前馈网络"""

    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)  # 第一个全连接层，输入维度为ffn_num_input，输出维度为ffn_num_hiddens
        self.relu = nn.ReLU()  # 激活函数ReLU
        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)  # 第二个全连接层，输入维度为ffn_num_hiddens，输出维度为ffn_num_outputs

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))  # 前向传播函数，先经过第一个全连接层和ReLU激活函数，再经过第二个全连接层
```

下面的例子显示，[**改变张量的最里层维度的尺寸**]，会改变成基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。

```python
ffn = PositionWiseFFN(4, 4, 8)  # 创建一个输入维度为4，隐藏层维度为4，输出维度为8的位置感知前馈网络
ffn.eval()  # 将网络设置为评估模式，这会影响某些层（如Dropout层）的行为
ffn(torch.ones((2, 3, 4)))[0]  # 对一个大小为(2, 3, 4)的张量进行前向传播，并取第一个样本的输出
```

## 残差连接和层规范化

现在让我们关注 :*加法和规范化*（add&norm）组件。正如在本节开头所述，这是由残差连接和紧随其后的层规范化组成的。两者都是构建有效的深度架构的关键。

层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。

以下代码[**对比不同维度的层规范化和批量规范化的效果**]。

```python
ln = nn.LayerNorm(2)  # 创建一个维度为2的Layer Normalization（层归一化）层
bn = nn.BatchNorm1d(2)  # 创建一个维度为2的Batch Normalization（批归一化）层
X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)  # 创建一个大小为2x2的张量X，数据类型为float32

# 在训练模式下计算X的均值和方差
print('layer norm:', ln(X), '\nbatch norm:', bn(X))  # 打印Layer Normalization和Batch Normalization的结果
```

现在可以[**使用残差连接和层规范化**]来实现`AddNorm`类。暂退法也被作为正则化方法使用。

```python
#@save
class AddNorm(nn.Module):
    """残差连接后进行层规范化"""
    def __init__(self, normalized_shape, dropout, **kwargs):
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)  # 定义一个Dropout层，用于随机置零输入张量的元素
        self.ln = nn.LayerNorm(normalized_shape)  # 定义一个Layer Normalization层，对输入进行归一化处理(层规范化)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)  # 返回层规范化后的残差连接结果
```

残差连接要求两个输入的形状相同，以便[**加法操作后输出张量的形状相同**]。

```python
add_norm = AddNorm([3, 4], 0.5)  # 创建一个AddNorm实例，normalized_shape为[3, 4]，dropout率为0.5
add_norm.eval()  # 将add_norm设置为评估模式，这会影响到内部的dropout行为
output = add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4)))  # 对两个大小为(2, 3, 4)的张量进行前向传播
output.shape  # 打印输出张量的形状
```

## 编码器

有了组成Transformer编码器的基础组件，现在可以先[**实现编码器中的一个层**]。下面的`EncoderBlock`类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。

```python
#@save
class EncoderBlock(nn.Module):
    """Transformer编码器块"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, use_bias=False, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        # 初始化多头注意力机制
        self.attention = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            use_bias)
        # 初始化第一个残差连接块和层规范化
        self.addnorm1 = AddNorm(norm_shape, dropout)
        # 初始化位置编码前馈网络
        self.ffn = PositionWiseFFN(
            ffn_num_input, ffn_num_hiddens, num_hiddens)
        # 初始化第二个残差连接块和层规范化
        self.addnorm2 = AddNorm(norm_shape, dropout)

    def forward(self, X, valid_lens):
        # 使用多头注意力机制进行编码器块的前向传播
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        # 使用位置编码前馈网络进行编码器块的前向传播
        return self.addnorm2(Y, self.ffn(Y))
```

正如从代码中所看到的，[**Transformer编码器中的任何层都不会改变其输入的形状**]。

```python
X = torch.ones((2, 100, 24))  # 创建一个形状为 (2, 100, 24) 的张量X，元素值全为1
valid_lens = torch.tensor([3, 2])  # 创建一个长度为2的张量，表示每个序列的有效长度为3和2
encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)  # 创建一个EncoderBlock对象，参数包括输入维度、隐藏层维度、注意力头数等
encoder_blk.eval()  # 设置encoder_blk为评估模式，不使用dropout等随机操作
encoder_blk(X, valid_lens).shape  # 对输入张量X进行编码操作，使用有效长度valid_lens，输出编码后的张量形状
```

下面实现的[**Transformer编码器**]的代码中，堆叠了`num_layers`个`EncoderBlock`类的实例。由于这里使用的是值范围在$-1$和$1$之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。

```python
#@save
class TransformerEncoder(d2l.Encoder):
    """Transformer编码器"""
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, use_bias=False, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens  # 设定隐藏层的维度（隐藏单元数）
        self.embedding = nn.Embedding(vocab_size, num_hiddens)  # 创建一个词嵌入层，将词汇索引映射到隐藏层维度
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)  # 创建位置编码器实例
        self.blks = nn.Sequential()  # 创建一个空的序列容器，用于存放多个编码块
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                EncoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, use_bias))  # 在序列中添加多个编码块

    def forward(self, X, valid_lens, *args):
        # 因为位置编码值在-1和1之间，
        # 因此嵌入值乘以嵌入维度的平方根进行缩放，
        # 然后再与位置编码相加。
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))  # 嵌入值乘以缩放因子后与位置编码相加
        self.attention_weights = [None] * len(self.blks)  # 初始化注意力权重列表
        for i, blk in enumerate(self.blks):
            X = blk(X, valid_lens)  # 逐个编码块进行前向传播
            self.attention_weights[
                i] = blk.attention.attention.attention_weights  # 存储当前编码块的注意力权重
        return X  # 返回编码器的输出张量
```

下面我们指定了超参数来[**创建一个两层的Transformer编码器**]。Transformer编码器输出的形状是（批量大小，时间步数目，`num_hiddens`）。

```python
# 创建一个Transformer编码器实例
encoder = TransformerEncoder(
    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)

# 设置为评估模式
encoder.eval()

# 执行前向传播并获取输出张量的形状
encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape
```

## 解码器

[**Transformer解码器也是由多个相同的层组成**]。在`DecoderBlock`类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。

正如在本节前面所述，在掩蔽多头解码器自注意力层（第一个子层）中，查询、键和值都来自上一个解码器层的输出。关于*序列到序列模型*（sequence-to-sequence model），在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数`dec_valid_lens`，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。

```python
class DecoderBlock(nn.Module):
    """解码器中第i个块"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, i, **kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.i = i
        # 第一个多头注意力层
        self.attention1 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        # 第一个残差连接和层归一化
        self.addnorm1 = AddNorm(norm_shape, dropout)
        # 第二个多头注意力层
        self.attention2 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        # 第二个残差连接和层归一化
        self.addnorm2 = AddNorm(norm_shape, dropout)
        # 位置前馈网络
        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,
                                   num_hiddens)
        # 第三个残差连接和层归一化
        self.addnorm3 = AddNorm(norm_shape, dropout)

    def forward(self, X, state):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # 训练阶段，输出序列的所有词元都在同一时间处理，
        # 因此state[2][self.i]初始化为None。
        # 预测阶段，输出序列是通过词元一个接着一个解码的，
        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = torch.cat((state[2][self.i], X), axis=1)
        state[2][self.i] = key_values
        if self.training:
            batch_size, num_steps, _ = X.shape
            # dec_valid_lens的开头:(batch_size,num_steps),
            # 其中每一行是[1,2,...,num_steps]
            dec_valid_lens = torch.arange(
                1, num_steps + 1, device=X.device).repeat(batch_size, 1)
        else:
            dec_valid_lens = None

        # 自注意力
        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
        Y = self.addnorm1(X, X2)
        # 编码器－解码器注意力。
        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z = self.addnorm2(Y, Y2)
        # 位置前馈网络
        return self.addnorm3(Z, self.ffn(Z)), state
```

为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，[**编码器和解码器的特征维度都是`num_hiddens`。**]

```python
decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)  # 创建一个 DecoderBlock 实例，设定参数
decoder_blk.eval()  # 设置模型为评估模式，禁用 dropout 和 batch normalization 的随机性
X = torch.ones((2, 100, 24))  # 创建一个大小为 (2, 100, 24) 的张量 X，用于输入
state = [encoder_blk(X, valid_lens), valid_lens, [None]]  # 使用 encoder_blk 对 X 进行编码，初始化状态
decoder_blk(X, state)[0].shape  # 调用 decoder_blk 进行前向传播，并获取输出的形状
```

现在我们构建了由`num_layers`个`DecoderBlock`实例组成的完整的[**Transformer解码器**]。最后，通过一个全连接层计算所有`vocab_size`个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。

```python
class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens  # 隐藏单元的数量
        self.num_layers = num_layers  # 解码器层数
        self.embedding = nn.Embedding(vocab_size, num_hiddens)  # 定义词嵌入层
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)  # 定义位置编码层
        self.blks = nn.Sequential()  # 创建一个空的序列容器用于存放解码器块
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                DecoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, i))  # 添加多个解码器块到序列容器中
        self.dense = nn.Linear(num_hiddens, vocab_size)  # 定义最终的全连接层，用于输出预测的词汇表大小

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]  # 初始化解码器的状态

    def forward(self, X, state):
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))  # 应用词嵌入和位置编码
        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]  # 初始化注意力权重列表
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state)  # 逐层进行解码器块的前向传播
            # 记录解码器自注意力权重
            self._attention_weights[0][i] = blk.attention1.attention.attention_weights
            # 记录编码器-解码器注意力权重
            self._attention_weights[1][i] = blk.attention2.attention.attention_weights
        return self.dense(X), state  # 返回最终的预测输出和最终状态

    @property
    def attention_weights(self):
        return self._attention_weights  # 返回注意力权重的属性
```

## **训练**

依照Transformer架构来实例化编码器－解码器模型。在这里，指定Transformer的编码器和解码器都是2层，都使用4头注意力。为了进行序列到序列的学习，下面在“英语－法语”机器翻译数据集上训练Transformer模型。

```python
num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10
# 隐藏单元数、层数、dropout率、批量大小、时间步数

lr, num_epochs, device = 0.005, 200, d2l.try_gpu()
# 学习率、训练轮数、设备（尝试使用GPU）

ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4
# 前馈网络输入大小、前馈网络隐藏层大小、注意力头数

key_size, query_size, value_size = 32, 32, 32
# 键、查询、值的大小

norm_shape = [32]
# 归一化层的大小

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
# 加载用于神经机器翻译训练的数据迭代器、源语言词汇表、目标语言词汇表

encoder = TransformerEncoder(
    len(src_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
# 初始化Transformer编码器模型

decoder = TransformerDecoder(
    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
# 初始化Transformer解码器模型

net = d2l.EncoderDecoder(encoder, decoder)
# 构建编码器-解码器模型

d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
# 使用神经机器翻译模型进行序列到序列的训练
```

训练结束后，使用Transformer模型[**将一些英语句子翻译成法语**]，并且计算它们的BLEU分数。

```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, dec_attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, True)
    # 预测给定英语句子的翻译，并获取解码器注意力权重序列
    print(f'{eng} => {translation}, ',
          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')
    # 打印原始英语句子、预测的翻译结果及其BLEU分数
```

当进行最后一个英语到法语的句子翻译工作时，让我们[**可视化Transformer的注意力权重**]。编码器自注意力权重的形状为（编码器层数，注意力头数，`num_steps`或查询的数目，`num_steps`或“键－值”对的数目）。

```python
# 将所有编码器注意力权重张量沿批次维度（维度0）拼接起来
enc_attention_weights = torch.cat(net.encoder.attention_weights, 0)
# 将拼接后的张量重塑为指定形状
enc_attention_weights = enc_attention_weights.reshape((num_layers, num_heads, -1, num_steps))
# 打印重塑后的张量形状
print(enc_attention_weights.shape)
```

在编码器的自注意力中，查询和键都来自相同的输入序列。因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。

```python
# 调用函数显示编码器注意力权重的热图
d2l.show_heatmaps(
    enc_attention_weights.cpu(),  # 传入CPU上的编码器注意力权重张量
    xlabel='Key positions',  # 设置X轴标签
    ylabel='Query positions',  # 设置Y轴标签
    titles=['Head %d' % i for i in range(1, 5)],  # 设置子图标题，从头部1到头部4
    figsize=(7, 3.5)  # 设置图像的大小
)
```

[**为了可视化解码器的自注意力权重和“编码器－解码器”的注意力权重，我们需要完成更多的数据操作工作。**]例如用零填充被掩蔽住的注意力权重。值得注意的是，解码器的自注意力权重和“编码器－解码器”的注意力权重都有相同的查询：即以*序列开始词元*（beginning-of-sequence,BOS）打头，再与后续输出的词元共同组成序列。

```python
# 将解码器注意力权重序列转换为2D列表
dec_attention_weights_2d = [head[0].tolist()
                            for step in dec_attention_weight_seq
                            for attn in step for blk in attn for head in blk]

# 填充NaN值并转换为PyTorch张量
dec_attention_weights_filled = torch.tensor(
    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)

# 将填充后的张量重塑为指定形状
dec_attention_weights = dec_attention_weights_filled.reshape((-1, 2, num_layers, num_heads, num_steps))

# 对重塑后的张量进行维度置换，分离自注意力权重和交互注意力权重
dec_self_attention_weights, dec_inter_attention_weights = \
    dec_attention_weights.permute(1, 2, 3, 0, 4)

# 打印自注意力权重和交互注意力权重的形状
dec_self_attention_weights.shape, dec_inter_attention_weights.shape
```

由于解码器自注意力的自回归属性，查询不会对当前位置之后的“键－值”对进行注意力计算。

```python
# 调用函数显示解码器自注意力权重的热图，包括序列开始标记
d2l.show_heatmaps(
    dec_self_attention_weights[:, :, :, :len(translation.split()) + 1],  # 提取包括序列开始标记在内的自注意力权重
    xlabel='Key positions',  # 设置X轴标签
    ylabel='Query positions',  # 设置Y轴标签
    titles=['Head %d' % i for i in range(1, 5)],  # 设置子图标题，从头部1到头部4
    figsize=(7, 3.5)  # 设置图像的大小
)
```

与编码器的自注意力的情况类似，通过指定输入序列的有效长度，[**输出序列的查询不会与输入序列中填充位置的词元进行注意力计算**]。

```python
# 调用函数显示解码器交互注意力权重的热图
d2l.show_heatmaps(
    dec_inter_attention_weights,  # 解码器交互注意力权重张量
    xlabel='Key positions',  # 设置X轴标签
    ylabel='Query positions',  # 设置Y轴标签
    titles=['Head %d' % i for i in range(1, 5)],  # 设置子图标题，从头部1到头部4
    figsize=(7, 3.5)  # 设置图像的大小
)
```

