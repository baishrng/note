# 门控循环单元（GRU）

### 重置门和更新门
## 从零开始实现

为了更好地理解门控循环单元模型，我们从零开始实现它。首先，我们读取使用的时间机器数据集：

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35  # 设置批量大小和时间步数
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)  # 加载时间机器数据集并创建迭代器
```

### [**初始化模型参数**]

下一步是初始化模型参数。我们从标准差为$0.01$的高斯分布中提取权重，并将偏置项设为$0$，超参数`num_hiddens`定义隐藏单元的数量，实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。

```python
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size  # 输入和输出的维度都是词汇表的大小

    # 函数用于生成正态分布的随机张量
    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01  # 标准差为0.01的正态分布随机张量

    # 初始化参数
    def three():
        return (normal((num_inputs, num_hiddens)),  # 输入到更新门的权重矩阵
                normal((num_hiddens, num_hiddens)),  # 隐状态到更新门的权重矩阵
                torch.zeros(num_hiddens, device=device))  # 更新门的偏置

    # 初始化更新门、重置门和候选隐状态的参数
    W_xz, W_hz, b_z = three()  # 更新门参数
    W_xr, W_hr, b_r = three()  # 重置门参数
    W_xh, W_hh, b_h = three()  # 候选隐状态参数

    # 初始化输出层参数
    W_hq = normal((num_hiddens, num_outputs))  # 隐状态到输出层的权重矩阵
    b_q = torch.zeros(num_outputs, device=device)  # 输出层的偏置向量

    # 附加梯度
    # 将所有参数放入列表中，并设置requires_grad为True以便反向传播时计算梯度
    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)

    return params
```

### 定义模型

现在我们将[**定义隐状态的初始化函数**]`init_gru_state`。与 :numref:`sec_rnn_scratch`中定义的`init_rnn_state`函数一样，此函数返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零。

```python
def init_gru_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )
    # 返回一个元组，包含一个形状为(batch_size, num_hiddens)的全零张量，放置在指定的device上。
```

现在我们准备[**定义门控循环单元模型**]，模型的架构与基本的循环神经网络单元是相同的，只是权重更新公式更为复杂。

```python
def gru(inputs, state, params):
    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params  # 参数解包
    H, = state  # 获取当前状态

    outputs = []  # 存储每个时间步的输出
    for X in inputs:
        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)  # 更新门，@ 表示矩阵乘法
        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)  # 重置门
        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)  # 候选隐藏状态
        H = Z * H + (1 - Z) * H_tilda  # 更新隐藏状态
        Y = H @ W_hq + b_q  # 输出
        outputs.append(Y)  # 添加当前时间步的输出到列表中

    return torch.cat(outputs, dim=0), (H,)  # 将所有时间步的输出连接起来并返回，同时返回最终的隐藏状态
```

### [**训练**]与预测

训练和预测的工作方式与 :numref:`sec_rnn_scratch`完全相同。训练结束后，我们分别打印输出训练集的困惑度，以及前缀“time traveler”和“traveler”的预测序列上的困惑度。

```python
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()  # 获取词汇表大小，隐藏单元数量，选择设备（GPU或CPU）
num_epochs, lr = 500, 1  # 设置训练的轮数和学习率
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,
                            init_gru_state, gru)  # 创建基于GRU的模型
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)  # 训练模型
```

## [**简洁实现**]

高级API包含了前文介绍的所有配置细节，所以我们可以直接实例化门控循环单元模型。这段代码的运行速度要快得多，因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。

```python
num_inputs = vocab_size  # 输入特征的数量等于词汇表的大小
gru_layer = nn.GRU(num_inputs, num_hiddens)  # 创建一个GRU层，输入大小为num_inputs，隐藏状态大小为num_hiddens
model = d2l.RNNModel(gru_layer, len(vocab))  # 创建基于GRU的循环神经网络模型，输出大小为词汇表的大小
model = model.to(device)  # 将模型移动到指定的设备（GPU或CPU）
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)  # 使用训练函数训练模型
```

# 长短期记忆网络（LSTM）
## 门控记忆元
### 输入门、忘记门和输出门
### 候选记忆元
### 记忆元
### 隐状态
## 从零开始实现

现在，我们从零开始实现长短期记忆网络。我们首先加载时光机器数据集。

```python
import torch
from torch import nn
from d2l import torch as d2l  # 导入PyTorch和d2l库中的torch部分作为别名d2l

batch_size, num_steps = 32, 35  # 设置批量大小和时间步数
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)  # 使用d2l库中的函数加载时间机器数据集，得到训练数据迭代器和词汇表
```

### [**初始化模型参数**]

接下来，我们需要定义和初始化模型参数。如前所述，超参数`num_hiddens`定义隐藏单元的数量。我们按照标准差$0.01$的高斯分布初始化权重，并将偏置项设为$0$。

```python
def get_lstm_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    # 定义一个函数normal，用于生成指定形状的正态分布随机数，并移到指定设备上
    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01

    # 定义一个函数three，用于生成LSTM需要的权重和偏置参数
    def three():
        # 生成输入和隐藏状态之间的权重矩阵，以及隐藏状态自身的权重矩阵，以及偏置向量
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    # 初始化LSTM模型的各个门的参数
    W_xi, W_hi, b_i = three()  # 输入门参数
    W_xf, W_hf, b_f = three()  # 遗忘门参数
    W_xo, W_ho, b_o = three()  # 输出门参数
    W_xc, W_hc, b_c = three()  # 候选记忆元参数

    # 初始化输出层的参数
    W_hq = normal((num_hiddens, num_outputs))  # 输出层权重
    b_q = torch.zeros(num_outputs, device=device)  # 输出层偏置

    # 附加梯度
    # 将所有需要优化的参数放入列表params中，并设置requires_grad=True以便后续计算梯度
    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,
              b_c, W_hq, b_q]

    for param in params:
        param.requires_grad_(True)  # 设置requires_grad=True，表示这些参数需要计算梯度

    return params  # 返回初始化的所有参数列表
```

### 定义模型

在[**初始化函数**]中，长短期记忆网络的隐状态需要返回一个*额外*的记忆元，单元的值为0，形状为（批量大小，隐藏单元数）。因此，我们得到以下的状态初始化。

```python
def init_lstm_state(batch_size, num_hiddens, device):
    # 返回初始化的LSTM隐藏状态元组 (h_0, c_0)，均为全零张量
    return (torch.zeros((batch_size, num_hiddens), device=device),  # 初始化隐藏状态 h_0
            torch.zeros((batch_size, num_hiddens), device=device))  # 初始化细胞状态 c_0
```

[**实际模型**]的定义与我们前面讨论的一样：提供三个门和一个额外的记忆元。请注意，只有隐状态才会传递到输出层，而记忆元$\mathbf{C}_t$不直接参与输出计算。

```python
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
     W_hq, b_q] = params  # 解包参数列表

    (H, C) = state  # 获取隐藏状态和细胞状态
    outputs = []
    for X in inputs:
        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)  # 输入门, @ 表示矩阵乘法
        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)  # 遗忘门
        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)  # 输出门
        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)  # 更新细胞状态
        C = F * C + I * C_tilda  # 更新细胞状态
        H = O * torch.tanh(C)  # 更新隐藏状态
        Y = (H @ W_hq) + b_q  # 输出预测值
        outputs.append(Y)  # 添加到输出列表
    return torch.cat(outputs, dim=0), (H, C)  # 返回拼接后的输出和更新后的状态
```

### [**训练**]和预测

让我们通过实例化 `RNNModelScratch`类来训练一个长短期记忆网络，

```python
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()  # 获取词汇表大小、隐藏层单元数和设备
num_epochs, lr = 500, 1  # 设置训练轮数和学习率
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,
                            init_lstm_state, lstm)  # 初始化模型
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)  # 训练模型
```

## [**简洁实现**]

使用高级API，我们可以直接实例化`LSTM`模型。高级API封装了前文介绍的所有配置细节。这段代码的运行速度要快得多，因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。

```python
num_inputs = vocab_size  # 输入的特征数等于词汇表的大小
lstm_layer = nn.LSTM(num_inputs, num_hiddens)  # 创建一个包含单个LSTM层的模型
model = d2l.RNNModel(lstm_layer, len(vocab))  # 使用d2l库中的RNNModel封装LSTM模型并设置词汇表大小
model = model.to(device)  # 将模型移动到指定的设备（CPU或GPU）
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)  # 使用d2l库中的train_ch8函数训练模型
```

# 深度循环神经网络
## 简洁实现

实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的。简单起见，我们仅示范使用此类内置函数的实现方式。以长短期记忆网络模型为例，我们指定了层的数量。像往常一样，我们从加载数据集开始。

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35  # 设置批量大小和时间步数
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)  # 使用d2l库中的load_data_time_machine函数加载时间机器数据集
```

因为我们有不同的词元，所以输入和输出都选择相同数量，即`vocab_size`。隐藏单元的数量仍然是$256$。我们现在(**通过`num_layers`的值来设定隐藏层数**)。

```python
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2  # 设置词汇表大小，隐藏单元数量为256，LSTM层数为2
num_inputs = vocab_size  # 输入大小等于词汇表大小
device = d2l.try_gpu()  # 尝试使用GPU设备，如果不可用则使用CPU
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)  # 创建LSTM层，输入大小为num_inputs，隐藏单元数量为num_hiddens，层数为num_layers
model = d2l.RNNModel(lstm_layer, len(vocab))  # 创建RNN模型，使用上述的LSTM层和词汇表大小
model = model.to(device)  # 将模型移动到指定的设备（GPU或CPU）
```

## **训练**与预测

由于使用了长短期记忆网络模型来实例化两个层，因此训练速度被大大降低了。

```python
num_epochs, lr = 500, 2  # 设置训练的总轮数为500，学习率为2
d2l.train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device)  # 使用d2l库中的train_ch8函数进行模型训练，传入模型、训练数据迭代器、词汇表、学习率、总轮数和设备信息
```

# 机器翻译与数据集
本书的关注点是神经网络机器翻译方法，强调的是端到端的学习。机器翻译的数据集是由源语言和目标语言的文本序列对组成的。因此，我们需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。下面，我们看一下如何将预处理后的数据加载到小批量中用于训练。

```python
import os
import torch
from d2l import torch as d2l
```

## **下载和预处理数据集**

首先，下载一个由[Tatoeba项目的双语句子对](http://www.manythings.org/anki/)组成的“英－法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成。请注意，每个文本序列可以是一个句子，也可以是包含多个句子的一个段落。在这个将英语翻译成法语的机器翻译问题中，英语是*源语言*（source language），法语是*目标语言*（target language）。

```python
d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
                           '94646ad1522d915e7b0f9296181140edcf86a4f5')

def read_data_nmt():
    """载入“英语－法语”数据集"""
    data_dir = d2l.download_extract('fra-eng')  # 下载并解压“英语－法语”数据集
    with open(os.path.join(data_dir, 'fra.txt'), 'r',
             encoding='utf-8') as f:
        return f.read()  # 读取并返回文件内容

raw_text = read_data_nmt()
print(raw_text[:75])  # 打印文件内容的前75个字符
```

下载数据集后，原始文本数据需要经过[**几个预处理步骤**]。例如，我们用空格代替*不间断空格*（non-breaking space），使用小写字母替换大写字母，并在单词和标点符号之间插入空格。

```python
#@save
def preprocess_nmt(text):
    """预处理“英语－法语”数据集"""
    def no_space(char, prev_char):
        return char in set(',.!?') and prev_char != ' '

    # 使用空格替换不间断空格
    # 使用小写字母替换大写字母
    text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower()
    # 在单词和标点符号之间插入空格
    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char
           for i, char in enumerate(text)]
    return ''.join(out)

text = preprocess_nmt(raw_text)
print(text[:80])
```

## **词元化**

在机器翻译中，我们更喜欢单词级词元化（最先进的模型可能使用更高级的词元化技术）。下面的`tokenize_nmt`函数对前`num_examples`个文本序列对进行词元，其中每个词元要么是一个词，要么是一个标点符号。此函数返回两个词元列表：`source`和`target`：`source[i]`是源语言（这里是英语）第$i$个文本序列的词元列表，`target[i]`是目标语言（这里是法语）第$i$个文本序列的词元列表。

```python
#@save
def tokenize_nmt(text, num_examples=None):
    """词元化“英语－法语”数据数据集"""
    source, target = [], []
    for i, line in enumerate(text.split('\n')):
        if num_examples and i > num_examples:
            break
        parts = line.split('\t')
        if len(parts) == 2:
            source.append(parts[0].split(' '))  # 将源语言句子拆分为词元列表
            target.append(parts[1].split(' '))  # 将目标语言句子拆分为词元列表
    return source, target

source, target = tokenize_nmt(text)
source[:6], target[:6]
```

让我们[**绘制每个文本序列所包含的词元数量的直方图**]。在这个简单的“英－法”数据集中，大多数文本序列的词元数量少于$20$个。

```python
#@save
def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):
    """绘制列表长度对的直方图"""
    d2l.set_figsize()  # 设置图表尺寸
    _, _, patches = d2l.plt.hist(
        [[len(l) for l in xlist], [len(l) for l in ylist]])  # 绘制两个列表长度的直方图
    d2l.plt.xlabel(xlabel)  # 设置x轴标签
    d2l.plt.ylabel(ylabel)  # 设置y轴标签
    for patch in patches[1].patches:
        patch.set_hatch('/')  # 对第二个数据集的直方图添加斜线填充
    d2l.plt.legend(legend)  # 添加图例

show_list_len_pair_hist(['source', 'target'], '# tokens per sequence',
                        'count', source, target);
```

## **词表**

由于机器翻译数据集由语言对组成，因此我们可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，这里我们将出现次数少于2次的低频率词元视为相同的未知（“&lt;unk&gt;”）词元。除此之外，我们还指定了额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元（“&lt;pad&gt;”），以及序列的开始词元（“&lt;bos&gt;”）和结束词元（“&lt;eos&gt;”）。这些特殊词元在自然语言处理任务中比较常用。

```python
src_vocab = d2l.Vocab(source, min_freq=2,
                      reserved_tokens=['<pad>', '<bos>', '<eos>'])
len(src_vocab)  # 输出源语言词汇表的长度
```

## 加载数据集
回想一下，语言模型中的[**序列样本都有一个固定的长度**]，无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。这个固定长度是由 `num_steps`（时间步数或词元数量）参数指定的。为了提高计算效率，我们仍然可以通过*截断*（truncation）和*填充*（padding）方式实现一次只处理一个小批量的文本序列。假设同一个小批量中的每个序列都应该具有相同的长度`num_steps`，那么如果文本序列的词元数目少于`num_steps`时，我们将继续在其末尾添加特定的“&lt;pad&gt;”词元，直到其长度达到`num_steps`；反之，我们将截断文本序列时，只取其前`num_steps` 个词元，并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度，以便以相同形状的小批量进行加载。

如前所述，下面的`truncate_pad`函数将(**截断或填充文本序列**)。

```python
#@save
def truncate_pad(line, num_steps, padding_token):
    """截断或填充文本序列"""
    if len(line) > num_steps:
        return line[:num_steps]  # 如果长度超过 num_steps，则截断
    return line + [padding_token] * (num_steps - len(line))  # 如果长度不足 num_steps，则填充

truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])
```

现在我们定义一个函数，可以将文本序列[**转换成小批量数据集用于训练**]。我们将特定的“&lt;eos&gt;”词元添加到所有序列的末尾，用于表示序列的结束。当模型通过一个词元接一个词元地生成序列进行预测时，生成的“&lt;eos&gt;”词元说明完成了序列输出工作。此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，在稍后将要介绍的一些模型会需要这个长度信息。

```python
#@save
def build_array_nmt(lines, vocab, num_steps):
    """将机器翻译的文本序列转换成小批量"""
    lines = [vocab[l] for l in lines]
    lines = [l + [vocab['<eos>']] for l in lines]
    array = torch.tensor([truncate_pad(
        l, num_steps, vocab['<pad>']) for l in lines])
    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)
    return array, valid_len
```

## **训练模型**

最后，我们定义`load_data_nmt`函数来返回数据迭代器，以及源语言和目标语言的两种词表。

```python
#@save
def load_data_nmt(batch_size, num_steps, num_examples=600):
    """返回翻译数据集的迭代器和词表"""
    text = preprocess_nmt(read_data_nmt())  # 预处理文本数据
    source, target = tokenize_nmt(text, num_examples)  # 分词得到源语言和目标语言文本
    src_vocab = d2l.Vocab(source, min_freq=2,  # 创建源语言词表
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])
    tgt_vocab = d2l.Vocab(target, min_freq=2,  # 创建目标语言词表
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])
    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)  # 构建源语言数据数组
    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)  # 构建目标语言数据数组
    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
    data_iter = d2l.load_array(data_arrays, batch_size)  # 加载数据并生成迭代器
    return data_iter, src_vocab, tgt_vocab  # 返回数据迭代器、源语言词表和目标语言词表
```

下面我们[**读出“英语－法语”数据集中的第一个小批量数据**]。

```python
train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)
for X, X_valid_len, Y, Y_valid_len in train_iter:
    print('X:', X.type(torch.int32))  # 打印源语言数据张量及其数据类型
    print('X的有效长度:', X_valid_len)  # 打印源语言数据的有效长度
    print('Y:', Y.type(torch.int32))  # 打印目标语言数据张量及其数据类型
    print('Y的有效长度:', Y_valid_len)  # 打印目标语言数据的有效长度
    break
```

# 编码器-解码器架构
## **编码器**

在编码器接口中，我们只指定长度可变的序列作为编码器的输入`X`。任何继承这个`Encoder`基类的模型将完成代码实现。

```python
from torch import nn


#@save
class Encoder(nn.Module):
    """编码器-解码器架构的基本编码器接口"""
    def __init__(self, **kwargs):
        super(Encoder, self).__init__(**kwargs)

    def forward(self, X, *args):
        raise NotImplementedError  # 抛出未实现错误
```

## **解码器**

在下面的解码器接口中，我们新增一个`init_state`函数，用于将编码器的输出（`enc_outputs`）转换为编码后的状态。注意，此步骤可能需要额外的输入，为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入，例如：在前一时间步生成的词元）和编码后的状态映射成当前时间步的输出词元。

```python
#@save
class Decoder(nn.Module):
    """编码器-解码器架构的基本解码器接口"""
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)

    def init_state(self, enc_outputs, *args):
        raise NotImplementedError  # 抛出未实现错误

    def forward(self, X, state):
        raise NotImplementedError  # 抛出未实现错误
```

## **合并编码器和解码器**

总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分。

```python
#@save
class EncoderDecoder(nn.Module):
    """编码器-解码器架构的基类"""
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder  # 设置编码器模型
        self.decoder = decoder  # 设置解码器模型

    def forward(self, enc_X, dec_X, *args):
        enc_outputs = self.encoder(enc_X, *args)  # 编码器处理输入
        dec_state = self.decoder.init_state(enc_outputs, *args)  # 初始化解码器状态
        return self.decoder(dec_X, dec_state)  # 解码器处理输入和状态
```

#  序列到序列学习（seq2seq）

```python
import collections  # 导入collections模块
import math  # 导入math模块
import torch  # 导入torch模块
from torch import nn  # 从torch模块导入nn子模块
from d2l import torch as d2l  # 导入d2l库中的torch模块并重命名为d2l
```

## 编码器
现在，让我们[**实现循环神经网络编码器**]。注意，我们使用了*嵌入层*（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（`vocab_size`），其列数等于特征向量的维度（`embed_size`）。对于任意输入词元的索引$i$，嵌入层获取权重矩阵的第$i$行（从$0$开始）以返回其特征向量。另外，本文选择了一个多层门控循环单元来实现编码器。

```python
#@save
class Seq2SeqEncoder(d2l.Encoder):
    """用于序列到序列学习的循环神经网络编码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqEncoder, self).__init__(**kwargs)
        # 嵌入层
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # GRU循环神经网络层
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,
                          dropout=dropout)

    def forward(self, X, *args):
        # 输入X的形状：(batch_size, num_steps)
        X = self.embedding(X)  # 嵌入层处理输入
        X = X.permute(1, 0, 2)  # 调整输入形状以适应RNN模型的要求
        output, state = self.rnn(X)  # RNN模型处理输入
        # output的形状：(num_steps, batch_size, num_hiddens)
        # state的形状：(num_layers, batch_size, num_hiddens)
        return output, state
```

下面，我们实例化[**上述编码器的实现**]：我们使用一个两层门控循环单元编码器，其隐藏单元数为$16$。给定一小批量的输入序列`X`（批量大小为$4$，时间步为$7$）。在完成所有时间步后，最后一层的隐状态的输出是一个张量（`output`由编码器的循环层返回），其形状为（时间步数，批量大小，隐藏单元数）。

```python
encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)  # 创建一个Seq2SeqEncoder实例，指定词汇大小为10，嵌入大小为8，隐藏单元数为16，层数为2

encoder.eval()  # 将编码器设置为评估模式，通常用于推断阶段，会关闭dropout等随机性操作

X = torch.zeros((4, 7), dtype=torch.long)  # 创建一个形状为(4, 7)的零张量作为输入X，数据类型为长整型

output, state = encoder(X)  # 使用编码器处理输入X，获取输出output和状态state

output.shape  # 输出output的形状

```

由于这里使用的是门控循环单元，所以在最后一个时间步的多层隐状态的形状是（隐藏层的数量，批量大小，隐藏单元的数量）。如果使用长短期记忆网络，`state`中还将包含记忆单元信息。

```python
state.shape
```

## **解码器**
在输出序列上的任意时间步$t^\prime$，循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$和上下文变量$\mathbf{c}$作为其输入，然后在当前时间步将它们和上一隐状态$\mathbf{s}_{t^\prime-1}$转换为隐状态$\mathbf{s}_{t^\prime}$。因此，可以使用函数$g$来表示解码器的隐藏层的变换：

$$\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1}).$$

当实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。

```python
class Seq2SeqDecoder(d2l.Decoder):
    """用于序列到序列学习的循环神经网络解码器"""
    
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)  # 嵌入层，将输入索引转换为嵌入向量
        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,
                          dropout=dropout)  # GRU循环神经网络，用于学习序列数据的动态特征
        self.dense = nn.Linear(num_hiddens, vocab_size)  # 全连接层，将GRU输出映射到词汇表大小的向量

    def init_state(self, enc_outputs, *args):
        return enc_outputs[1]  # 初始化解码器状态，使用编码器的最终状态作为初始状态

    def forward(self, X, state):
        # X的形状：(batch_size, num_steps)，表示输入序列
        # 将X的形状转换为(batch_size, num_steps, embed_size)，并交换维度顺序以适应GRU的输入要求
        X = self.embedding(X).permute(1, 0, 2)
        
        # 使用编码器的最终隐藏状态作为context，通过广播使其与X具有相同的num_steps
        context = state[-1].repeat(X.shape[0], 1, 1)
        
        # 将输入X和context连接起来作为GRU的输入
        X_and_context = torch.cat((X, context), 2)
        
        # 将连接后的输入传入GRU，得到输出output和更新后的状态state
        output, state = self.rnn(X_and_context, state)
        
        # 将GRU的输出output的形状转换为(batch_size, num_steps, vocab_size)，以便通过全连接层得到最终输出
        output = self.dense(output).permute(1, 0, 2)
        
        # 返回输出output和更新后的状态state
        # output的形状:(batch_size, num_steps, vocab_size)
        # state的形状:(num_layers, batch_size, num_hiddens)
        return output, state
```

下面，我们用与前面提到的编码器中相同的超参数来[**实例化解码器**]。如我们所见，解码器的输出形状变为（批量大小，时间步数，词表大小），其中张量的最后一个维度存储预测的词元分布。

```python
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)  # 初始化Seq2Seq解码器，设置词汇表大小、嵌入大小、隐藏单元数和层数

decoder.eval()  # 设置解码器为评估模式，这意味着在推理时不使用dropout等训练特定的操作

state = decoder.init_state(encoder(X))  # 使用编码器的输出作为初始状态来初始化解码器的状态

output, state = decoder(X, state)  # 将输入X和初始化后的状态state输入解码器，计算输出和更新后的状态

output.shape, state.shape  # 输出解码器的输出张量形状和最终状态张量形状
```

## 损失函数

在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用softmax来获得分布，并通过计算交叉熵损失函数来进行优化。特定的填充词元被添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。但是，我们应该将填充词元的预测排除在损失函数的计算之外。

为此，我们可以使用下面的`sequence_mask`函数[**通过零值化屏蔽不相关的项**]，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。例如，如果两个序列的有效长度（不包括填充词元）分别为$1$和$2$，则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。

```python
#@save
def sequence_mask(X, valid_len, value=0):
    """在序列中屏蔽不相关的项"""
    maxlen = X.size(1)  # 获取序列的最大长度
    mask = torch.arange((maxlen), dtype=torch.float32,
                        device=X.device)[None, :] < valid_len[:, None]  # 创建一个掩码，将大于有效长度的位置设为0
    X[~mask] = value  # 将不符合掩码条件的位置设为指定的值
    return X

X = torch.tensor([[1, 2, 3], [4, 5, 6]])
sequence_mask(X, torch.tensor([1, 2]))  # 调用sequence_mask函数，屏蔽每个序列中不相关的项
```

(**我们还可以使用此函数屏蔽最后几个轴上的所有项。**)如果愿意，也可以使用指定的非零值来替换这些项。

```python
X = torch.ones(2, 3, 4)
sequence_mask(X, torch.tensor([1, 2]), value=-1)
```

现在，我们可以[**通过扩展softmax交叉熵损失函数来遮蔽不相关的预测**]。最初，所有预测词元的掩码都设置为1。一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。

```python
#@save
class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
    """带遮蔽的softmax交叉熵损失函数"""
    # pred的形状：(batch_size,num_steps,vocab_size)
    # label的形状：(batch_size,num_steps)
    # valid_len的形状：(batch_size,)
    
    def forward(self, pred, label, valid_len):
        weights = torch.ones_like(label)  # 创建一个与label形状相同的全1张量作为权重
        weights = sequence_mask(weights, valid_len)  # 根据有效长度对权重进行掩码处理
        self.reduction='none'  # 设置损失计算方式为不进行汇总
        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(
            pred.permute(0, 2, 1), label)  # 调用父类的交叉熵损失函数计算未加权的损失
        weighted_loss = (unweighted_loss * weights).mean(dim=1)  # 计算加权损失，并对每个样本取平均
        return weighted_loss
```

我们可以创建三个相同的序列来进行[**代码健全性检查**]，然后分别指定这些序列的有效长度为$4$、$2$和$0$。结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。

```python
loss = MaskedSoftmaxCELoss()  # 创建 MaskedSoftmaxCELoss 的实例

# 计算损失，输入预测值、标签和有效长度
# 预测值的形状为 (3, 4, 10)，标签的形状为 (3, 4)，有效长度为 [4, 2, 0]
loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),
     torch.tensor([4, 2, 0]))
```

## [**训练**]

在下面的循环训练过程中，特定的序列开始词元（“&lt;bos&gt;”）和原始的输出序列（不包括序列结束词元“&lt;eos&gt;”）拼接在一起作为解码器的输入。这被称为*强制教学*（teacher forcing），因为原始的输出序列（词元的标签）被送入解码器。或者，将来自上一个时间步的*预测*得到的词元作为解码器的当前输入。

```python
#@save
def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
    """训练序列到序列模型"""
    def xavier_init_weights(m):
        if type(m) == nn.Linear:
            nn.init.xavier_uniform_(m.weight)
        if type(m) == nn.GRU:
            for param in m._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(m._parameters[param])

    net.apply(xavier_init_weights)  # 应用 Xavier 初始化权重
    net.to(device)  # 将模型移动到指定的设备（如 GPU）
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)  # 使用 Adam 优化器
    loss = MaskedSoftmaxCELoss()  # 定义 MaskedSoftmaxCELoss 损失函数
    net.train()  # 将模型设置为训练模式
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                     xlim=[10, num_epochs])  # 创建动画对象，用于显示训练过程中的损失曲线
    for epoch in range(num_epochs):
        timer = d2l.Timer()  # 创建计时器，用于测量每个 epoch 的训练时间
        metric = d2l.Accumulator(2)  # 累加器，用于计算训练损失总和和词元数量
        for batch in data_iter:
            optimizer.zero_grad()  # 梯度清零
            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]  # 将数据移动到指定的设备
            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],
                          device=device).reshape(-1, 1)  # 创建起始符号 "<bos>" 的张量并移动到设备上
            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 构造解码器的输入（强制教学）
            Y_hat, _ = net(X, dec_input, X_valid_len)  # 前向传播计算预测值
            l = loss(Y_hat, Y, Y_valid_len)  # 计算损失
            l.sum().backward()  # 损失的标量进行反向传播
            d2l.grad_clipping(net, 1)  # 梯度裁剪
            num_tokens = Y_valid_len.sum()  # 计算有效词元数量
            optimizer.step()  # 更新模型参数
            with torch.no_grad():
                metric.add(l.sum(), num_tokens)  # 累加当前批次的损失和词元数量
        if (epoch + 1) % 10 == 0:
            animator.add(epoch + 1, (metric[0] / metric[1],))  # 在动画中添加当前 epoch 的损失值
    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
        f'tokens/sec on {str(device)}')  # 打印最终的损失值和训练速度
```

现在，在机器翻译数据集上，我们可以[**创建和训练一个循环神经网络“编码器－解码器”模型**]用于序列到序列的学习。

```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1  # 设置词嵌入大小、隐藏单元数、层数、dropout概率
batch_size, num_steps = 64, 10  # 设置批量大小和每个序列的时间步数
lr, num_epochs, device = 0.005, 300, d2l.try_gpu()  # 设置学习率、训练轮数、设备（尝试使用GPU）

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)  # 加载神经机器翻译的训练数据迭代器、源语言和目标语言词汇表
encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)  # 定义编码器部分的序列到序列模型
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)  # 定义解码器部分的序列到序列模型
net = d2l.EncoderDecoder(encoder, decoder)  # 将编码器和解码器组合成一个编码器-解码器模型
train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)  # 训练序列到序列模型
```

## [**预测**]

为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预测词元。与训练类似，序列开始词元（“&lt;bos&gt;”）在初始时间步被输入到解码器中。当输出序列的预测遇到序列结束词元（“&lt;eos&gt;”）时，预测就结束了。

```python
#@save
def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
                    device, save_attention_weights=False):
    """序列到序列模型的预测"""
    # 在预测时将net设置为评估模式
    net.eval()
    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
        src_vocab['<eos>']]  # 将源语句分词并转换为对应的词汇索引，并添加结束标记
    enc_valid_len = torch.tensor([len(src_tokens)], device=device)  # 计算有效长度
    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])  # 对源语句进行截断或填充
    # 添加批量轴
    enc_X = torch.unsqueeze(
        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)  # 将处理好的源语句转换为张量，并添加批量维度
    enc_outputs = net.encoder(enc_X, enc_valid_len)  # 编码器计算输出
    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)  # 初始化解码器状态
    # 添加批量轴
    dec_X = torch.unsqueeze(torch.tensor(
        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)  # 在目标语句开始处添加起始标记，并转换为张量
    output_seq, attention_weight_seq = [], []
    for _ in range(num_steps):
        Y, dec_state = net.decoder(dec_X, dec_state)  # 解码器生成输出及更新状态
        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入
        dec_X = Y.argmax(dim=2)  # 根据最高可能性的词元生成下一个时间步的输入
        pred = dec_X.squeeze(dim=0).type(torch.int32).item()  # 获取预测结果
        # 保存注意力权重（稍后讨论）
        if save_attention_weights:
            attention_weight_seq.append(net.decoder.attention_weights)
        # 一旦序列结束词元被预测，输出序列的生成就完成了
        if pred == tgt_vocab['<eos>']:
            break
        output_seq.append(pred)  # 将预测结果添加到输出序列中
    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq  # 将预测的目标语句转换为文本并返回，同时返回注意力权重（如有保存）
```

## 预测序列的评估
[**BLEU的代码实现**]如下。

```python
def bleu(pred_seq, label_seq, k):  #@save
    """计算BLEU"""
    # 将预测序列和参考序列按空格分割成单词列表
    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
    # 计算预测序列和参考序列的长度
    len_pred, len_label = len(pred_tokens), len(label_tokens)
    # 计算短句惩罚分数
    score = math.exp(min(0, 1 - len_label / len_pred))
    # 对于每个n-gram大小，计算匹配情况
    for n in range(1, k + 1):
        num_matches, label_subs = 0, collections.defaultdict(int)
        # 统计参考序列中每个n-gram的出现次数
        for i in range(len_label - n + 1):
            label_subs[' '.join(label_tokens[i: i + n])] += 1
        # 统计预测序列中与参考序列匹配的n-gram数量
        for i in range(len_pred - n + 1):
            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:
                num_matches += 1
                label_subs[' '.join(pred_tokens[i: i + n])] -= 1
        # 计算当前n-gram大小的精确度，并将其按照权重因子 math.pow(0.5, n) 加权后乘到分数上
        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
    return score
```

最后，利用训练好的循环神经网络“编码器－解码器”模型，[**将几个英语句子翻译成法语**]，并计算BLEU的最终结果。

```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']  # 英文句子列表
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']  # 法语句子列表
for eng, fra in zip(engs, fras):
    translation, attention_weight_seq = predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device)
    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')  # 打印英文句子及其翻译结果，并计算BLEU分数
```

