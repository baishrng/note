# 编译器和解释器

目前为止，本书主要关注的是*命令式编程*（imperative programming）。命令式编程使用诸如`print`、“`+`”和`if`之类的语句来更改程序的状态。考虑下面这段简单的命令式程序：

```python
def add(a, b):
    return a + b

def fancy_func(a, b, c, d):
    e = add(a, b)
    f = add(c, d)
    g = add(e, f)
    return g

print(fancy_func(1, 2, 3, 4))
```

## 符号式编程
下面，我们将通过模拟命令式编程来进一步了解符号式编程的概念。

```python
def add_():
    return '''
def add(a, b):
    return a + b
'''

def fancy_func_():
    return '''
def fancy_func(a, b, c, d):
    e = add(a, b)
    f = add(c, d)
    g = add(e, f)
    return g
'''

def evoke_():
    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'  # 组合代码字符串，包括定义函数和打印函数调用结果的语句

prog = evoke_()  # 生成完整的程序代码字符串
print(prog)  # 打印生成的程序代码字符串

y = compile(prog, '', 'exec')  # 编译程序代码字符串
exec(y)  # 执行编译后的代码
```

## `Sequential`的混合式编程

要了解混合式编程的工作原理，最简单的方法是考虑具有多层的深层网络。按照惯例，Python解释器需要执行所有层的代码来生成一条指令，然后将该指令转发到CPU或GPU。对于单个的（快速的）计算设备，这不会导致任何重大问题。另一方面，如果我们使用先进的8-GPU服务器，比如AWS P3dn.24xlarge实例，Python将很难让所有的GPU都保持忙碌。在这里，瓶颈是单线程的Python解释器。让我们看看如何通过将`Sequential`替换为`HybridSequential`来解决代码中这个瓶颈。首先，我们定义一个简单的多层感知机。

```python
import torch
from torch import nn
from d2l import torch as d2l  # 导入必要的库

# 生产网络的工厂模式
def get_net():
    net = nn.Sequential(nn.Linear(512, 256),  # 添加一个线性层，输入维度512，输出维度256
                        nn.ReLU(),  # 添加ReLU激活函数
                        nn.Linear(256, 128),  # 添加一个线性层，输入维度256，输出维度128
                        nn.ReLU(),  # 添加ReLU激活函数
                        nn.Linear(128, 2))  # 添加一个线性层，输入维度128，输出维度2
    return net  # 返回定义好的网络模型

x = torch.randn(size=(1, 512))  # 生成一个大小为(1, 512)的随机输入张量
net = get_net()  # 获得一个定义好的神经网络模型
net(x)  # 对输入x进行前向传播计算
```

通过使用`torch.jit.script`函数来转换模型，我们就有能力编译和优化多层感知机中的计算，而模型的计算结果保持不变。

```python
net = torch.jit.script(net)  # 将网络模型转换为TorchScript模块
net(x)  # 对输入x进行前向传播计算
```

### 通过混合式编程加速

为了证明通过编译获得了性能改进，我们比较了混合编程前后执行`net(x)`所需的时间。让我们先定义一个度量时间的类，它在本章中在衡量（和改进）模型性能时将非常有用。

```python
class Benchmark:
    """用于测量运行时间的上下文管理器"""

    def __init__(self, description='Done'):
        self.description = description

    def __enter__(self):
        self.timer = d2l.Timer()  # 使用d2l.Timer来计时
        return self

    def __exit__(self, *args):
        print(f'{self.description}: {self.timer.stop():.4f} sec')  # 打印描述和计时结果
```

现在我们可以调用网络两次，一次使用torchscript，一次不使用torchscript。

```python
net = get_net()  # 获取未经TorchScript编译的神经网络模型
# 测试无 TorchScript 的情况
with Benchmark('无torchscript'):
    for i in range(1000):
        net(x)  # 对模型进行1000次前向传播计算

net = torch.jit.script(net)  # 将模型转换为 TorchScript 模型
# 测试有 TorchScript 的情况
with Benchmark('有torchscript'):
    for i in range(1000):
        net(x)  # 对 TorchScript 模型进行1000次前向传播计算
```

### 序列化
编译模型的好处之一是我们可以将模型及其参数序列化（保存）到磁盘。这允许这些训练好的模型部署到其他设备上，并且还能方便地使用其他前端编程语言。同时，通常编译模型的代码执行速度也比命令式编程更快。让我们看看`save`的实际功能。

```python
net.save('my_mlp')  # 保存模型为名为 'my_mlp' 的文件

# 使用操作系统命令来检查文件大小
!ls -lh my_mlp*
```

# 异步计算

```python
import os  # 导入操作系统相关的库
import subprocess  # 导入子进程管理相关的库
import numpy  # 导入数值计算相关的库
import torch  # 导入 PyTorch 深度学习库
from torch import nn  # 从 PyTorch 中导入神经网络模块
from d2l import torch as d2l  # 从 d2l（动手学深度学习）库中导入 torch 模块
```

## 通过后端异步处理
作为热身，考虑一个简单问题：生成一个随机矩阵并将其相乘。让我们在NumPy和PyTorch张量中都这样做，看看它们的区别。请注意，PyTorch的`tensor`是在GPU上定义的。

```python
# GPU计算热身
# 获取可用的 GPU 设备（如果有的话）
device = d2l.try_gpu()

# 使用 Torch 在 GPU 上生成随机矩阵，并进行矩阵乘法运算
a = torch.randn(size=(1000, 1000), device=device)
b = torch.mm(a, a)

# 使用动手学深度学习库中的 Benchmark 工具，测试 numpy 的性能
with d2l.Benchmark('numpy'):
    for _ in range(10):
        a = numpy.random.normal(size=(1000, 1000))
        b = numpy.dot(a, a)

# 使用动手学深度学习库中的 Benchmark 工具，测试 torch 的性能
with d2l.Benchmark('torch'):
    for _ in range(10):
        a = torch.randn(size=(1000, 1000), device=device)
        b = torch.mm(a, a)

```

通过PyTorch的基准输出比较快了几个数量级。NumPy点积是在CPU上执行的，而PyTorch矩阵乘法是在GPU上执行的，后者的速度要快得多。但巨大的时间差距表明一定还有其他原因。默认情况下，GPU操作在PyTorch中是异步的。强制PyTorch在返回之前完成所有计算，这种强制说明了之前发生的情况：计算是由后端执行，而前端将控制权返回给了Python。

```python
# 使用动手学深度学习库中的 Benchmark 工具，测试以下代码块的性能
with d2l.Benchmark():
    for _ in range(10):
        # 在 GPU 上生成随机矩阵并进行矩阵乘法
        a = torch.randn(size=(1000, 1000), device=device)
        b = torch.mm(a, a)
    
    # 在测试完成后，同步 GPU，确保所有操作都已完成
    torch.cuda.synchronize(device)
```

广义上说，PyTorch有一个用于与用户直接交互的前端（例如通过Python），还有一个由系统用来执行计算的后端。用户可以用各种前端语言编写PyTorch程序，如Python和C++。不管使用的前端编程语言是什么，PyTorch程序的执行主要发生在C++实现的后端。由前端语言发出的操作被传递到后端执行。后端管理自己的线程，这些线程不断收集和执行排队的任务。请注意，要使其工作，后端必须能够跟踪计算图中各个步骤之间的依赖关系。因此，不可能并行化相互依赖的操作。

接下来看看另一个简单例子，以便更好地理解依赖关系图。

```python
x = torch.ones((1, 2), device=device)  # 创建一个大小为 (1, 2) 的张量 x，所有元素为1，位于指定的设备上
y = torch.ones((1, 2), device=device)  # 创建一个大小为 (1, 2) 的张量 y，所有元素为1，位于指定的设备上
z = x * y + 2  # 计算张量 x 和 y 的逐元素乘积，并加上标量2，结果赋值给张量 z
z  # 返回张量 z 的值
```

# 自动并行
请注意，本节中的实验至少需要两个GPU来运行。

```python
import torch
from d2l import torch as d2l
```

## 基于GPU的并行计算

从定义一个具有参考性的用于测试的工作负载开始：下面的`run`函数将执行$10$次*矩阵－矩阵*乘法时需要使用的数据分配到两个变量（`x_gpu1`和`x_gpu2`）中，这两个变量分别位于选择的不同设备上。

```python
devices = d2l.try_all_gpus()  # 尝试使用所有可用的GPU设备

def run(x):
    return [x.mm(x) for _ in range(50)]  # 对输入的张量 x 进行50次矩阵乘法操作，并返回结果列表

x_gpu1 = torch.rand(size=(4000, 4000), device=devices[0])  # 在第一个GPU设备上创建一个大小为 (4000, 4000) 的随机张量 x_gpu1
x_gpu2 = torch.rand(size=(4000, 4000), device=devices[1])  # 在第二个GPU设备上创建一个大小为 (4000, 4000) 的随机张量 x_gpu2
```

现在使用函数来处理数据。通过在测量之前需要预热设备（对设备执行一次传递）来确保缓存的作用不影响最终的结果。`torch.cuda.synchronize()`函数将会等待一个CUDA设备上的所有流中的所有核心的计算完成。函数接受一个`device`参数，代表是哪个设备需要同步。如果device参数是`None`（默认值），它将使用`current_device()`找出的当前设备。

```python
run(x_gpu1)  # 运行函数 run，对 x_gpu1 进行预热操作
run(x_gpu2)  # 运行函数 run，对 x_gpu2 进行预热操作
torch.cuda.synchronize(devices[0])  # 同步第一个GPU设备
torch.cuda.synchronize(devices[1])  # 同步第二个GPU设备

with d2l.Benchmark('GPU1 time'):  # 使用 d2l.Benchmark 测量 GPU1 的运行时间
    run(x_gpu1)  # 运行函数 run，对 x_gpu1 进行性能测试
    torch.cuda.synchronize(devices[0])  # 同步第一个GPU设备

with d2l.Benchmark('GPU2 time'):  # 使用 d2l.Benchmark 测量 GPU2 的运行时间
    run(x_gpu2)  # 运行函数 run，对 x_gpu2 进行性能测试
    torch.cuda.synchronize(devices[1])  # 同步第二个GPU设备
```

如果删除两个任务之间的`synchronize`语句，系统就可以在两个设备上自动实现并行计算。

```python
with d2l.Benchmark('GPU1 & GPU2'):  # 使用 d2l.Benchmark 测量 GPU1 和 GPU2 的运行时间
    run(x_gpu1)  # 运行函数 run，对 x_gpu1 进行性能测试
    run(x_gpu2)  # 运行函数 run，对 x_gpu2 进行性能测试
    torch.cuda.synchronize()  # 同步所有GPU设备，确保所有操作均已完成
```

在上述情况下，总执行时间小于两个部分执行时间的总和，因为深度学习框架自动调度两个GPU设备上的计算，而不需要用户编写复杂的代码。
## 并行计算与通信

在许多情况下，我们需要在不同的设备之间移动数据，比如在CPU和GPU之间，或者在不同的GPU之间。例如，当执行分布式优化时，就需要移动数据来聚合多个加速卡上的梯度。让我们通过在GPU上计算，然后将结果复制回CPU来模拟这个过程。

```python
def copy_to_cpu(x, non_blocking=False):    # 将输入列表中的每个张量 y 复制到 CPU 上，可以选择是否使用非阻塞方式
    return [y.to('cpu', non_blocking=non_blocking) for y in x]

with d2l.Benchmark('在GPU1上运行'):  # 使用 d2l.Benchmark 测量在 GPU1 上运行的时间
    y = run(x_gpu1)  # 运行函数 run，在 GPU1 上执行计算
    torch.cuda.synchronize()  # 同步 GPU1

with d2l.Benchmark('复制到CPU'):  # 使用 d2l.Benchmark 测量复制到 CPU 的时间
    y_cpu = copy_to_cpu(y)  # 将 y 复制到 CPU
    torch.cuda.synchronize()  # 同步 GPU，确保复制操作完成
```

这种方式效率不高。注意到当列表中的其余部分还在计算时，我们可能就已经开始将`y`的部分复制到CPU了。例如，当计算一个小批量的（反传）梯度时。某些参数的梯度将比其他参数的梯度更早可用。因此，在GPU仍在运行时就开始使用PCI-Express总线带宽来移动数据是有利的。在PyTorch中，`to()`和`copy_()`等函数都允许显式的`non_blocking`参数，这允许在不需要同步时调用方可以绕过同步。设置`non_blocking=True`以模拟这个场景。

```python
with d2l.Benchmark('在GPU1上运行并复制到CPU'):
    y = run(x_gpu1)  # 在 GPU1 上运行计算并获取结果 y
    y_cpu = copy_to_cpu(y, True)  # 将 y 张量复制到 CPU，并使用非阻塞方式
    torch.cuda.synchronize()  # 确保 GPU 操作的同步
```

# 多GPU训练
## 数据并行性
在实践中请注意，当在$k$个GPU上训练时，需要扩大小批量的大小为$k$的倍数，这样每个GPU都有相同的工作量，就像只在单个GPU上训练一样。因此，在16-GPU服务器上可以显著地增加小批量数据量的大小，同时可能还需要相应地提高学习率。还请注意，批量规范化也需要调整，例如，为每个GPU保留单独的批量规范化参数。

下面我们将使用一个简单网络来演示多GPU训练。

```python
%matplotlib inline
import torch  # 导入 PyTorch 库
from torch import nn  # 导入神经网络模块
from torch.nn import functional as F  # 导入神经网络函数模块
from d2l import torch as d2l  # 导入 d2l 中的 PyTorch 版本
```

## **简单网络**

我们使用介绍的（稍加修改的）LeNet，从零开始定义它，从而详细说明参数交换和同步。

```python
# 初始化模型参数
scale = 0.01
W1 = torch.randn(size=(20, 1, 3, 3)) * scale  # 第一层卷积层权重
b1 = torch.zeros(20)  # 第一层卷积层偏置
W2 = torch.randn(size=(50, 20, 5, 5)) * scale  # 第二层卷积层权重
b2 = torch.zeros(50)  # 第二层卷积层偏置
W3 = torch.randn(size=(800, 128)) * scale  # 第一个全连接层权重
b3 = torch.zeros(128)  # 第一个全连接层偏置
W4 = torch.randn(size=(128, 10)) * scale  # 第二个全连接层权重
b4 = torch.zeros(10)  # 第二个全连接层偏置
params = [W1, b1, W2, b2, W3, b3, W4, b4]

# 定义模型
def lenet(X, params):
    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])  # 第一层卷积
    h1_activation = F.relu(h1_conv)  # 第一层激活函数（ReLU）
    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))  # 第一层平均池化
    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])  # 第二层卷积
    h2_activation = F.relu(h2_conv)  # 第二层激活函数（ReLU）
    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))  # 第二层平均池化
    h2 = h2.reshape(h2.shape[0], -1)  # 将第二层输出展平成一维向量
    h3_linear = torch.mm(h2, params[4]) + params[5]  # 第一个全连接层
    h3 = F.relu(h3_linear)  # 第一个全连接层激活函数（ReLU）
    y_hat = torch.mm(h3, params[6]) + params[7]  # 第二个全连接层
    return y_hat

# 交叉熵损失函数
loss = nn.CrossEntropyLoss(reduction='none')  # 使用交叉熵损失函数，并返回每个样本的损失值
```

## 数据同步

对于高效的多GPU训练，我们需要两个基本操作。首先，我们需要[**向多个设备分发参数**]并附加梯度（`get_params`）。如果没有参数，就不可能在GPU上评估网络。第二，需要跨多个设备对参数求和，也就是说，需要一个`allreduce`函数。

```python
def get_params(params, device):
    new_params = [p.to(device) for p in params]  # 将参数列表中的每个张量移动到指定的设备
    for p in new_params:
        p.requires_grad_()  # 设置每个参数张量需要计算梯度
    return new_params
```

通过将模型参数复制到一个GPU。

```python
new_params = get_params(params, d2l.try_gpu(0))  # 获取在第一个 GPU 设备上的模型参数
print('b1 权重:', new_params[1])  # 打印第一层卷积层的偏置参数（权重）
print('b1 梯度:', new_params[1].grad)  # 打印第一层卷积层的偏置参数的梯度
```

由于还没有进行任何计算，因此权重参数的梯度仍然为零。假设现在有一个向量分布在多个GPU上，下面的[**`allreduce`函数将所有向量相加，并将结果广播给所有GPU**]。请注意，我们需要将数据复制到累积结果的设备，才能使函数正常工作。

```python
def allreduce(data):
    for i in range(1, len(data)):
        data[0][:] += data[i].to(data[0].device)  # 将其他设备上的数据加到第一个设备上
    for i in range(1, len(data)):
        data[i][:] = data[0].to(data[i].device)  # 将第一个设备上的数据同步到其他设备
```

通过在不同设备上创建具有不同值的向量并聚合它们。

```python
data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)] # 创建包含两个张量的列表，每个张量形状为（1, 2），存储在不同的GPU上，值分别为 1 和 2
print('allreduce之前：\n', data[0], '\n', data[1]) # 打印执行allreduce操作前的数据
allreduce(data) # 对数据列表进行allreduce操作，将各GPU上的数据进行全局求和或平均
print('allreduce之后：\n', data[0], '\n', data[1]) # 打印执行allreduce操作后的数据

```

## 数据分发

我们需要一个简单的工具函数，[**将一个小批量数据均匀地分布在多个GPU上**]。例如，有两个GPU时，我们希望每个GPU可以复制一半的数据。因为深度学习框架的内置函数编写代码更方便、更简洁，所以在$4 \times 5$矩阵上使用它进行尝试。

```python
data = torch.arange(20).reshape(4, 5)  # 创建一个形状为 (4, 5) 的张量，数值从 0 到 19
devices = [torch.device('cuda:0'), torch.device('cuda:1')]  # 定义两个 CUDA 设备
split = nn.parallel.scatter(data, devices)  # 将数据张量分散加载到指定的 CUDA 设备上
print('input:', data)  # 打印输入的数据张量
print('load into', devices)  # 打印加载的设备列表
print('output:', split)  # 打印分散加载后的输出结果
```

为了方便以后复用，我们定义了可以同时拆分数据和标签的`split_batch`函数。

```python
#@save
def split_batch(X, y, devices):
    """将X和y拆分到多个设备上"""
    assert X.shape[0] == y.shape[0]  # 断言确保X和y的批次维度相同
    return (nn.parallel.scatter(X, devices),  # 将输入数据X分散加载到指定设备上
            nn.parallel.scatter(y, devices))  # 将标签数据y分散加载到指定设备上
```

## 训练

现在我们可以[**在一个小批量上实现多GPU训练**]。在多个GPU之间同步数据将使用刚才讨论的辅助函数`allreduce`和`split_and_load`。我们不需要编写任何特定的代码来实现并行性。因为计算图在小批量内的设备之间没有任何依赖关系，因此它是“自动地”并行执行。

```python
def train_batch(X, y, device_params, devices, lr):
    X_shards, y_shards = split_batch(X, y, devices)
    # 在每个GPU上分别计算损失
    ls = [loss(lenet(X_shard, device_W), y_shard).sum()  # 计算每个GPU上的损失
          for X_shard, y_shard, device_W in zip(
              X_shards, y_shards, device_params)]
    for l in ls:  # 在每个GPU上分别执行反向传播
        l.backward()
    # 将每个GPU的所有梯度相加，并将其广播到所有GPU
    with torch.no_grad():
        for i in range(len(device_params[0])):
            allreduce(
                [device_params[c][i].grad for c in range(len(devices))])  # 执行梯度聚合和广播
    # 在每个GPU上分别更新模型参数
    for param in device_params:
        d2l.sgd(param, lr, X.shape[0])  # 使用全尺寸的小批量更新模型参数
```

现在，我们可以[**定义训练函数**]。与前几章中略有不同：训练函数需要分配GPU并将所有模型参数复制到所有设备。显然，每个小批量都是使用`train_batch`函数来处理多个GPU。我们只在一个GPU上计算模型的精确度，而让其他GPU保持空闲，尽管这是相对低效的，但是使用方便且代码简洁。

```python
def train(num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  # 加载Fashion MNIST数据集
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]  # 获取num_gpus个GPU设备
    # 将模型参数复制到num_gpus个GPU
    device_params = [get_params(params, d) for d in devices]  # 使用函数get_params复制模型参数到每个GPU
    num_epochs = 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])  # 创建动画记录训练进度
    timer = d2l.Timer()  # 计时器
    for epoch in range(num_epochs):
        timer.start()  # 开始计时
        for X, y in train_iter:  # 遍历训练集小批量数据
            # 为单个小批量执行多GPU训练
            train_batch(X, y, device_params, devices, lr)  # 调用train_batch函数进行多GPU训练
            torch.cuda.synchronize()  # 同步所有GPU，等待所有设备上的当前流操作完成
        timer.stop()  # 停止计时
        # 在GPU0上评估模型
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(
            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))  # 在GPU0上评估模型精度
    # 打印最终测试精度、平均每轮训练时间和使用的设备
    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'
          f'在{str(devices)}')
```

让我们看看[**在单个GPU上运行**]效果得有多好。首先使用的批量大小是$256$，学习率是$0.2$。

```python
train(num_gpus=1, batch_size=256, lr=0.2)  # 调用train函数，使用单个GPU，每个批量大小为256，学习率为0.2
```

保持批量大小和学习率不变，并[**增加为2个GPU**]，我们可以看到测试精度与之前的实验基本相同。不同的GPU个数在算法寻优方面是相同的。不幸的是，这里没有任何有意义的加速：模型实在太小了；而且数据集也太小了。在这个数据集中，我们实现的多GPU训练的简单方法受到了巨大的Python开销的影响。在未来，我们将遇到更复杂的模型和更复杂的并行化方法。尽管如此，让我们看看Fashion-MNIST数据集上会发生什么。

```python
train(num_gpus=2, batch_size=256, lr=0.2)  # 调用train函数，使用两个GPU，每个批量大小为256，学习率为0.2
```

# 多GPU的简洁实现

每个新模型的并行计算都从零开始实现是无趣的。此外，优化同步工具以获得高性能也是有好处的。下面我们将展示如何使用深度学习框架的高级API来实现这一点。本节的代码至少需要两个GPU来运行。

```python
import torch
from torch import nn
from d2l import torch as d2l
```

## **简单网络**

让我们使用一个比LeNet更有意义的网络，它依然能够容易地和快速地训练。我们选择的是ResNet-18。因为输入的图像很小，所以稍微修改了一下。我们在开始时使用了更小的卷积核、步长和填充，而且删除了最大汇聚层。

```python
#@save
def resnet18(num_classes, in_channels=1):
    """稍加修改的ResNet-18模型"""
    
    def resnet_block(in_channels, out_channels, num_residuals,
                     first_block=False):
        """ResNet基本块的堆叠"""
        blk = []
        for i in range(num_residuals):
            if i == 0 and not first_block:
                blk.append(d2l.Residual(in_channels, out_channels,
                                        use_1x1conv=True, strides=2))
            else:
                blk.append(d2l.Residual(out_channels, out_channels))
        return nn.Sequential(*blk)

    # 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层
    net = nn.Sequential(
        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),  # 第一层卷积层
        nn.BatchNorm2d(64),  # 批归一化层
        nn.ReLU())  # 激活函数ReLU
    
    net.add_module("resnet_block1", resnet_block(
        64, 64, 2, first_block=True))  # 第一个ResNet块
    
    net.add_module("resnet_block2", resnet_block(64, 128, 2))  # 第二个ResNet块
    
    net.add_module("resnet_block3", resnet_block(128, 256, 2))  # 第三个ResNet块
    
    net.add_module("resnet_block4", resnet_block(256, 512, 2))  # 第四个ResNet块
    
    net.add_module("global_avg_pool", nn.AdaptiveAvgPool2d((1,1)))  # 全局平均池化层
    
    net.add_module("fc", nn.Sequential(nn.Flatten(),
                                       nn.Linear(512, num_classes)))  # 全连接层
    
    return net
```

## 网络初始化
我们将在训练回路中初始化网络。请参见 4.8节复习初始化方法。

```python
net = resnet18(10)   # 创建一个ResNet-18模型，输出类别数为10
# 获取GPU列表
devices = d2l.try_all_gpus()
# 我们将在训练代码实现中初始化网络
```

## **训练**

如前所述，用于训练的代码需要执行几个基本功能才能实现高效并行：

* 需要在所有设备上初始化网络参数；
* 在数据集上迭代时，要将小批量数据分配到所有设备上；
* 跨设备并行计算损失及其梯度；
* 聚合梯度，并相应地更新参数。

最后，并行地计算精确度和发布网络的最终性能。除了需要拆分和聚合数据外，训练代码与前几章的实现非常相似。

```python
def train(net, num_gpus, batch_size, lr):
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  # 加载数据
    devices = [d2l.try_gpu(i) for i in range(num_gpus)]   # 获取GPU列表
    
    # 初始化模型权重
    def init_weights(m):
        if type(m) in [nn.Linear, nn.Conv2d]:
            nn.init.normal_(m.weight, std=0.01)
    net.apply(init_weights)
    
    # 在多个GPU上设置模型
    net = nn.DataParallel(net, device_ids=devices)  # 使用nn.DataParallel将模型net复制到多个GPU设备上进行并行计算
    
    # 设置优化器和损失函数
    trainer = torch.optim.SGD(net.parameters(), lr)
    loss = nn.CrossEntropyLoss()
    
    timer, num_epochs = d2l.Timer(), 10   # 迭代周期为 10
    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])  # 设置画图
    
    # 训练模型
    for epoch in range(num_epochs):
        net.train()  # 设置模型为训练模式
        timer.start()  # 启动计时器，开始计时当前epoch的训练时间
        for X, y in train_iter:
            trainer.zero_grad()  # 梯度清零，以防止梯度累积
            X, y = X.to(devices[0]), y.to(devices[0])  # 将数据移动到指定的设备（通常是GPU）
            l = loss(net(X), y)  # 计算模型对当前批量数据的损失
            l.backward()  # 反向传播，计算梯度
            trainer.step()  # 根据计算的梯度更新模型参数
        timer.stop()  # 停止计时器，记录当前epoch的训练时间
        
        # 记录并显示训练进度
        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))
    
    # 打印最终测试精度和训练时间
    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'
          f'在{str(devices)}')
```

接下来看看这在实践中是如何运作的。我们先[**在单个GPU上训练网络**]进行预热。

```python
train(net, num_gpus=1, batch_size=256, lr=0.1)  # 调用训练函数，设置使用1个GPU，批量大小为256，学习率为0.1
```

接下来我们[**使用2个GPU进行训练**]。与LeNet相比，ResNet-18的模型要复杂得多。这就是显示并行化优势的地方，计算所需时间明显大于同步参数需要的时间。因为并行化开销的相关性较小，因此这种操作提高了模型的可伸缩性。

```python
train(net, num_gpus=2, batch_size=512, lr=0.2)  # 调用训练函数，设置使用2个GPU，批量大小为512，学习率为0.2
```

