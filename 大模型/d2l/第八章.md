# 序列模型
## 训练

在了解了上述统计工具后，让我们在实践中尝试一下！首先，我们生成一些数据：(**使用正弦函数和一些可加性噪声来生成序列数据，时间步为$1, 2, \ldots, 1000$。**)

```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l
```

```python
T = 1000  # 总共产生1000个点
time = torch.arange(1, T + 1, dtype=torch.float32)  # 生成时间序列
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))  # 根据正弦函数生成x，并添加正态分布噪声
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))  # 绘制时间序列x的图形
```

接下来，我们将这个序列转换为模型的*特征－标签*（feature-label）对。基于嵌入维度$\tau$，我们[**将数据映射为数据对$y_t = x_t$和$\mathbf{x}_t = [x_{t-\tau}, \ldots, x_{t-1}]$。**]这比我们提供的数据样本少了$\tau$个，因为我们没有足够的历史记录来描述前$\tau$个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列。在这里，我们仅使用前600个“特征－标签”对进行训练。

```python
tau = 4  # 设置时间延迟为4

features = torch.zeros((T - tau, tau))  # 初始化特征矩阵
for i in range(tau):
    features[:, i] = x[i: T - tau + i]  # 构建特征矩阵，每一列为x在不同时间延迟下的值

labels = x[tau:].reshape((-1, 1))  # 设置标签，使用x的延迟后的值作为标签
```

```python
batch_size, n_train = 16, 600  # 批量大小和训练集大小

# 加载训练数据集，只使用前n_train个样本进行训练
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
                            batch_size, is_train=True)
```

在这里，我们[**使用一个相当简单的架构训练模型：一个拥有两个全连接层的多层感知机**]，ReLU激活函数和平方损失。

```python
# 初始化网络权重的函数
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)  # 使用Xavier均匀分布初始化线性层的权重

# 一个简单的多层感知机
def get_net():
    net = nn.Sequential(nn.Linear(4, 10),  # 输入大小为4，输出大小为10的线性层
                        nn.ReLU(),         # ReLU激活函数
                        nn.Linear(10, 1))   # 输入大小为10，输出大小为1的线性层
    net.apply(init_weights)  # 对网络应用初始化权重的函数
    return net

# 平方损失。注意：MSELoss计算平方误差时不带系数1/2
loss = nn.MSELoss(reduction='none')  # 使用平方损失函数，reduction='none'表示不对批次求平均
```

现在，准备[**训练模型**]了。

```python
def train(net, train_iter, loss, epochs, lr):
    trainer = torch.optim.Adam(net.parameters(), lr)  # 使用Adam优化器，学习率为lr
    for epoch in range(epochs):  # 迭代epochs次
        for X, y in train_iter:  # 遍历训练数据迭代器
            trainer.zero_grad()  # 梯度清零
            l = loss(net(X), y)  # 计算损失
            l.sum().backward()  # 损失求和并反向传播
            trainer.step()  # 更新模型参数
        # 打印当前epoch的损失
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):.6f}')

net = get_net()  # 获取定义好的神经网络模型
train(net, train_iter, loss, 5, 0.01)  # 训练模型，共迭代5个epochs，学习率为0.01
```

## 预测

由于训练损失很小，因此我们期望模型能有很好的工作效果。让我们看看这在实践中意味着什么。首先是检查[**模型预测下一个时间步**]的能力，也就是*单步预测*（one-step-ahead prediction）。

```python
onestep_preds = net(features)  # 使用训练好的神经网络模型进行一步预测

# 使用d2l.plot进行数据可视化
d2l.plot([time, time[tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000],
         figsize=(6, 3))
```

通常，对于直到$x_t$的观测序列，其在时间步$t+k$处的预测输出$\hat{x}_{t+k}$称为$k$*步预测*（$k$-step-ahead-prediction）。由于我们的观察已经到了$x_{604}$，它的$k$步预测是$\hat{x}_{604+k}$。换句话说，我们必须使用我们自己的预测（而不是原始数据）来[**进行多步预测**]。让我们看看效果如何。

```python
multistep_preds = torch.zeros(T)  # 创建一个长度为T的张量，用于存储多步预测结果
multistep_preds[: n_train + tau] = x[: n_train + tau]  # 将已知的数据 x[: n_train + tau] 复制到 multistep_preds 中

for i in range(n_train + tau, T):  # 从 n_train + tau 开始遍历到 T-1
    # 使用神经网络模型 net 进行预测，输入为 multistep_preds 中前 tau 个时间点的数据，reshape成(1, -1)的形状
    multistep_preds[i] = net(multistep_preds[i - tau:i].reshape((1, -1)))
```

```python
d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy(),
          multistep_preds[n_train + tau:].detach().numpy()], 'time',
         'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))
```

如上面的例子所示，绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。为什么这个算法效果这么差呢？事实是由于错误的累积：假设在步骤$1$之后，我们积累了一些错误$\epsilon_1 = \bar\epsilon$。
于是，步骤$2$的输入被扰动了$\epsilon_1$，结果积累的误差是依照次序的$\epsilon_2 = \bar\epsilon + c \epsilon_1$，其中$c$为某个常数，后面的预测误差依此类推。因此误差可能会相当快地偏离真实的观测结果。例如，未来$24$小时的天气预报往往相当准确，但超过这一点，精度就会迅速下降。我们将在本章及后续章节中讨论如何改进这一点。

基于$k = 1, 4, 16, 64$，通过对整个序列预测的计算，让我们[**更仔细地看一下$k$步预测**]的困难。

```python
max_steps = 64

features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))
# 列i（i<tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）
for i in range(tau):
    features[:, i] = x[i: i + T - tau - max_steps + 1]

# 列i（i>=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）
for i in range(tau, tau + max_steps):
    features[:, i] = net(features[:, i - tau:i]).reshape(-1)
```

```python
steps = (1, 4, 16, 64)
d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],  # 提取不同步长下的时间数据片段
         [features[:, (tau + i - 1)].detach().numpy() for i in steps],  # 提取不同步长下的特征数据并转换为 NumPy 数组
         'time',  # x 轴标签为时间
         'x',  # y 轴标签为 x
         legend=[f'{i}-step preds' for i in steps],  # 图例，显示每个步长预测结果的步数
         xlim=[5, 1000],  # 设置 x 轴显示范围
         figsize=(6, 3))  # 设置图形尺寸为 6x3 英寸
```

# 文本预处理
本节中，我们将解析文本的常见预处理步骤。这些步骤通常包括：

1. 将文本作为字符串加载到内存中。
1. 将字符串拆分为词元（如单词和字符）。
1. 建立一个词表，将拆分的词元映射到数字索引。
1. 将文本转换为数字索引序列，方便模型操作。

```python
import collections  # 导入 collections 模块，用于处理集合类数据结构
import re  # 导入 re 模块，用于处理正则表达式
from d2l import torch as d2l  # 从 d2l 库中导入 torch 模块，并使用 d2l 作为别名
```

## 读取数据集

首先，我们从H.G.Well的[时光机器](https://www.gutenberg.org/ebooks/35)中加载文本。这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，而现实中的文档集合可能会包含数十亿个单词。下面的函数(**将数据集读取到由多条文本行组成的列表中**)，其中每条文本行都是一个字符串。为简单起见，我们在这里忽略了标点符号和字母大写。

```python
#@save
d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():  #@save
    """将时间机器数据集加载到文本行的列表中"""
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()   # 读取文件中的所有行到列表 lines 中
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]  # 使用正则表达式去除非字母字符并转换为小写后返回每行文本列表

lines = read_time_machine()
print(f'# 文本总行数: {len(lines)}')
print(lines[0])
print(lines[10])
```

## 词元化

下面的`tokenize`函数将文本行列表（`lines`）作为输入，列表中的每个元素是一个文本序列（如一条文本行）。[**每个文本序列又被拆分成一个词元列表**]，*词元*（token）是文本的基本单位。最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。

```python
def tokenize(lines, token='word'):  #@save
    """将文本行拆分为单词或字符词元"""
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
```

## 词表

词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。现在，让我们[**构建一个字典，通常也叫做*词表*（vocabulary），用来将字符串类型的词元映射到从$0$开始的数字索引中**]。我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为*语料*（corpus）。然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“&lt;unk&gt;”。我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元（“&lt;pad&gt;”）；序列开始词元（“&lt;bos&gt;”）；序列结束词元（“&lt;eos&gt;”）。

```python
class Vocab:  #@save
    """文本词表"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                   reverse=True)
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):  # 未知词元的索引为0
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):  #@save
    """统计词元的频率"""
    # 这里的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成一个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)
```

我们首先使用时光机器数据集作为语料库来[**构建词表**]，然后打印前几个高频词元及其索引。

```python
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[:10])
```

现在，我们可以(**将每一条文本行转换成一个数字索引列表**)。

```python
for i in [0, 10]:
    print('文本:', tokens[i])
    print('索引:', vocab[tokens[i]])
```

## 整合所有功能

在使用上述函数时，我们[**将所有功能打包到`load_corpus_time_machine`函数中**]，该函数返回`corpus`（词元索引列表）和`vocab`（时光机器语料库的词表）。我们在这里所做的改变是：

1. 为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化；
1. 时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的`corpus`仅处理为单个列表，而不是使用多词元列表构成的一个列表。

```python
def load_corpus_time_machine(max_tokens=-1):  #@save
    """返回时光机器数据集的词元索引列表和词表"""
    lines = read_time_machine()  # 读取时光机器数据集的文本行
    tokens = tokenize(lines, 'char')  # 对文本行进行字符级别的分词
    vocab = Vocab(tokens)  # 创建词表对象
    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，
    # 所以将所有文本行展平到一个列表中
    corpus = [vocab[token] for line in tokens for token in line]  # 将文本行转换为词元索引列表
    if max_tokens > 0:
        corpus = corpus[:max_tokens]  # 截取指定数量的词元索引
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)  # 返回加载后的语料库大小和词表大小
```

# 语言模型和数据集
## 自然语言统计

我们看看在真实数据上如果进行自然语言统计。根据 :numref:`sec_text_preprocessing`中介绍的时光机器数据集构建词表，并打印前$10$个最常用的（频率最高的）单词。

```python
import random
import torch
from d2l import torch as d2l
```

```python
# 对文本进行分词，并将所有行的分词结果拼接成一个词元列表
tokens = d2l.tokenize(d2l.read_time_machine())
# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起
corpus = [token for line in tokens for token in line]

# 根据词元列表创建词汇表
vocab = d2l.Vocab(corpus)

# 打印出现频率最高的前10个词元及其频率
vocab.token_freqs[:10]
```

正如我们所看到的，(**最流行的词**)看起来很无聊，这些词通常(**被称为*停用词***)（stop words），因此可以被过滤掉。尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。此外，还有个明显的问题是词频衰减的速度相当地快。例如，最常用单词的词频对比，第$10$个还不到第$1$个的$1/5$。为了更好地理解，我们可以[**画出的词频图**]：

```python
freqs = [freq for token, freq in vocab.token_freqs]  # 从词汇表中获取每个词的频率列表
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',  # 绘制频率分布图，横轴为词汇，纵轴为频率
         xscale='log', yscale='log')  # 设置对数尺度显示横纵坐标
```

这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。那么[**其他的词元组合，比如二元语法、三元语法等等，又会如何呢？**]我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。

```python
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]  # 生成由相邻词对组成的列表
bigram_vocab = d2l.Vocab(bigram_tokens)  # 创建包含相邻词对的词汇表对象
bigram_vocab.token_freqs[:10]  # 显示前10个最常见的相邻词对及其频率
```

这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的，只有一个与“the time”有关。我们再进一步看看三元语法的频率是否表现出相同的行为方式。

```python
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]  # 生成由连续三个词组成的列表
trigram_vocab = d2l.Vocab(trigram_tokens)  # 创建包含连续三个词的词汇表对象
trigram_vocab.token_freqs[:10]  # 显示前10个最常见的连续三个词及其频率
```

最后，我们[**直观地对比三种模型中的词元频率**]：一元语法、二元语法和三元语法。

```python
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]  # 提取二元组词频列表
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]  # 提取三元组词频列表
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',  # 绘制频率分布图，横轴为词汇，纵轴为频率
         ylabel='frequency: n(x)', xscale='log', yscale='log',  # 设置对数尺度显示横纵坐标
         legend=['unigram', 'bigram', 'trigram'])  # 添加图例，分别表示单词、相邻词对、连续三词的频率分布
```

## 读取长序列数据
当序列变得太长而不能被模型一次性全部处理时，我们可能希望拆分这样的序列方便模型读取。
### 随机采样

(**在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。**)在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。

下面的代码每次可以从数据中随机生成一个小批量。在这里，参数`batch_size`指定了每个小批量中子序列样本的数目，参数`num_steps`是每个子序列中预定义的时间步数。

```python
def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """使用随机抽样生成一个小批量子序列"""
    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
    corpus = corpus[random.randint(0, num_steps - 1):]
    # 减去1，是因为我们需要考虑标签
    num_subseqs = (len(corpus) - 1) // num_steps
    # 长度为num_steps的子序列的起始索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # 在随机抽样的迭代过程中，
    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
    random.shuffle(initial_indices)  # 将序列类型的数据（如列表）中的元素随机排序

    def data(pos):
        # 返回从pos位置开始的长度为num_steps的序列
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # 在这里，initial_indices包含子序列的随机起始索引
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
```

下面我们[**生成一个从$0$到$34$的序列**]。
假设批量大小为$2$，时间步数为$5$，这意味着可以生成$\lfloor (35 - 1) / 5 \rfloor= 6$个“特征－标签”子序列对。如果设置小批量大小为$2$，我们只能得到$3$个小批量。

```python
my_seq = list(range(35))  # 创建一个包含数字0到34的列表

# 使用seq_data_iter_random函数生成随机批次的数据，每个批次包含2个样本，每个样本有5个时间步
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)  # 打印每个批次的输入X和对应的输出Y
```

### 顺序分区

在迭代过程中，除了对原始序列可以随机抽样外，我们还可以[**保证两个相邻的小批量中的子序列在原始序列上也是相邻的**]。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。

```python
def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save
    """使用顺序分区生成一个小批量子序列"""
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)  # 随机选择一个偏移量，范围在0到num_steps之间
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size  # 计算可以生成的样本数量
    Xs = torch.tensor(corpus[offset: offset + num_tokens])  # 创建输入张量Xs，从corpus中截取样本
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])  # 创建输出张量Ys，从corpus中截取对应的输出
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)  # 将Xs和Ys重塑为(batch_size, -1)的形状
    num_batches = Xs.shape[1] // num_steps  # 计算每个批次的时间步数
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]  # 生成输入X的批次
        Y = Ys[:, i: i + num_steps]  # 生成输出Y的批次
        yield X, Y  # 返回X和Y作为一个批次的数据
```

基于相同的设置，通过顺序分区[**读取每个小批量的子序列的特征`X`和标签`Y`**]。通过将它们打印出来可以发现：迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。

```python
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

现在，我们[**将上面的两个采样函数包装到一个类中**]，以便稍后可以将其用作数据迭代器。

```python
class SeqDataLoader:  #@save
    """加载序列数据的迭代器"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        # 根据 use_random_iter 参数选择迭代器函数
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random  # 使用随机迭代器函数
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential  # 使用顺序迭代器函数
        
        # 加载语料库并创建词汇表
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)  # 加载语料库和词汇表
        self.batch_size, self.num_steps = batch_size, num_steps  # 设置批量大小和时间步数

    def __iter__(self):
        # 返回使用选择的迭代器函数生成的迭代器
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)  # 返回数据迭代器
```

[**最后，我们定义了一个函数`load_data_time_machine`，它同时返回数据迭代器和词表**]，因此可以与其他带有`load_data`前缀的函数如 :numref:`sec_fashion_mnist`中定义的`d2l.load_data_fashion_mnist`）类似地使用。

```python
def load_data_time_machine(batch_size, num_steps,  #@save
                           use_random_iter=False, max_tokens=10000):
    """返回时光机器数据集的迭代器和词表"""
    # 创建 SeqDataLoader 实例
    data_iter = SeqDataLoader(
        batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab  # 返回数据迭代器和词汇表
```

# 循环神经网络的从零开始实现

本节将根据 :numref:`sec_rnn`中的描述，从头开始基于循环神经网络实现字符级语言模型。这样的模型将在H.G.Wells的时光机器数据集上训练。和前面 :numref:`sec_language_model`中介绍过的一样，我们先读取数据集。

```python
%matplotlib inline
import math  # 导入math模块，提供数学运算函数
import torch  # 导入PyTorch库
from torch import nn  # 导入神经网络模块
from torch.nn import functional as F  # 导入神经网络的函数模块，使用别名F
from d2l import torch as d2l  # 导入"Dive into Deep Learning"（D2L）书籍的PyTorch实现库
```

```python
batch_size, num_steps = 32, 35  # 定义批量大小和时间步数
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)  # 载入时间机器数据集并创建训练迭代器和词汇表
```

## **独热编码**

回想一下，在`train_iter`中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为*独热编码*（one-hot encoding），它在 :numref:`subsec_classification-problem`中介绍过。

简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为$N$（即`len(vocab)`），词元索引的范围为$0$到$N-1$。如果词元的索引是整数$i$，那么我们将创建一个长度为$N$的全$0$向量，并将第$i$处的元素设置为$1$。此向量是原始词元的一个独热向量。索引为$0$和$2$的独热向量如下所示：

```python
F.one_hot(torch.tensor([0, 2]), len(vocab))  # 使用F.one_hot函数将索引张量[0, 2]转换为独热编码，编码长度为词汇表的大小（vocab）
```

我们每次采样的(**小批量数据形状是二维张量：（批量大小，时间步数）。**)`one_hot`函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（`len(vocab)`）。我们经常转换输入的维度，以便获得形状为（时间步数，批量大小，词表大小）的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态。

```python
X = torch.arange(10).reshape((2, 5))  # 创建张量 X，形状为 (2, 5)
F.one_hot(X.T, 28).shape  # 将 X 转置后的每列视作一个索引张量，使用 F.one_hot 函数将每列转换为独热编码，编码长度为 28，返回结果的形状
```

## 初始化模型参数

接下来，我们[**初始化循环神经网络模型的模型参数**]。隐藏单元数`num_hiddens`是一个可调的超参数。当训练语言模型时，输入和输出来自相同的词表。因此，它们具有相同的维度，即词表的大小。

```python
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01  # 定义一个函数，返回一个符合正态分布的张量，形状为shape，设备为device

    # 隐藏层参数
    W_xh = normal((num_inputs, num_hiddens))  # 输入到隐藏层的权重矩阵，形状为(num_inputs, num_hiddens)
    W_hh = normal((num_hiddens, num_hiddens))  # 隐藏层到隐藏层的权重矩阵，形状为(num_hiddens, num_hiddens)
    b_h = torch.zeros(num_hiddens, device=device)  # 隐藏层的偏置，形状为(num_hiddens,)
    
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))  # 隐藏层到输出层的权重矩阵，形状为(num_hiddens, num_outputs)
    b_q = torch.zeros(num_outputs, device=device)  # 输出层的偏置，形状为(num_outputs,)
    
    # 附加梯度
    # 将所有参数放入列表中，并将它们的梯度要求设置为True
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    
    return params  # 返回所有的参数列表
```

## 循环神经网络模型

为了定义循环神经网络模型，我们首先需要[**一个`init_rnn_state`函数在初始化时返回隐状态**]。这个函数的返回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）。在后面的章节中我们将会遇到隐状态包含多个变量的情况，而使用元组可以更容易地处理些。

```python
def init_rnn_state(batch_size, num_hiddens, device):
    # 返回一个元组，包含一个形状为(batch_size, num_hiddens)的全零张量，表示隐藏状态的初始化
    return (torch.zeros((batch_size, num_hiddens), device=device), )
```

[**下面的`rnn`函数定义了如何在一个时间步内计算隐状态和输出。**]循环神经网络模型通过`inputs`最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态`H`。此外，这里使用$\tanh$函数作为激活函数。如 :numref:`sec_mlp`所述，当元素在实数上满足均匀分布时，$\tanh$函数的平均值为0。

```python
def rnn(inputs, state, params):
    # inputs的形状：(时间步数量，批量大小，词表大小)
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state  # H 是隐藏状态，形状为 (批量大小，隐藏单元数量)
    outputs = []
    
    # X 的形状：(批量大小，词表大小)
    for X in inputs:
        # 计算当前时间步的隐藏状态
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        # 计算当前时间步的输出
        Y = torch.mm(H, W_hq) + b_q
        # 将当前时间步的输出添加到输出列表中
        outputs.append(Y)
    
    # 将所有时间步的输出连接起来，沿着时间步维度（第0维）进行连接
    return torch.cat(outputs, dim=0), (H,)
```

定义了所有需要的函数之后，接下来我们[**创建一个类来包装这些函数**]，并存储从零开始实现的循环神经网络模型的参数。

```python
class RNNModelScratch: #@save
    """从零开始实现的循环神经网络模型"""
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        # 初始化模型参数
        self.params = get_params(vocab_size, num_hiddens, device)
        # 初始化隐藏状态的函数和前向传播的函数
        self.init_state, self.forward_fn = init_state, forward_fn

    def __call__(self, X, state):
        # 将输入转换为 one-hot 编码
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        # 调用前向传播函数进行计算
        return self.forward_fn(X, state, self.params)

    def begin_state(self, batch_size, device):
        # 返回初始化的隐藏状态
        return self.init_state(batch_size, self.num_hiddens, device)
```

让我们[**检查输出是否具有正确的形状**]。例如，隐状态的维数是否保持不变。

```python
num_hiddens = 512  # 定义隐藏单元的数量

# 创建一个 RNNModelScratch 类的实例 net
net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,
                      init_rnn_state, rnn)

# 初始化模型的隐藏状态
state = net.begin_state(X.shape[0], d2l.try_gpu())

# 将输入数据 X 移动到 GPU 上，并传入网络进行计算
Y, new_state = net(X.to(d2l.try_gpu()), state)

# 打印输出结果的形状、新隐藏状态的长度以及第一个隐藏状态的形状
Y.shape, len(new_state), new_state[0].shape
```

我们可以看到输出形状是（时间步数$\times$批量大小，词表大小），而隐状态形状保持不变，即（批量大小，隐藏单元数）。

## 预测

让我们[**首先定义预测函数来生成`prefix`之后的新字符**]，其中的`prefix`是一个用户提供的包含多个字符的字符串。在循环遍历`prefix`中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为*预热*（warm-up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。

```python
def predict_ch8(prefix, num_preds, net, vocab, device):   #@save
    """在prefix后面生成新字符"""
    # 初始化模型的隐藏状态
    state = net.begin_state(batch_size=1, device=device)
    # 将prefix的第一个字符转换为索引，并添加到outputs中
    outputs = [vocab[prefix[0]]]
    # 定义一个函数，用于获取下一个输入字符
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))

    # 预热期：利用prefix的字符来预热模型的状态
    for y in prefix[1:]:
        # 调用模型进行预测，并更新状态
        _, state = net(get_input(), state)
        # 将当前字符的索引添加到outputs中
        outputs.append(vocab[y])

    # 预测num_preds步：生成接下来的num_preds个字符
    for _ in range(num_preds):
        # 调用模型进行预测，并更新状态
        y, state = net(get_input(), state)
        # 将预测的字符索引添加到outputs中
        outputs.append(int(y.argmax(dim=1).reshape(1)))

    # 将索引转换为实际的字符，并返回生成的文本
    return ''.join([vocab.idx_to_token[i] for i in outputs])
```

现在我们可以测试`predict_ch8`函数。我们将前缀指定为`time traveller `，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。

```python
predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())
```

## **梯度裁剪**
下面我们定义一个函数来裁剪模型的梯度，模型是从零开始实现的模型或由高级API构建的模型。我们在此计算了所有模型参数的梯度的范数。

```python
def grad_clipping(net, theta):  #@save
    """裁剪梯度"""
    # 检查网络是否为 nn.Module 类型，获取需要计算梯度的参数列表
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params  # 如果不是 nn.Module 类型，则从 net.params 中获取参数

    # 计算梯度的 L2 范数
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))

    # 如果梯度的 L2 范数超过阈值 theta，则进行梯度裁剪
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm  # 缩放超过阈值的梯度
```

## 训练

```python
#@save
def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
    """训练网络一个迭代周期（定义见第8章）"""
    state, timer = None, d2l.Timer()  # 初始化状态和计时器
    metric = d2l.Accumulator(2)  # 累加器用于计算训练损失之和和词元数量
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # 在第一次迭代或使用随机抽样时初始化state
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:
            if isinstance(net, nn.Module) and not isinstance(state, tuple):
                # state对于nn.GRU是个张量
                state.detach_()
            else:
                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量
                for s in state:
                    s.detach_()
        
        y = Y.T.reshape(-1)  # 将目标 Y 转置并展平
        X, y = X.to(device), y.to(device)  # 将输入 X 和目标 y 移动到设备上
        y_hat, state = net(X, state)  # 前向传播计算预测和更新状态
        l = loss(y_hat, y.long()).mean()  # 计算损失 l，并取平均
        
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()  # 使用优化器时，先将梯度清零
            l.backward()  # 反向传播计算梯度
            grad_clipping(net, 1)  # 调用梯度裁剪函数
            updater.step()  # 调用优化器的更新步骤
        else:
            l.backward()  # 反向传播计算梯度
            grad_clipping(net, 1)  # 调用梯度裁剪函数
            # 因为已经调用了mean函数
            updater(batch_size=1)  # 执行自定义的更新步骤
        
        metric.add(l * y.numel(), y.numel())  # 更新累加器，累加损失和词元数量
    
    # 返回平均困惑度和每秒处理的词元数量
    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()
```

[**循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现。**]

```python
#@save
def train_ch8(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    """训练模型（定义见第8章）"""
    loss = nn.CrossEntropyLoss()  # 定义交叉熵损失函数
    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
                            legend=['train'], xlim=[10, num_epochs])  # 创建动画绘图对象

    # 初始化
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)  # 使用SGD优化器
    else:
        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)  # 使用自定义SGD函数
    
    # 定义预测函数
    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device) # 接受一个参数 prefix，并调用 predict_ch8 函数进行预测。

    # 训练和预测
    for epoch in range(num_epochs):
        # 运行一个训练周期并返回困惑度和处理速度
        ppl, speed = train_epoch_ch8(
            net, train_iter, loss, updater, device, use_random_iter)

        # 每隔10个周期打印一次预测结果
        if (epoch + 1) % 10 == 0:
            print(predict('time traveller'))
            animator.add(epoch + 1, [ppl])  # 添加数据到动画绘图对象中

    # 打印最终的困惑度和处理速度
    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')
    print(predict('time traveller'))
    print(predict('traveller'))
```

[**现在，我们训练循环神经网络模型。**]因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛。

```python
# 设置训练周期数为500，学习率为1
num_epochs, lr = 500, 1
# 调用train_ch8函数进行模型训练，使用net模型、train_iter数据集迭代器、vocab词汇表、学习率lr、总周期数num_epochs，并尝试在GPU上进行训练
train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())
```

[**最后，让我们检查一下使用随机抽样方法的结果。**]

```python
# 创建一个自定义的循环神经网络模型net，模型参数包括词汇表的大小len(vocab)、隐藏单元数量num_hiddens、GPU设备（如果可用的话）、获取参数的函数get_params、初始化RNN状态的函数init_rnn_state、RNN层次结构rnn
net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,
                      init_rnn_state, rnn)
# 使用train_ch8函数进行模型训练，训练数据集为train_iter，词汇表为vocab，学习率为lr，总周期数为num_epochs，尝试在GPU上进行训练，同时使用随机迭代器进行训练(use_random_iter=True)
train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),
          use_random_iter=True)
```

# 循环神经网络的简洁实现
本节将展示如何使用深度学习框架的高级API提供的函数更有效地实现相同的语言模型。我们仍然从读取时光机器数据集开始。

```python
import torch  # 导入PyTorch库
from torch import nn  # 导入神经网络模块
from torch.nn import functional as F  # 导入函数式接口，并使用别名F
from d2l import torch as d2l  # 导入d2l库中的torch模块，使用别名d2l

batch_size, num_steps = 32, 35  # 定义批量大小和时间步数
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)  # 调用d2l库中的load_data_time_machine函数加载时间机器数据集
```

## [**定义模型**]

高级API提供了循环神经网络的实现。我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层`rnn_layer`。

```python
num_hiddens = 256  # 设置隐藏单元数为256
rnn_layer = nn.RNN(len(vocab), num_hiddens)  # 创建一个RNN层，输入大小为词汇表的长度，隐藏单元数为num_hiddens
```

我们(**使用张量来初始化隐状态**)，它的形状是（隐藏层数，批量大小，隐藏单元数）。

```python
state = torch.zeros((1, batch_size, num_hiddens))  # 创建一个全零张量作为初始状态，形状为(1, batch_size, num_hiddens)
state.shape  # 输出state张量的形状
```

[**通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。**]需要强调的是，`rnn_layer`的“输出”（`Y`）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。

```python
X = torch.rand(size=(num_steps, batch_size, len(vocab)))  # 创建一个随机张量X，形状为(num_steps, batch_size, len(vocab))
Y, state_new = rnn_layer(X, state)  # 使用RNN层rnn_layer处理输入张量X和初始状态state，得到输出Y和更新后的状态state_new
Y.shape, state_new.shape  # 输出Y和state_new的形状
```

[**我们为一个完整的循环神经网络模型定义了一个`RNNModel`类**]。注意，`rnn_layer`只包含隐藏的循环层，我们还需要创建一个单独的输出层。

```python
#@save
class RNNModel(nn.Module):
    """循环神经网络模型"""
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer  # 设置RNN层
        self.vocab_size = vocab_size  # 词汇表大小
        self.num_hiddens = self.rnn.hidden_size  # 隐藏单元大小
        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)  # 全连接层，输入维度为隐藏单元数，输出维度为词汇表大小
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)  # 双向RNN时，输入维度为隐藏单元数乘以2

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)  # 对输入进行one-hot编码，维度为(时间步数, 批量大小, 词汇表大小)
        X = X.to(torch.float32)  # 转换为float32类型
        Y, state = self.rnn(X, state)  # RNN层处理输入X和初始状态state，得到输出Y和新状态state
        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)
        # 它的输出形状是(时间步数*批量大小,词表大小)。
        output = self.linear(Y.reshape((-1, Y.shape[-1])))  # 全连接层的输出
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # nn.GRU以张量作为隐状态
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)
        else:
            # nn.LSTM以元组作为隐状态
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))
```

## 训练与预测

在训练模型之前，让我们[**基于一个具有随机权重的模型进行预测**]。

```python
device = d2l.try_gpu()  # 尝试使用GPU设备
net = RNNModel(rnn_layer, vocab_size=len(vocab))  # 创建RNN模型
net = net.to(device)  # 将模型移动到设备（GPU或CPU）
d2l.predict_ch8('time traveller', 10, net, vocab, device)  # 使用模型进行文本生成
```

接下来，我们使用 上一节中定义的超参数调用`train_ch8`，并且[**使用高级API训练模型**]。

```python
num_epochs, lr = 500, 1  # 设置训练的轮数和学习率
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)  # 使用训练函数进行模型训练
```

