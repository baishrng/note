# 情感分析及数据集
由于情感可以被分类为离散的极性或尺度（例如，积极的和消极的），我们可以将情感分析看作一项文本分类任务，它将可变长度的文本序列转换为固定长度的文本类别。在本章中，我们将使用斯坦福大学的[大型电影评论数据集（large movie review dataset）](https://ai.stanford.edu/~amaas/data/sentiment/)进行情感分析。它由一个训练集和一个测试集组成，其中包含从IMDb下载的25000个电影评论。在这两个数据集中，“积极”和“消极”标签的数量相同，表示不同的情感极性。

```python
import os
import torch
from torch import nn
from d2l import torch as d2l
```

##  读取数据集

首先，下载并提取路径`../data/aclImdb`中的IMDb评论数据集。

```python
#@save
d2l.DATA_HUB['aclImdb'] = (
    'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',
    '01ada507287d82875905620988597833ad4e0903')

# 下载并解压缩数据集'aclImdb'到指定的目录'data_dir'
data_dir = d2l.download_extract('aclImdb', 'aclImdb')
```

接下来，读取训练和测试数据集。每个样本都是一个评论及其标签：1表示“积极”，0表示“消极”。

```python
#@save
def read_imdb(data_dir, is_train):
    """读取IMDb评论数据集文本序列和标签"""
    data, labels = [], []  # 初始化空列表，用于存储数据和标签
    for label in ('pos', 'neg'):  # 遍历正面和负面两种标签
        folder_name = os.path.join(data_dir, 'train' if is_train else 'test', label)  # 构建文件夹路径，区分训练集和测试集
        for file in os.listdir(folder_name):  # 遍历文件夹中的文件
            with open(os.path.join(folder_name, file), 'rb') as f:  # 打开文件
                review = f.read().decode('utf-8').replace('\n', '')  # 读取文件内容并解码为UTF-8格式，移除换行符
                data.append(review)  # 将评论文本添加到数据列表
                labels.append(1 if label == 'pos' else 0)  # 根据标签类型添加相应的标签（正面为1，负面为0）
    return data, labels  # 返回数据和标签列表

# 读取训练集数据
train_data = read_imdb(data_dir, is_train=True)

# 打印训练集的数量
print('训练集数目：', len(train_data[0]))

# 打印前三个样本的标签和部分评论内容
for x, y in zip(train_data[0][:3], train_data[1][:3]):
    print('标签：', y, 'review:', x[0:60])  # 输出标签和评论的部分内容
```

## 预处理数据集

将每个单词作为一个词元，过滤掉出现不到5次的单词，我们从训练数据集中创建一个词表。

```python
train_tokens = d2l.tokenize(train_data[0], token='word')  # 对训练数据集的文本进行分词处理，以单词为单位
vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])  # 基于分词结果构建词汇表，最小词频为5，保留特殊符号'<pad>'
```

在词元化之后，让我们绘制评论词元长度的直方图。

```python
d2l.set_figsize()  # 设置图形大小
d2l.plt.xlabel('# tokens per review')  # 设置X轴标签
d2l.plt.ylabel('count')  # 设置Y轴标签
d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));  # 绘制评论文本长度的直方图，设置50个长度的间隔
```

正如我们所料，评论的长度各不相同。为了每次处理一小批量这样的评论，我们通过截断和填充将每个评论的长度设置为500。这类似于对机器翻译数据集的预处理步骤。

```python
num_steps = 500  # 设定序列长度
train_features = torch.tensor([d2l.truncate_pad(
    vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])  # 对训练数据集的每个文本进行截断或填充，使其长度为500，并转换为PyTorch张量
print(train_features.shape)  # 打印训练特征张量的形状
```

## 创建数据迭代器

现在我们可以创建数据迭代器了。在每次迭代中，都会返回一小批量样本。

```python
train_iter = d2l.load_array((train_features,
    torch.tensor(train_data[1])), 64)  # 使用d2l.load_array函数加载训练数据集和标签，每个小批量大小为64

for X, y in train_iter:  # 迭代训练迭代器
    print('X:', X.shape, ', y:', y.shape)  # 打印每个小批量的特征张量形状和标签张量形状
    break  # 仅打印第一个小批量作为示例

print('小批量数目：', len(train_iter))  # 打印训练迭代器的小批量数目
```

## 整合代码

最后，我们将上述步骤封装到`load_data_imdb`函数中。它返回训练和测试数据迭代器以及IMDb评论数据集的词表。

```python
def load_data_imdb(batch_size, num_steps=500):
    """返回数据迭代器和IMDb评论数据集的词表"""
    data_dir = d2l.download_extract('aclImdb', 'aclImdb')  # 下载和解压IMDb数据集
    train_data = read_imdb(data_dir, True)  # 读取训练数据
    test_data = read_imdb(data_dir, False)  # 读取测试数据
    train_tokens = d2l.tokenize(train_data[0], token='word')  # 将训练文本分词为单词
    test_tokens = d2l.tokenize(test_data[0], token='word')  # 将测试文本分词为单词
    vocab = d2l.Vocab(train_tokens, min_freq=5)  # 建立词汇表，设置最小频率为5
    train_features = torch.tensor([d2l.truncate_pad(
        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])  # 将训练文本转换为张量，并进行截断或填充
    test_features = torch.tensor([d2l.truncate_pad(
        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])  # 将测试文本转换为张量，并进行截断或填充
    train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),  # 加载训练数据迭代器
                                batch_size)
    test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),  # 加载测试数据迭代器
                               batch_size,
                               is_train=False)
    return train_iter, test_iter, vocab  # 返回训练数据迭代器、测试数据迭代器和词汇表
```

# 情感分析：使用循环神经网络

与词相似度和类比任务一样，我们也可以将预先训练的词向量应用于情感分析。由于IMDb评论数据集不是很大，使用在大规模语料库上预训练的文本表示可以减少模型的过拟合。我们将使用预训练的GloVe模型来表示每个词元，并将这些词元表示送入多层双向循环神经网络以获得文本序列表示，该文本序列表示将被转换为情感分析输出。对于相同的下游应用，我们稍后将考虑不同的架构选择。

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 64  # 设置批量大小为64
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)  # 加载IMDb电影评论数据集，获取训练集迭代器、测试集迭代器和词汇表
```

## 使用循环神经网络表示单个文本

在文本分类任务（如情感分析）中，可变长度的文本序列将被转换为固定长度的类别。在下面的`BiRNN`类中，虽然文本序列的每个词元经由嵌入层（`self.embedding`）获得其单独的预训练GloVe表示，但是整个序列由双向循环神经网络（`self.encoder`）编码。更具体地说，双向长短期记忆网络在初始和最终时间步的隐状态（在最后一层）被连结起来作为文本序列的表示。然后，通过一个具有两个输出（“积极”和“消极”）的全连接层（`self.decoder`），将此单一文本表示转换为输出类别。

```python
class BiRNN(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens,
                 num_layers, **kwargs):
        super(BiRNN, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)  # 定义词嵌入层
        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,
                                bidirectional=True)  # 定义双向LSTM编码器
        self.decoder = nn.Linear(4 * num_hiddens, 2)  # 定义线性解码层

    def forward(self, inputs):
        embeddings = self.embedding(inputs.T)  # 获取输入序列的词嵌入表示
        self.encoder.flatten_parameters()  # 展平LSTM参数以提高效率
        outputs, _ = self.encoder(embeddings)  # 将词嵌入输入编码为隐藏状态序列
        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)  # 拼接双向LSTM的前向和后向隐藏状态
        outs = self.decoder(encoding)  # 解码得到最终输出
        return outs

```

让我们构造一个具有两个隐藏层的双向循环神经网络来表示单个文本以进行情感分析。

```python
embed_size, num_hiddens, num_layers = 100, 100, 2  # 定义词嵌入大小、隐藏单元大小和LSTM层数
devices = d2l.try_all_gpus()  # 尝试获取所有可用的GPU设备
net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)  # 创建BiRNN模型实例，传入词汇表大小、词嵌入大小、隐藏单元大小和层数作为参数
```

```python
def init_weights(m):
    if type(m) == nn.Linear:  # 如果是线性层
        nn.init.xavier_uniform_(m.weight)  # 使用Xavier均匀初始化权重
    if type(m) == nn.LSTM:  # 如果是LSTM层
        for param in m._flat_weights_names:  # 遍历所有参数
            if "weight" in param:  # 如果参数名包含 "weight"
                nn.init.xavier_uniform_(m._parameters[param])  # 使用Xavier均匀初始化权重

net.apply(init_weights);  # 对模型应用初始化权重的函数
```

## 加载预训练的词向量

下面，我们为词表中的单词加载预训练的100维（需要与`embed_size`一致）的GloVe嵌入。

```python
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')  # 加载预训练的GloVe词嵌入模型，使用维度为100维的版本
```

打印词表中所有词元向量的形状。

```python
embeds = glove_embedding[vocab.idx_to_token]  # 获取词汇表中每个词对应的GloVe词嵌入
embeds.shape  # 输出嵌入矩阵的形状
```

我们使用这些预训练的词向量来表示评论中的词元，并且在训练期间不要更新这些向量。

```python
net.embedding.weight.data.copy_(embeds)  # 将预训练的词嵌入复制到模型的词嵌入层权重中
net.embedding.weight.requires_grad = False  # 冻结词嵌入层的权重，不计算梯度
```

## 训练和评估模型

现在我们可以训练双向循环神经网络进行情感分析。

```python
lr, num_epochs = 0.01, 5  # 设置学习率和训练轮数
trainer = torch.optim.Adam(net.parameters(), lr=lr)  # 使用Adam优化器来优化模型参数，传入学习率
loss = nn.CrossEntropyLoss(reduction="none")  # 定义损失函数为交叉熵损失，不进行降维
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
    devices)  # 使用train_ch13函数训练模型，传入模型、训练集迭代器、测试集迭代器、损失函数、优化器、训练轮数和设备列表
```

我们定义以下函数来使用训练好的模型`net`预测文本序列的情感。

```python
#@save
def predict_sentiment(net, vocab, sequence):
    """预测文本序列的情感"""
    # 将输入序列按空格分割为单词，并将其转换为词汇表中的索引张量
    sequence = torch.tensor(vocab[sequence.split()], device=d2l.try_gpu())
    
    # 通过网络预测序列的情感标签，获取最大值的索引作为预测的类别
    label = torch.argmax(net(sequence.reshape(1, -1)), dim=1)
    
    # 返回预测的情感标签，如果预测标签为1，则返回'positive'，否则返回'negative'
    return 'positive' if label == 1 else 'negative'
```

最后，让我们使用训练好的模型对两个简单的句子进行情感预测。

```python
predict_sentiment(net, vocab, 'this movie is so great')  # 调用predict_sentiment函数，预测文本序列"this movie is so great"的情感极性

predict_sentiment(net, vocab, 'this movie is so bad')  # 调用predict_sentiment函数，预测文本序列"this movie is so bad"的情感极性
```

# 情感分析：使用卷积神经网络

我们探讨过使用二维卷积神经网络处理二维图像数据的机制，并将其应用于局部特征，如相邻像素。虽然卷积神经网络最初是为计算机视觉设计的，但它也被广泛用于自然语言处理。简单地说，只要将任何文本序列想象成一维图像即可。通过这种方式，一维卷积神经网络可以处理文本中的局部特征，例如$n$元语法。

本节将使用*textCNN*模型来演示如何设计一个表示单个文本的卷积神经网络架构。

```python
import torch  # 导入PyTorch库
from torch import nn  # 从PyTorch库中导入神经网络模块
from d2l import torch as d2l  # 导入d2l库的PyTorch版本，并使用别名d2l

batch_size = 64  # 设置批量大小为64，用于训练和测试数据集
train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)  # 调用d2l库中的函数load_data_imdb(batch_size)加载IMDb数据集，得到训练迭代器、测试迭代器和词汇表
```

## 一维卷积
我们在下面的`corr1d`函数中实现了一维互相关。给定输入张量`X`和核张量`K`，它返回输出张量`Y`。

```python
def corr1d(X, K):
    w = K.shape[0]  # 获取核张量K的长度
    Y = torch.zeros((X.shape[0] - w + 1))  # 初始化输出张量Y的长度
    for i in range(Y.shape[0]):  # 遍历Y中的每个元素
        Y[i] = (X[i: i + w] * K).sum()  # 计算卷积运算结果并存储在Y中
    return Y  # 返回卷积运算的结果张量Y
```

我们可以构造输入张量`X`和核张量`K`来验证上述一维互相关实现的输出。

```python
X, K = torch.tensor([0, 1, 2, 3, 4, 5, 6]), torch.tensor([1, 2])  # 定义输入张量X和卷积核张量K
corr1d(X, K)  # 调用corr1d函数，对输入张量X和卷积核张量K进行一维卷积运算
```

对于任何具有多个通道的一维输入，卷积核需要具有相同数量的输入通道。然后，对于每个通道，对输入的一维张量和卷积核的一维张量执行互相关运算，将所有通道上的结果相加以产生一维输出张量。

我们可以实现多个输入通道的一维互相关运算。

```python
def corr1d_multi_in(X, K):
    # 对每个通道的输入张量和对应的卷积核进行一维卷积并求和
    return sum(corr1d(x, k) for x, k in zip(X, K))

X = torch.tensor([[0, 1, 2, 3, 4, 5, 6],      # 第一个通道的输入张量
                  [1, 2, 3, 4, 5, 6, 7],      # 第二个通道的输入张量
                  [2, 3, 4, 5, 6, 7, 8]])     # 第三个通道的输入张量
K = torch.tensor([[1, 2],                   # 第一个卷积核
                  [3, 4],                   # 第二个卷积核
                  [-1, -3]])                # 第三个卷积核
corr1d_multi_in(X, K)                       # 对输入张量X和卷积核张量K进行多通道一维卷积操作
```

注意，多输入通道的一维互相关等同于单输入通道的二维互相关。
## 最大时间汇聚层

类似地，我们可以使用汇聚层从序列表示中提取最大值，作为跨时间步的最重要特征。textCNN中使用的*最大时间汇聚层*的工作原理类似于一维全局汇聚。对于每个通道在不同时间步存储值的多通道输入，每个通道的输出是该通道的最大值。请注意，最大时间汇聚允许在不同通道上使用不同数量的时间步。
## textCNN模型
### 定义模型

我们在下面的类中实现textCNN模型。与双向循环神经网络模型相比，除了用卷积层代替循环神经网络层外，我们还使用了两个嵌入层：一个是可训练权重，另一个是固定权重。

```python
class TextCNN(nn.Module):
    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,
                 **kwargs):
        super(TextCNN, self).__init__(**kwargs)
        
        # Embedding层，分别为可学习和固定的embedding
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.constant_embedding = nn.Embedding(vocab_size, embed_size)
        
        # Dropout层
        self.dropout = nn.Dropout(0.5)
        
        # 全连接层，输出为2（假设是二分类任务）
        self.decoder = nn.Linear(sum(num_channels), 2)
        
        # 平均池化层
        self.pool = nn.AdaptiveAvgPool1d(1)
        
        # ReLU激活函数
        self.relu = nn.ReLU()
        
        # 多个卷积层，使用ModuleList来存储
        self.convs = nn.ModuleList()
        for c, k in zip(num_channels, kernel_sizes):
            self.convs.append(nn.Conv1d(2 * embed_size, c, k))

    def forward(self, inputs):
        # 获取输入文本的embedding表示
        embeddings = torch.cat((
            self.embedding(inputs), self.constant_embedding(inputs)), dim=2)
        
        # 将通道维度放到第二个位置，适应卷积层的输入要求
        embeddings = embeddings.permute(0, 2, 1)
        
        # 对每个卷积层进行卷积操作、池化操作、ReLU激活，并将结果拼接起来
        encoding = torch.cat([
            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)
            for conv in self.convs], dim=1)
        
        # 使用dropout后，通过全连接层进行分类预测
        outputs = self.decoder(self.dropout(encoding))
        
        return outputs
```

让我们创建一个textCNN实例。它有3个卷积层，卷积核宽度分别为3、4和5，均有100个输出通道。

```python
embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]  # 设置嵌入大小、卷积核尺寸和通道数
devices = d2l.try_all_gpus()  # 尝试获取所有可用的GPU设备
net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)  # 创建TextCNN模型，传入词汇表大小、嵌入大小、卷积核尺寸和通道数

def init_weights(m):
    if type(m) in (nn.Linear, nn.Conv1d):
        nn.init.xavier_uniform_(m.weight)

net.apply(init_weights);  # 对模型应用初始化权重函数
```

### 加载预训练词向量

我们加载预训练的100维GloVe嵌入作为初始化的词元表示。这些词元表示（嵌入权重）在`embedding`中将被训练，在`constant_embedding`中将被固定。

```python
glove_embedding = d2l.TokenEmbedding('glove.6b.100d')  # 加载预训练的GloVe词嵌入模型，维度为100维
embeds = glove_embedding[vocab.idx_to_token]  # 从词汇表中获取词的嵌入向量
net.embedding.weight.data.copy_(embeds)  # 将TextCNN模型的嵌入层权重初始化为GloVe词嵌入
net.constant_embedding.weight.data.copy_(embeds)  # 将TextCNN模型的常量嵌入层权重初始化为GloVe词嵌入
net.constant_embedding.weight.requires_grad = False  # 冻结TextCNN模型的常量嵌入层权重，不进行梯度更新
```

### 训练和评估模型

现在我们可以训练textCNN模型进行情感分析。

```python
lr, num_epochs = 0.001, 5  # 学习率设为0.001，训练轮数设为5轮
trainer = torch.optim.Adam(net.parameters(), lr=lr)  # 使用Adam优化器来优化TextCNN模型的参数
loss = nn.CrossEntropyLoss(reduction="none")  # 定义交叉熵损失函数，不进行降维
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)  # 调用训练函数train_ch13进行模型训练，传入训练集迭代器、测试集迭代器、损失函数、优化器、训练轮数和设备列表
```

下面，我们使用训练好的模型来预测两个简单句子的情感。

```python
d2l.predict_sentiment(net, vocab, 'this movie is so great')  # 使用定义好的神经网络net和词汇表vocab来预测文本"this movie is so great"的情感倾向
```

```python
d2l.predict_sentiment(net, vocab, 'this movie is so bad')  # 使用定义好的神经网络net和词汇表vocab来预测文本"this movie is so bad"的情感倾向
```

# 自然语言推断与数据集
## 自然语言推断
自然语言推断决定了一对文本序列之间的逻辑关系。这类关系通常分为三种类型：

* *蕴涵*（entailment）：假设可以从前提中推断出来。
* *矛盾*（contradiction）：假设的否定可以从前提中推断出来。
* *中性*（neutral）：所有其他情况。

自然语言推断也被称为识别文本蕴涵任务。
## 斯坦福自然语言推断（SNLI）数据集

[**斯坦福自然语言推断语料库（Stanford Natural Language Inference，SNLI）**]是由500000多个带标签的英语句子对组成的集合 :cite:`Bowman.Angeli.Potts.ea.2015`。我们在路径`../data/snli_1.0`中下载并存储提取的SNLI数据集。

```python
import os
import re
import torch
from torch import nn
from d2l import torch as d2l

#@save
d2l.DATA_HUB['SNLI'] = (
    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',  # SNLI数据集的下载链接
    '9fcde07509c7e87ec61c640c1b2753d9041758e4')  # SNLI数据集的文件哈希值

# data_dir = d2l.download_extract('SNLI')  # 下载并解压SNLI数据集到data_dir目录
data_dir = '..\data\snli_1.0'   # 手动解压
```

### [**读取数据集**]

原始的SNLI数据集包含的信息比我们在实验中真正需要的信息丰富得多。因此，我们定义函数`read_snli`以仅提取数据集的一部分，然后返回前提、假设及其标签的列表。

```python
#@save
def read_snli(data_dir, is_train):
    """将SNLI数据集解析为前提、假设和标签"""
    def extract_text(s):
        # 删除我们不会使用的信息
        s = re.sub('\\(', '', s)  # 删除左括号
        s = re.sub('\\)', '', s)  # 删除右括号
        s = re.sub('\\s{2,}', ' ', s)  # 用一个空格替换两个或多个连续的空格
        return s.strip()  # 去除首尾空格

    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}  # 定义标签集合，对应关系为：蕴涵（entailment）-> 0，矛盾（contradiction）-> 1，中性（neutral）-> 2
    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')  # 根据is_train变量选择训练集或测试集文件路径

    with open(file_name, 'r') as f:
        rows = [row.split('\t') for row in f.readlines()[1:]]  # 读取文件中的每一行（跳过第一行标题行），以制表符分割每行内容并存储为列表

    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]  # 提取前提文本，仅保留标签在label_set中的行
    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]  # 提取假设文本，仅保留标签在label_set中的行
    labels = [label_set[row[0]] for row in rows if row[0] in label_set]  # 提取标签，仅保留标签在label_set中的行

    return premises, hypotheses, labels  # 返回提取的前提文本、假设文本和对应标签

```

现在让我们[**打印前3对**]前提和假设，以及它们的标签（“0”“1”和“2”分别对应于“蕴涵”“矛盾”和“中性”）。

```python
train_data = read_snli(data_dir, is_train=True)  # 读取训练集数据

# 打印前三个样本
for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):
    print('前提：', x0)  # 打印前提
    print('假设：', x1)  # 打印假设
    print('标签：', y)   # 打印标签
```

训练集约有550000对，测试集约有10000对。下面显示了训练集和测试集中的三个[**标签“蕴涵”“矛盾”和“中性”是平衡的**]。

```python
test_data = read_snli(data_dir, is_train=False)  # 读取测试集数据

# 统计训练集和测试集中每个类别的样本数量
for data in [train_data, test_data]:
    print([[row for row in data[2]].count(i) for i in range(3)])
```

### [**定义用于加载数据集的类**]

下面我们来定义一个用于加载SNLI数据集的类。类构造函数中的变量`num_steps`指定文本序列的长度，使得每个小批量序列将具有相同的形状。换句话说，在较长序列中的前`num_steps`个标记之后的标记被截断，而特殊标记“&lt;pad&gt;”将被附加到较短的序列后，直到它们的长度变为`num_steps`。通过实现`__getitem__`功能，我们可以任意访问带有索引`idx`的前提、假设和标签。

```python
#@save
class SNLIDataset(torch.utils.data.Dataset):
    """用于加载SNLI数据集的自定义数据集"""
    def __init__(self, dataset, num_steps, vocab=None):
        self.num_steps = num_steps  # 序列长度
        all_premise_tokens = d2l.tokenize(dataset[0])  # 分词化前提句子
        all_hypothesis_tokens = d2l.tokenize(dataset[1])  # 分词化假设句子
        if vocab is None:
            self.vocab = d2l.Vocab(all_premise_tokens + \
                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])  # 构建词汇表
        else:
            self.vocab = vocab
        self.premises = self._pad(all_premise_tokens)  # 填充处理后的前提句子
        self.hypotheses = self._pad(all_hypothesis_tokens)  # 填充处理后的假设句子
        self.labels = torch.tensor(dataset[2])  # 标签
        print('read ' + str(len(self.premises)) + ' examples')  # 打印读取的样本数量

    def _pad(self, lines):
        return torch.tensor([d2l.truncate_pad(
            self.vocab[line], self.num_steps, self.vocab['<pad>'])  # 对句子进行截断和填充
                         for line in lines])

    def __getitem__(self, idx):
        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]  # 返回索引对应的数据和标签

    def __len__(self):
        return len(self.premises)  # 返回数据集的长度
```

### [**整合代码**]

现在，我们可以调用`read_snli`函数和`SNLIDataset`类来下载SNLI数据集，并返回训练集和测试集的`DataLoader`实例，以及训练集的词表。值得注意的是，我们必须使用从训练集构造的词表作为测试集的词表。因此，在训练集中训练的模型将不知道来自测试集的任何新词元。

```python
#@save
def load_data_snli(batch_size, num_steps=50):
    """下载SNLI数据集并返回数据迭代器和词表"""
    num_workers = d2l.get_dataloader_workers()  # 获取数据加载器的工作进程数
#     data_dir = d2l.download_extract('SNLI')  # 下载并解压SNLI数据集
    data_dir = '..\\data\\snli_1.0'   # 手动解压
    train_data = read_snli(data_dir, True)  # 读取训练数据
    test_data = read_snli(data_dir, False)  # 读取测试数据
    train_set = SNLIDataset(train_data, num_steps)  # 创建训练集数据集对象
    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)  # 创建测试集数据集对象，并使用训练集的词表
    train_iter = torch.utils.data.DataLoader(train_set, batch_size,
                                             shuffle=True,
                                             num_workers=num_workers)  # 创建训练集数据迭代器
    test_iter = torch.utils.data.DataLoader(test_set, batch_size,
                                            shuffle=False,
                                            num_workers=num_workers)  # 创建测试集数据迭代器
    return train_iter, test_iter, train_set.vocab  # 返回训练集迭代器、测试集迭代器和词表

```

在这里，我们将批量大小设置为128时，将序列长度设置为50，并调用`load_data_snli`函数来获取数据迭代器和词表。然后我们打印词表大小。

```python
train_iter, test_iter, vocab = load_data_snli(128, 50)  # 调用函数加载SNLI数据集，返回训练集迭代器、测试集迭代器和词表
vocab_size = len(vocab)  # 获取词表的大小，即词汇的数量
```

现在我们打印第一个小批量的形状。与情感分析相反，我们有分别代表前提和假设的两个输入`X[0]`和`X[1]`。

```python
for X, Y in train_iter:
    print(X[0].shape)  # 打印第一个句子的形状
    print(X[1].shape)  # 打印第二个句子的形状
    print(Y.shape)     # 打印标签的形状
    break  # 仅打印第一个批次的数据，然后跳出循环
```

# 自然语言推断：使用注意力
我们介绍了自然语言推断任务和SNLI数据集。鉴于许多模型都是基于复杂而深度的架构，Parikh等人提出用注意力机制解决自然语言推断问题，并称之为“可分解注意力模型”。这使得模型没有循环层或卷积层，在SNLI数据集上以更少的参数实现了当时的最佳结果。本节将描述并实现这种基于注意力的自然语言推断方法（使用MLP）。
## 模型

与保留前提和假设中词元的顺序相比，我们可以将一个文本序列中的词元与另一个文本序列中的每个词元对齐，然后比较和聚合这些信息，以预测前提和假设之间的逻辑关系。与机器翻译中源句和目标句之间的词元对齐类似，前提和假设之间的词元对齐可以通过注意力机制灵活地完成。
使用注意力机制的自然语言推断方法，从高层次上讲，它由三个联合训练的步骤组成：对齐、比较和汇总。我们将在下面一步一步地对它们进行说明。

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l
```

### 注意（Attending）

第一步是将一个文本序列中的词元与另一个序列中的每个词元对齐。假设前提是“我确实需要睡眠”，假设是“我累了”。由于语义上的相似性，我们不妨将假设中的“我”与前提中的“我”对齐，将假设中的“累”与前提中的“睡眠”对齐。同样，我们可能希望将前提中的“我”与假设中的“我”对齐，将前提中的“需要”和“睡眠”与假设中的“累”对齐。请注意，这种对齐是使用加权平均的“软”对齐，其中理想情况下较大的权重与要对齐的词元相关联。

现在，我们更详细地描述使用注意力机制的软对齐。用$\mathbf{A} = (\mathbf{a}_1, \ldots, \mathbf{a}_m)$和$\mathbf{B} = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$表示前提和假设，其词元数量分别为$m$和$n$，其中$\mathbf{a}_i, \mathbf{b}_j \in \mathbb{R}^{d}$（$i = 1, \ldots, m, j = 1, \ldots, n$）是$d$维的词向量。对于软对齐，我们将注意力权重$e_{ij} \in \mathbb{R}$计算为：$$e_{ij} = f(\mathbf{a}_i)^\top f(\mathbf{b}_j),$$其中函数$f$是在下面的`mlp`函数中定义的多层感知机。输出维度$f$由`mlp`的`num_hiddens`参数指定。

```python
def mlp(num_inputs, num_hiddens, flatten):
    net = []  # 初始化网络层列表
    net.append(nn.Dropout(0.2))  # 添加一个20%的Dropout层
    net.append(nn.Linear(num_inputs, num_hiddens))  # 添加一个线性层，输入特征数为num_inputs，输出特征数为num_hiddens
    net.append(nn.ReLU())  # 添加ReLU激活函数
    if flatten:
        net.append(nn.Flatten(start_dim=1))  # 如果需要展平，则添加展平层，从第一个维度开始展平
    net.append(nn.Dropout(0.2))  # 再次添加一个20%的Dropout层
    net.append(nn.Linear(num_hiddens, num_hiddens))  # 添加第二个线性层，输入和输出特征数均为num_hiddens
    net.append(nn.ReLU())  # 添加第二个ReLU激活函数
    if flatten:
        net.append(nn.Flatten(start_dim=1))  # 如果需要展平，则再次添加展平层，从第一个维度开始展平
    return nn.Sequential(*net)  # 返回一个顺序容器，其中包含以上定义的所有层
```

下面，我们定义`Attend`类来计算假设（`beta`）与输入前提`A`的软对齐以及前提（`alpha`）与输入假设`B`的软对齐。

```python
class Attend(nn.Module):
    def __init__(self, num_inputs, num_hiddens, **kwargs):
        super(Attend, self).__init__(**kwargs)
        self.f = mlp(num_inputs, num_hiddens, flatten=False)  # 使用MLP函数定义self.f，用于序列的隐藏表示

    def forward(self, A, B):
        f_A = self.f(A)  # 计算序列A的隐藏表示，形状为（批量大小，序列A的词元数，num_hiddens）
        f_B = self.f(B)  # 计算序列B的隐藏表示，形状为（批量大小，序列B的词元数，num_hiddens）
        e = torch.bmm(f_A, f_B.permute(0, 2, 1))  # 计算注意力分数e，形状为（批量大小，序列A的词元数，序列B的词元数）
        beta = torch.bmm(F.softmax(e, dim=-1), B)  # 将序列B软对齐到序列A的每个词元，形状为（批量大小，序列A的词元数，embed_size）
        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)  # 将序列A软对齐到序列B的每个词元，形状为（批量大小，序列B的词元数，embed_size）
        return beta, alpha
```

### 比较
下面的`Compare`个类定义了比较步骤。

```python
class Compare(nn.Module):
    def __init__(self, num_inputs, num_hiddens, **kwargs):
        super(Compare, self).__init__(**kwargs)
        self.g = mlp(num_inputs, num_hiddens, flatten=False)  # 使用MLP函数定义self.g，用于比较和融合两个输入序列

    def forward(self, A, B, beta, alpha):
        # 使用self.g对序列A和对应的软对齐结果beta进行比较和融合，得到V_A
        V_A = self.g(torch.cat([A, beta], dim=2))
        # 使用self.g对序列B和对应的软对齐结果alpha进行比较和融合，得到V_B
        V_B = self.g(torch.cat([B, alpha], dim=2))
        return V_A, V_B
```

### 聚合
聚合步骤在以下`Aggregate`类中定义。

```python
class Aggregate(nn.Module):
    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):
        super(Aggregate, self).__init__(**kwargs)
        self.h = mlp(num_inputs, num_hiddens, flatten=True)  # 使用MLP函数定义self.h，用于聚合输入向量
        self.linear = nn.Linear(num_hiddens, num_outputs)  # 使用线性层将聚合后的向量映射到输出空间

    def forward(self, V_A, V_B):
        # 对两组比较向量分别求和
        V_A = V_A.sum(dim=1)
        V_B = V_B.sum(dim=1)
        # 将两个求和结果的连结送到多层感知机中
        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))
        return Y_hat
```

### 整合代码

通过将注意步骤、比较步骤和聚合步骤组合在一起，我们定义了可分解注意力模型来联合训练这三个步骤。

```python
class DecomposableAttention(nn.Module):
    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,
                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):
        super(DecomposableAttention, self).__init__(**kwargs)
        self.embedding = nn.Embedding(len(vocab), embed_size)  # 嵌入层，用于将输入索引转换为嵌入向量
        self.attend = Attend(num_inputs_attend, num_hiddens)  # Attend模块，执行注意力机制
        self.compare = Compare(num_inputs_compare, num_hiddens)  # Compare模块，比较和融合两个输入序列
        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)  # Aggregate模块，聚合和映射输入向量

    def forward(self, X):
        premises, hypotheses = X
        A = self.embedding(premises)  # 对前提进行嵌入
        B = self.embedding(hypotheses)  # 对假设进行嵌入
        beta, alpha = self.attend(A, B)  # 执行注意力机制，得到对A和B的软对齐结果beta和alpha
        V_A, V_B = self.compare(A, B, beta, alpha)  # 比较和融合A、B及其软对齐结果beta和alpha
        Y_hat = self.aggregate(V_A, V_B)  # 聚合比较结果V_A和V_B，得到最终预测结果
        return Y_hat  # 返回预测结果
```

## 训练和评估模型

现在，我们将在SNLI数据集上对定义好的可分解注意力模型进行训练和评估。我们从读取数据集开始。

### 读取数据集

我们使用中定义的函数下载并读取SNLI数据集。批量大小和序列长度分别设置为$256$和$50$。

```python
batch_size, num_steps = 256, 50  # 批量大小和序列步数设置为256和50
train_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps)  # 加载SNLI数据集，并设置训练集和测试集迭代器，同时获取词汇表
```

### 创建模型

我们使用预训练好的100维GloVe嵌入来表示输入词元。我们将向量$\mathbf{a}_i$和$\mathbf{b}_j$的维数预定义为100。函数$f$和 函数$g$的输出维度被设置为200.然后我们创建一个模型实例，初始化它的参数，并加载GloVe嵌入来初始化输入词元的向量。

```python
embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()  # 设置嵌入大小、隐藏单元数，并尝试在所有可用 GPU 上运行

net = DecomposableAttention(vocab, embed_size, num_hiddens)  # 创建基于SNLI数据的DecomposableAttention模型

glove_embedding = d2l.TokenEmbedding('glove.6b.100d')  # 加载预训练的GloVe词向量（100维）
embeds = glove_embedding[vocab.idx_to_token]  # 根据词汇表获取词向量的嵌入表示
net.embedding.weight.data.copy_(embeds);  # 将GloVe词向量嵌入复制到模型的嵌入层权重中
```

### 训练和评估模型

我们定义了一个`split_batch_multi_inputs`函数以小批量接受多个输入，如前提和假设。现在我们可以在SNLI数据集上训练和评估模型。

```python
lr, num_epochs = 0.001, 4  # 学习率设为0.001，训练周期数设为4

trainer = torch.optim.Adam(net.parameters(), lr=lr)  # 使用Adam优化器来优化模型参数

loss = nn.CrossEntropyLoss(reduction="none")  # 使用交叉熵损失函数，不进行降维处理

d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
    devices)  # 调用d2l工具包中的train_ch13函数进行模型训练，传入模型、训练集、测试集、损失函数、优化器、训练周期数和设备信息
```

### 使用模型

最后，定义预测函数，输出一对前提和假设之间的逻辑关系。

```python
#@save
def predict_snli(net, vocab, premise, hypothesis):
    """预测前提和假设之间的逻辑关系"""
    net.eval()  # 将模型设置为评估模式
    premise = torch.tensor(vocab[premise], device=d2l.try_gpu())  # 将前提文本转换为对应的索引张量，并放在GPU上（如果可用）
    hypothesis = torch.tensor(vocab[hypothesis], device=d2l.try_gpu())  # 将假设文本转换为对应的索引张量，并放在GPU上（如果可用）
    label = torch.argmax(net([premise.reshape((1, -1)), hypothesis.reshape((1, -1))]), dim=1)  # 使用模型预测前提和假设的逻辑关系
    return 'entailment' if label == 0 else 'contradiction' if label == 1 else 'neutral'  # 根据预测的标签返回相应的逻辑关系类别
```

我们可以使用训练好的模型来获得对示例句子的自然语言推断结果。

```python
predict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'bad', '.'])
```

# 自然语言推断：微调BERT

在本章的前面几节中，我们已经为SNLI数据集上的自然语言推断任务设计了一个基于注意力的结构。现在，我们通过微调BERT来重新审视这项任务。自然语言推断是一个序列级别的文本对分类问题，而微调BERT只需要一个额外的基于多层感知机的架构。

本节将下载一个预训练好的小版本的BERT，然后对其进行微调，以便在SNLI数据集上进行自然语言推断。

```python
import json  # 导入json模块，用于处理JSON数据
import multiprocessing  # 导入multiprocessing模块，用于多进程处理
import os  # 导入os模块，用于操作系统相关的功能
import torch  # 导入torch模块，用于深度学习的计算
from torch import nn  # 从torch模块中导入nn子模块，用于构建神经网络
from d2l import torch as d2l  # 从d2l模块中导入torch子模块，并简化为d2l，便于深度学习相关操作
```

## [**加载预训练的BERT**]

我们已经在WikiText-2数据集上预训练BERT（请注意，原始的BERT模型是在更大的语料库上预训练的）。正如在所讨论的，原始的BERT模型有数以亿计的参数。在下面，我们提供了两个版本的预训练的BERT：“bert.base”与原始的BERT基础模型一样大，需要大量的计算资源才能进行微调，而“bert.small”是一个小版本，以便于演示。

```python
d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',  # 将'bert.base'数据集的URL和校验和添加到DATA_HUB字典中
                             '225d66f04cae318b841a13d32af3acc165f253ac')  # 'bert.base'数据集的校验和

d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',  # 将'bert.small'数据集的URL和校验和添加到DATA_HUB字典中
                              'c72329e68a732bef0452e4b96a1c341c8910f81f')  # 'bert.small'数据集的校验和
```

两个预训练好的BERT模型都包含一个定义词表的“vocab.json”文件和一个预训练参数的“pretrained.params”文件。我们实现了以下`load_pretrained_model`函数来[**加载预先训练好的BERT参数**]。

```python
def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,  # 定义加载预训练模型的函数
                          num_heads, num_layers, dropout, max_len, devices):
    data_dir = d2l.download_extract(pretrained_model)  # 下载并解压预训练模型数据
    
    vocab = d2l.Vocab()  # 创建一个空词表
    vocab.idx_to_token = json.load(open(os.path.join(data_dir,  # 从词汇表文件中加载索引到词的映射
        'vocab.json')))
    vocab.token_to_idx = {token: idx for idx, token in enumerate(  # 创建词到索引的映射
        vocab.idx_to_token)}
    
    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],  # 初始化BERT模型
                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,
                         num_heads=4, num_layers=2, dropout=0.2,
                         max_len=max_len, key_size=256, query_size=256,
                         value_size=256, hid_in_features=256,
                         mlm_in_features=256, nsp_in_features=256)
    
    bert.load_state_dict(torch.load(os.path.join(data_dir,  # 从文件中加载预训练的BERT参数
                                                 'pretrained.params')))
    return bert, vocab  # 返回初始化好的BERT模型和词表
```

为了便于在大多数机器上演示，我们将在本节中加载和微调经过预训练BERT的小版本（“bert.small”）。在练习中，我们将展示如何微调大得多的“bert.base”以显著提高测试精度。

```python
devices = d2l.try_all_gpus()  # 尝试获取所有可用的GPU设备

bert, vocab = load_pretrained_model(  # 调用函数加载预训练的BERT模型和词表
    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,
    num_layers=2, dropout=0.1, max_len=512, devices=devices)
```

## [**微调BERT的数据集**]

对于SNLI数据集的下游任务自然语言推断，我们定义了一个定制的数据集类`SNLIBERTDataset`。在每个样本中，前提和假设形成一对文本序列，并被打包成一个BERT输入序列。片段索引用于区分BERT输入序列中的前提和假设。利用预定义的BERT输入序列的最大长度（`max_len`），持续移除输入文本对中较长文本的最后一个标记，直到满足`max_len`。为了加速生成用于微调BERT的SNLI数据集，我们使用4个工作进程并行生成训练或测试样本。

```python
class SNLIBERTDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, max_len, vocab=None):
        # 处理前提和假设的token列表
        all_premise_hypothesis_tokens = [[
            p_tokens, h_tokens] for p_tokens, h_tokens in zip(
            *[d2l.tokenize([s.lower() for s in sentences])
              for sentences in dataset[:2]])]

        self.labels = torch.tensor(dataset[2])  # 将标签转换为Tensor
        self.vocab = vocab  # 词表
        self.max_len = max_len  # 最大长度
        # 预处理数据，生成token_ids、segments和valid_lens
        (self.all_token_ids, self.all_segments,
         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)
        print('read ' + str(len(self.all_token_ids)) + ' examples')  # 打印读取的样本数量


    def _preprocess(self, all_premise_hypothesis_tokens):
        pool = multiprocessing.Pool(4)  # 使用4个进程进行数据处理
        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)  # 并行处理数据
        all_token_ids = [
            token_ids for token_ids, segments, valid_len in out]  # 提取token_ids
        all_segments = [segments for token_ids, segments, valid_len in out]  # 提取segments
        valid_lens = [valid_len for token_ids, segments, valid_len in out]  # 提取有效长度
        # 将结果转换为Tensor并返回
        return (torch.tensor(all_token_ids, dtype=torch.long),
                torch.tensor(all_segments, dtype=torch.long),
                torch.tensor(valid_lens))

    def _mp_worker(self, premise_hypothesis_tokens):
        p_tokens, h_tokens = premise_hypothesis_tokens  # 解包前提和假设的token
        self._truncate_pair_of_tokens(p_tokens, h_tokens)  # 截断token对以适应最大长度
        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)  # 获取tokens和segments
        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \
                             * (self.max_len - len(tokens))  # 将tokens转换为token_ids并填充
        segments = segments + [0] * (self.max_len - len(segments))  # 填充segments
        valid_len = len(tokens)  # 计算有效长度
        return token_ids, segments, valid_len  # 返回处理后的结果

    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):
        # 保留BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元的位置
        while len(p_tokens) + len(h_tokens) > self.max_len - 3:
            # 如果前提token较多，则从前提中移除最后一个token
            if len(p_tokens) > len(h_tokens):
                p_tokens.pop()
            # 否则，从假设中移除最后一个token
            else:
                h_tokens.pop()

    def __getitem__(self, idx):
        # 根据索引idx从存储的所有数据中提取token_ids、segments、valid_lens和标签
        return (self.all_token_ids[idx], self.all_segments[idx],
                self.valid_lens[idx]), self.labels[idx]

    def __len__(self):
        return len(self.all_token_ids)  # 返回样本数量
```

下载完SNLI数据集后，我们通过实例化`SNLIBERTDataset`类来[**生成训练和测试样本**]。这些样本将在自然语言推断的训练和测试期间进行小批量读取。

```python
# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512
batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()  # 定义批量大小、最大长度和工作线程数

data_dir = d2l.download_extract('SNLI')  # 下载并解压SNLI数据集
train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)  # 创建训练数据集
test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)  # 创建测试数据集

train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)  # 创建训练DataLoader
test_iter = torch.utils.data.DataLoader(test_set, batch_size, num_workers=num_workers)  # 创建测试DataLoader
```

## 微调BERT

用于自然语言推断的微调BERT只需要一个额外的多层感知机，该多层感知机由两个全连接层组成（请参见下面`BERTClassifier`类中的`self.hidden`和`self.output`）。[**这个多层感知机将特殊的“&lt;cls&gt;”词元**]的BERT表示进行了转换，该词元同时编码前提和假设的信息(**为自然语言推断的三个输出**)：蕴涵、矛盾和中性。

```python
class BERTClassifier(nn.Module):  # 定义一个继承自nn.Module的BERT分类器类
    def __init__(self, bert):  # 构造函数，接收一个预训练的BERT模型作为参数
        super(BERTClassifier, self).__init__()  # 调用父类nn.Module的构造函数
        self.encoder = bert.encoder  # 获取BERT模型的编码器部分
        self.hidden = bert.hidden  # 获取BERT模型的隐藏层部分（请确认是否存在）
        self.output = nn.Linear(256, 3)  # 定义线性层，将256维的输入映射到3维（3个类别）

    def forward(self, inputs):  # 前向传播函数
        tokens_X, segments_X, valid_lens_x = inputs  # 解包输入，包括token IDs、分段信息和有效长度
        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)  # 使用BERT编码器处理输入数据
        return self.output(self.hidden(encoded_X[:, 0, :]))  # 通过隐藏层和输出层进行分类预测
```

在下文中，预训练的BERT模型`bert`被送到用于下游应用的`BERTClassifier`实例`net`中。在BERT微调的常见实现中，只有额外的多层感知机（`net.output`）的输出层的参数将从零开始学习。预训练BERT编码器（`net.encoder`）和额外的多层感知机的隐藏层（`net.hidden`）的所有参数都将进行微调。

```python
net = BERTClassifier(bert)  # 实例化BERTClassifier类，并将预训练的BERT模型传递给它
```

回想一下，`MaskLM`类和`NextSentencePred`类在其使用的多层感知机中都有一些参数。这些参数是预训练BERT模型`bert`中参数的一部分，因此是`net`中的参数的一部分。然而，这些参数仅用于计算预训练过程中的遮蔽语言模型损失和下一句预测损失。这两个损失函数与微调下游应用无关，因此当BERT微调时，`MaskLM`和`NextSentencePred`中采用的多层感知机的参数不会更新（陈旧的，staled）。

为了允许具有陈旧梯度的参数，标志`ignore_stale_grad=True`在`step`函数`d2l.train_batch_ch13`中被设置。我们通过该函数使用SNLI的训练集（`train_iter`）和测试集（`test_iter`）对`net`模型进行训练和评估。由于计算资源有限，[**训练**]和测试精度可以进一步提高：我们把对它的讨论留在练习中。

```python
lr, num_epochs = 1e-4, 5  # 设置学习率为0.0001，训练周期为5个epochs
trainer = torch.optim.Adam(net.parameters(), lr=lr)  # 使用Adam优化器，并设置学习率
loss = nn.CrossEntropyLoss(reduction='none')  # 定义交叉熵损失函数，且不进行归约操作
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)  # 调用自定义的训练函数进行模型训练
```

