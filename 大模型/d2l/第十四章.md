# 用于预训练词嵌入的数据集

现在我们已经了解了word2vec模型的技术细节和大致的训练方法，让我们来看看它们的实现。具体地说，我们将以跳元模型和负采样为例。本节从用于预训练词嵌入模型的数据集开始：数据的原始格式将被转换为可以在训练期间迭代的小批量。

```python
import math  # 导入数学模块
import os  # 导入操作系统模块
import random  # 导入随机数模块
import torch  # 导入PyTorch模块
from d2l import torch as d2l  # 从d2l库中导入torch别名为d2l
```

## 读取数据集

我们在这里使用的数据集是[Penn Tree Bank（PTB）](https://catalog.ldc.upenn.edu/LDC99T42)。该语料库取自“华尔街日报”的文章，分为训练集、验证集和测试集。在原始格式中，文本文件的每一行表示由空格分隔的一句话。在这里，我们将每个单词视为一个词元。

```python
#@save
d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',
                       '319d85e578af0cdc590547f26231e4e31cdf1e42')

#@save
def read_ptb():
    """将PTB数据集加载到文本行的列表中"""
    data_dir = d2l.download_extract('ptb')  # 下载并解压PTB数据集
    # 读取训练集数据
    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:
        raw_text = f.read()
    return [line.split() for line in raw_text.split('\n')]  # 返回每行文本分割后的列表

sentences = read_ptb()
f'# sentences数: {len(sentences)}'  # 输出句子数量
```

在读取训练集之后，我们为语料库构建了一个词表，其中出现次数少于10次的任何单词都将由“&lt;unk&gt;”词元替换。请注意，原始数据集还包含表示稀有（未知）单词的“&lt;unk&gt;”词元。

```python
vocab = d2l.Vocab(sentences, min_freq=10)  # 使用句子列表创建词汇表，最小频率设为10
f'vocab size: {len(vocab)}'  # 输出词汇表大小
```

## 下采样

文本数据通常有“the”“a”和“in”等高频词：它们在非常大的语料库中甚至可能出现数十亿次。然而，这些词经常在上下文窗口中与许多不同的词共同出现，提供的有用信息很少。例如，考虑上下文窗口中的词“chip”：直观地说，它与低频单词“intel”的共现比与高频单词“a”的共现在训练中更有用。此外，大量（高频）单词的训练速度很慢。因此，当训练词嵌入模型时，可以对高频单词进行*下采样* 。具体地说，数据集中的每个词$w_i$将有概率地被丢弃
$$ P(w_i) = \max\left(1 - \sqrt{\frac{t}{f(w_i)}}, 0\right),$$
其中$f(w_i)$是$w_i$的词数与数据集中的总词数的比率，常量$t$是超参数（在实验中为$10^{-4}$）。我们可以看到，只有当相对比率$f(w_i) > t$时，（高频）词$w_i$才能被丢弃，且该词的相对比率越高，被丢弃的概率就越大。

```python
#@save
def subsample(sentences, vocab):
    """下采样高频词"""
    # 排除未知词元'<unk>'
    sentences = [[token for token in line if vocab[token] != vocab.unk]
                 for line in sentences]
    counter = d2l.count_corpus(sentences)  # 统计语料库中的词频
    num_tokens = sum(counter.values())  # 计算语料库中的总词数

    # 定义一个函数，用于决定是否保留词元在下采样期间
    def keep(token):
        return (random.uniform(0, 1) <
                math.sqrt(1e-4 / counter[token] * num_tokens))

    # 对每一行进行下采样，只保留符合条件的词元
    return ([[token for token in line if keep(token)] for line in sentences],
            counter)

subsampled, counter = subsample(sentences, vocab)  # 对句子进行下采样
```

下面的代码片段绘制了下采样前后每句话的词元数量的直方图。正如预期的那样，下采样通过删除高频词来显著缩短句子，这将使训练加速。

```python
d2l.show_list_len_pair_hist(
    ['origin', 'subsampled'],  # 图例中显示的两个列表的标签
    '# tokens per sentence',   # x轴标签，表示每个句子中的标记数量
    'count',                  # y轴标签，表示计数
    sentences,                # 原始句子列表
    subsampled               # 经过下采样处理后的句子列表
)
```

对于单个词元，高频词“the”的采样率不到1/20。

```python
def compare_counts(token):
    return (f'"{token}"的数量：'
            f'之前={sum([l.count(token) for l in sentences])}, '  # 计算在原始句子列表中特定词元的总数
            f'之后={sum([l.count(token) for l in subsampled])}')  # 计算在经过下采样处理后的句子列表中特定词元的总数

compare_counts('the')  # 示例：比较词元 "the" 的数量
```

相比之下，低频词“join”则被完全保留。

```python
compare_counts('join')   # 比较词元 "join" 的数量
```

在下采样之后，我们将词元映射到它们在语料库中的索引。

```python
corpus = [vocab[line] for line in subsampled]  # 使用列表推导式将下采样后的句子列表 subsampled 转换为相应的表示形式，存储在 corpus 中
corpus[:3]  # 打印出 corpus 列表中的前三个元素，即转换后的句子表示形式
```

## 中心词和上下文词的提取

下面的`get_centers_and_contexts`函数从`corpus`中提取所有中心词及其上下文词。它随机采样1到`max_window_size`之间的整数作为上下文窗口。对于任一中心词，与其距离不超过采样上下文窗口大小的词为其上下文词。

```python
#@save
def get_centers_and_contexts(corpus, max_window_size):
    """返回跳元模型中的中心词和上下文词"""
    centers, contexts = [], []  # 初始化中心词列表和上下文词列表

    for line in corpus:  # 遍历语料库中的每个句子
        # 要形成“中心词-上下文词”对，每个句子至少需要有2个词
        if len(line) < 2:
            continue
        
        centers += line  # 将句子中的所有词添加到中心词列表

        for i in range(len(line)):  # 遍历句子中的每个词，作为上下文窗口的中心词
            window_size = random.randint(1, max_window_size)  # 随机选择一个上下文窗口大小
            # 计算上下文词的索引范围，确保不超出句子的边界
            indices = list(range(max(0, i - window_size),
                                 min(len(line), i + 1 + window_size)))
            # 从上下文词中排除中心词
            indices.remove(i)
            contexts.append([line[idx] for idx in indices])  # 将形成的上下文词列表添加到上下文列表中

    return centers, contexts  # 返回中心词列表和对应的上下文词列表
```

接下来，我们创建一个人工数据集，分别包含7个和3个单词的两个句子。设置最大上下文窗口大小为2，并打印所有中心词及其上下文词。

```python
tiny_dataset = [list(range(7)), list(range(7, 10))]
print('数据集', tiny_dataset)  # 打印原始数据集

# 遍历跳元模型生成的中心词和上下文词对，打印每个中心词及其对应的上下文词
for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):
    print('中心词', center, '的上下文词是', context)
```

在PTB数据集上进行训练时，我们将最大上下文窗口大小设置为5。下面提取数据集中的所有中心词及其上下文词。

```python
all_centers, all_contexts = get_centers_and_contexts(corpus, 5)
# 使用语料库和窗口大小为5调用函数，获取所有中心词和上下文词对

# 计算所有上下文词对的数量，并生成格式化的字符串
result = f'# “中心词-上下文词对”的数量: {sum([len(contexts) for contexts in all_contexts])}'
```

## 负采样

我们使用负采样进行近似训练。为了根据预定义的分布对噪声词进行采样，我们定义以下`RandomGenerator`类，其中（可能未规范化的）采样分布通过变量`sampling_weights`传递。

```python
#@save
class RandomGenerator:
    """根据n个采样权重在{1,...,n}中随机抽取"""
    def __init__(self, sampling_weights):
        self.population = list(range(1, len(sampling_weights) + 1))  # 初始化总体样本空间
        self.sampling_weights = sampling_weights  # 设置每个样本的采样权重
        self.candidates = []  # 存储随机采样结果的缓存列表
        self.i = 0  # 初始化采样结果索引

    def draw(self):
        if self.i == len(self.candidates):
            # 如果缓存中的采样结果已用尽，重新进行随机采样
            self.candidates = random.choices(
                self.population, self.sampling_weights, k=10000)  # 使用权重进行10000次随机采样
            self.i = 0  # 重置采样结果索引
        self.i += 1  # 更新采样结果索引
        return self.candidates[self.i - 1]  # 返回当前索引位置的采样结果
```

例如，我们可以在索引1、2和3中绘制10个随机变量$X$，采样概率为$P(X=1)=2/9, P(X=2)=3/9$和$P(X=3)=4/9$，如下所示。

```python
#@save
generator = RandomGenerator([2, 3, 4])
# 创建一个RandomGenerator对象，传入采样权重列表[2, 3, 4]

# 进行10次随机抽样并返回结果列表
[generator.draw() for _ in range(10)]
```

对于一对中心词和上下文词，我们随机抽取了`K`个（实验中为5个）噪声词。根据word2vec论文中的建议，将噪声词$w$的采样概率$P(w)$设置为其在字典中的相对频率，其幂为0.75。

```python
#@save
def get_negatives(all_contexts, vocab, counter, K):
    """返回负采样中的噪声词"""
    # 构建每个词的采样权重，根据counter中的词频
    sampling_weights = [counter[vocab.to_tokens(i)] ** 0.75
                        for i in range(1, len(vocab))]
    
    all_negatives = []  # 存储所有的负采样结果
    generator = RandomGenerator(sampling_weights)  # 使用RandomGenerator来进行随机采样
    
    for contexts in all_contexts:
        negatives = []
        while len(negatives) < len(contexts) * K:
            neg = generator.draw()  # 随机抽取一个噪声词
            # 确保噪声词不在上下文中
            if neg not in contexts:
                negatives.append(neg)  # 将合格的噪声词加入列表
        all_negatives.append(negatives)  # 将本次上下文的所有负采样结果加入总列表
    
    return all_negatives  # 返回所有上下文的负采样结果列表

# 调用函数进行负采样，生成所有上下文的负采样结果
all_negatives = get_negatives(all_contexts, vocab, counter, 5)
```

## 小批量加载训练实例

在提取所有中心词及其上下文词和采样噪声词后，将它们转换成小批量的样本，在训练过程中可以迭代加载。

在小批量中，$i^\mathrm{th}$个样本包括中心词及其$n_i$个上下文词和$m_i$个噪声词。由于上下文窗口大小不同，$n_i+m_i$对于不同的$i$是不同的。因此，对于每个样本，我们在`contexts_negatives`个变量中将其上下文词和噪声词连结起来，并填充零，直到连结长度达到$\max_i n_i+m_i$(`max_len`)。为了在计算损失时排除填充，我们定义了掩码变量`masks`。在`masks`中的元素和`contexts_negatives`中的元素之间存在一一对应关系，其中`masks`中的0（否则为1）对应于`contexts_negatives`中的填充。

为了区分正反例，我们在`contexts_negatives`中通过一个`labels`变量将上下文词与噪声词分开。类似于`masks`，在`labels`中的元素和`contexts_negatives`中的元素之间也存在一一对应关系，其中`labels`中的1（否则为0）对应于`contexts_negatives`中的上下文词的正例。

上述思想在下面的`batchify`函数中实现。其输入`data`是长度等于批量大小的列表，其中每个元素是由中心词`center`、其上下文词`context`和其噪声词`negative`组成的样本。此函数返回一个可以在训练期间加载用于计算的小批量，例如包括掩码变量。

```python
#@save
def batchify(data):
    """返回带有负采样的跳元模型的小批量样本"""
    max_len = max(len(c) + len(n) for _, c, n in data)  # 计算当前批次中最长的序列长度
    
    centers, contexts_negatives, masks, labels = [], [], [], []
    
    for center, context, negative in data:
        cur_len = len(context) + len(negative)  # 当前中心词的上下文长度和负采样长度之和
        centers.append(center)  # 将中心词加入列表
        contexts_negatives.append(context + negative + [0] * (max_len - cur_len))  # 合并上下文和负采样，填充到最大长度
        masks.append([1] * cur_len + [0] * (max_len - cur_len))  # 创建掩码，标记有效部分为1，填充部分为0
        labels.append([1] * len(context) + [0] * (max_len - len(context)))  # 创建标签，上下文标记为1，负采样和填充部分标记为0
    
    # 将所有列表转换为PyTorch张量，并返回元组
    return (
        torch.tensor(centers).reshape((-1, 1)),  # 中心词张量，形状为(batch_size, 1)
        torch.tensor(contexts_negatives),  # 上下文和负采样张量，形状为(batch_size, max_len)
        torch.tensor(masks),  # 掩码张量，形状为(batch_size, max_len)
        torch.tensor(labels)  # 标签张量，形状为(batch_size, max_len)
    )
```

让我们使用一个小批量的两个样本来测试此函数。

```python
x_1 = (1, [2, 2], [3, 3, 3, 3])  # 第一个样本，中心词为1，上下文为[2, 2]，负采样为[3, 3, 3, 3]
x_2 = (1, [2, 2, 2], [3, 3])  # 第二个样本，中心词为1，上下文为[2, 2, 2]，负采样为[3, 3]

# 调用batchify函数，将x_1和x_2作为输入组成一个批次
batch = batchify((x_1, x_2))

names = ['centers', 'contexts_negatives', 'masks', 'labels']

# 遍历批次中的每个数据项和对应的名称，并打印出来
for name, data in zip(names, batch):
    print(name, '=', data)
```

## 整合代码

最后，我们定义了读取PTB数据集并返回数据迭代器和词表的`load_data_ptb`函数。

```python
#@save
def load_data_ptb(batch_size, max_window_size, num_noise_words):
    """下载PTB数据集，然后将其加载到内存中"""
    num_workers = d2l.get_dataloader_workers()  # 获取数据加载器的工作线程数
    sentences = read_ptb()  # 从PTB数据集中读取句子
    vocab = d2l.Vocab(sentences, min_freq=10)  # 构建词汇表，最小词频设定为10
    subsampled, counter = subsample(sentences, vocab)  # 对句子进行子采样
    corpus = [vocab[line] for line in subsampled]  # 将子采样后的句子转换为词索引的列表
    all_centers, all_contexts = get_centers_and_contexts(
        corpus, max_window_size)  # 获取所有中心词和上下文
    all_negatives = get_negatives(
        all_contexts, vocab, counter, num_noise_words)  # 获取所有负样本

    class PTBDataset(torch.utils.data.Dataset):
        def __init__(self, centers, contexts, negatives):
            assert len(centers) == len(contexts) == len(negatives)
            self.centers = centers  # 中心词
            self.contexts = contexts  # 上下文
            self.negatives = negatives  # 负样本

        def __getitem__(self, index):
            return (self.centers[index], self.contexts[index],
                    self.negatives[index])  # 返回指定索引的数据项

        def __len__(self):
            return len(self.centers)  # 返回数据集的长度

    dataset = PTBDataset(all_centers, all_contexts, all_negatives)  # 创建数据集对象

    data_iter = torch.utils.data.DataLoader(
        dataset, batch_size, shuffle=True,  # 创建一个用于训练的数据加载器，每批次加载 batch_size 个样本，且每个 epoch 都进行数据洗牌
        collate_fn=batchify, num_workers=num_workers)  # 使用 batchify 函数来处理每个批次的数据，使用 num_workers 个子进程来加速数据加载
    return data_iter, vocab  # 返回数据迭代器和词汇表

```

让我们打印数据迭代器的第一个小批量。

```python
data_iter, vocab = load_data_ptb(512, 5, 5)  # 调用 load_data_ptb 函数加载PTB数据集，设置批次大小为512，最大窗口大小为5，噪声词数量为5
for batch in data_iter:  # 迭代数据迭代器 data_iter 中的每个批次数据
    for name, data in zip(names, batch):  # 遍历批次中的每个数据项与对应的名称
        print(name, 'shape:', data.shape)  # 打印每个数据张量的名称和其形状信息
    break  # 打印完第一个批次后退出循环
```

# 预训练word2vec

我们继续实现跳元语法模型。然后，我们将在PTB数据集上使用负采样预训练word2vec。首先，让我们通过调用`d2l.load_data_ptb`函数来获得该数据集的数据迭代器和词表。

```python
import math  # 导入 math 模块
import torch  # 导入 PyTorch 库
from torch import nn  # 从 PyTorch 导入 nn 模块
from d2l import torch as d2l  # 从 d2l 库中导入 torch 模块并命名为 d2l

batch_size, max_window_size, num_noise_words = 512, 5, 5  # 设置批次大小、最大窗口大小和噪声词数量

# 调用 d2l.load_data_ptb 函数加载 PTB 数据集
data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,
                                     num_noise_words)
```

## 跳元模型

我们通过嵌入层和批量矩阵乘法实现了跳元模型。首先，让我们回顾一下嵌入层是如何工作的。

### 嵌入层

嵌入层将词元的索引映射到其特征向量。该层的权重是一个矩阵，其行数等于字典大小（`input_dim`），列数等于每个标记的向量维数（`output_dim`）。在词嵌入模型训练之后，这个权重就是我们所需要的。

```python
embed = nn.Embedding(num_embeddings=20, embedding_dim=4)  # 创建一个 Embedding 层，设置词汇表大小为20，嵌入维度为4
print(f'Parameter embedding_weight ({embed.weight.shape}, '  # 打印嵌入层的权重张量形状和数据类型信息
      f'dtype={embed.weight.dtype})')
```

嵌入层的输入是词元（词）的索引。对于任何词元索引$i$，其向量表示可以从嵌入层中的权重矩阵的第$i$行获得。由于向量维度（`output_dim`）被设置为4，因此当小批量词元索引的形状为（2，3）时，嵌入层返回具有形状（2，3，4）的向量。

```python
x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 创建一个张量 x，包含两个样本，每个样本有三个词索引
embed(x)  # 将张量 x 传递给 embed 层进行嵌入操作
```

### 定义前向传播

在前向传播中，跳元语法模型的输入包括形状为（批量大小，1）的中心词索引`center`和形状为（批量大小，`max_len`）的上下文与噪声词索引`contexts_and_negatives`。这两个变量首先通过嵌入层从词元索引转换成向量，然后它们的批量矩阵相乘（在 :numref:`subsec_batch_dot`中描述）返回形状为（批量大小，1，`max_len`）的输出。输出中的每个元素是中心词向量和上下文或噪声词向量的点积。

```python
def skip_gram(center, contexts_and_negatives, embed_v, embed_u):
    v = embed_v(center)  # 对中心词进行嵌入，embed_v 是用于中心词嵌入的 Embedding 层
    u = embed_u(contexts_and_negatives)  # 对上下文和负样本进行嵌入，embed_u 是用于上下文和负样本嵌入的 Embedding 层
    pred = torch.bmm(v, u.permute(0, 2, 1))  # 计算预测值，torch.bmm 是批次矩阵乘法，v 是中心词嵌入结果，u 是上下文和负样本嵌入结果
    return pred  # 返回预测值
```

让我们为一些样例输入打印此`skip_gram`函数的输出形状。

```python
skip_gram(torch.ones((2, 1), dtype=torch.long),  # 中心词的索引，形状为 (2, 1)
          torch.ones((2, 4), dtype=torch.long),  # 上下文和负样本的索引，形状为 (2, 4)
          embed,  # 中心词嵌入层
          embed).shape  # 返回预测值的形状
```

## 训练

在训练带负采样的跳元模型之前，我们先定义它的损失函数。

### 二元交叉熵损失

根据负采样损失函数的定义，我们将使用二元交叉熵损失。

```python
class SigmoidBCELoss(nn.Module):
    # 带掩码的二元交叉熵损失
    def __init__(self):
        super().__init__()

    def forward(self, inputs, target, mask=None):
        # 计算带掩码的二元交叉熵损失
        out = nn.functional.binary_cross_entropy_with_logits(
            inputs, target, weight=mask, reduction="none")
        # 沿着第一维度求均值，即对每个样本计算损失的平均值
        return out.mean(dim=1)

loss = SigmoidBCELoss()  # 创建 SigmoidBCELoss 类的实例
```

下面计算给定变量的二进制交叉熵损失。

```python
pred = torch.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)  
# 创建一个形状为 (2, 4) 的张量，表示两个样本的预测值

label = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])  
# 创建一个形状为 (2, 4) 的张量，表示两个样本的真实标签

mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])  
# 创建一个形状为 (2, 4) 的张量，表示两个样本的掩码，用于指示损失函数在计算时要考虑的位置

loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)  
# 计算带掩码的二元交叉熵损失，然后乘以每个样本的特征数，再除以每个样本掩码的总和，得到每个样本的平均损失值
```

下面显示了如何使用二元交叉熵损失中的Sigmoid激活函数（以较低效率的方式）计算上述结果。我们可以将这两个输出视为两个规范化的损失，在非掩码预测上进行平均。

```python
def sigmd(x):
    return -math.log(1 / (1 + math.exp(-x)))  # 定义sigmoid函数

# 计算并打印平均值
print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')  # 对四个数的sigmoid函数值求平均值，保留四位小数并打印
print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')  # 对两个数的sigmoid函数值求平均值，保留四位小数并打印
```

### 初始化模型参数

我们定义了两个嵌入层，将词表中的所有单词分别作为中心词和上下文词使用。字向量维度`embed_size`被设置为100。

```python
embed_size = 100  # 设置嵌入向量的维度为100

# 创建一个Sequential模型，包含两个嵌入层
net = nn.Sequential(
    nn.Embedding(num_embeddings=len(vocab),  # 第一个嵌入层，词汇表大小为vocab的长度，嵌入维度为embed_size
                 embedding_dim=embed_size),
    nn.Embedding(num_embeddings=len(vocab),  # 第二个嵌入层，词汇表大小为vocab的长度，嵌入维度为embed_size
                 embedding_dim=embed_size)
```

### 定义训练阶段代码

训练阶段代码实现定义如下。由于填充的存在，损失函数的计算与以前的训练函数略有不同。

```python
def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):
    def init_weights(m):
        if type(m) == nn.Embedding:
            nn.init.xavier_uniform_(m.weight)  # 使用Xavier初始化方法初始化嵌入层权重

    net.apply(init_weights)  # 应用权重初始化函数到网络中的所有层
    net = net.to(device)  # 将网络移动到指定的设备（GPU或CPU）
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)  # 使用Adam优化器，并传入网络参数和学习率
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, num_epochs])  # 创建动画绘制对象，用于可视化训练过程中的损失变化
    metric = d2l.Accumulator(2)  # 创建一个累加器对象，用于累计损失和标记数

    for epoch in range(num_epochs):  # 迭代每个epoch
        timer, num_batches = d2l.Timer(), len(data_iter)  # 记录时间和获取数据迭代器长度
        for i, batch in enumerate(data_iter):  # 迭代每个数据批次
            optimizer.zero_grad()  # 梯度清零
            center, context_negative, mask, label = [data.to(device) for data in batch]  # 将批次数据移动到指定设备上

            # 调用skip_gram函数得到预测值
            pred = skip_gram(center, context_negative, net[0], net[1])
            
            # 计算损失，这里使用了带掩码的损失函数，并对损失进行规范化
            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)
                     / mask.sum(axis=1) * mask.shape[1])
            
            l.sum().backward()  # 反向传播计算梯度
            optimizer.step()  # 更新模型参数
            metric.add(l.sum(), l.numel())  # 累加损失和标记数

            # 每训练完成num_batches的1/5或者到达最后一个批次时，更新动画图表
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches, (metric[0] / metric[1],))

    # 训练完成后输出训练过程中的平均损失和每秒处理的标记数
    print(f'loss {metric[0] / metric[1]:.3f}, '
          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')
```

现在，我们可以使用负采样来训练跳元模型。

```python
lr, num_epochs = 0.002, 5  # 设置学习率和训练总epoch数
train(net, data_iter, lr, num_epochs)  # 调用训练函数开始模型训练
```

## 应用词嵌入

在训练word2vec模型之后，我们可以使用训练好模型中词向量的余弦相似度来从词表中找到与输入单词语义最相似的单词。

```python
def get_similar_tokens(query_token, k, embed):
    W = embed.weight.data  # 获取嵌入层的权重矩阵
    x = W[vocab[query_token]]  # 获取查询词的词向量
    # 计算余弦相似性。增加1e-9以获得数值稳定性
    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9)
    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')  # 找出余弦相似性最高的前k+1个词的索引
    for i in topk[1:]:  # 遍历除了查询词以外的前k个最相似词
        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')  # 输出相似度和词汇表中对应的词

get_similar_tokens('chip', 3, net[0])  # 示例调用：寻找与'chip'最相似的前3个词
```

# 子词嵌入
## fastText模型
## 字节对编码（Byte Pair Encoding）
在下面，我们将说明字节对编码是如何工作的。首先，我们将符号词表初始化为所有英文小写字符、特殊的词尾符号`'_'`和特殊的未知符号`'[UNK]'`。

```python
import collections  # 导入collections模块，用于高级容器数据类型的操作

symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
           '_', '[UNK]']  # 定义包含小写字母、下划线和未知符号的列表
```

因为我们不考虑跨越词边界的符号对，所以我们只需要一个字典`raw_token_freqs`将词映射到数据集中的频率（出现次数）。注意，特殊符号`'_'`被附加到每个词的尾部，以便我们可以容易地从输出符号序列（例如，“a_all er_man”）恢复单词序列（例如，“a_all er_man”）。由于我们仅从单个字符和特殊符号的词开始合并处理，所以在每个词（词典`token_freqs`的键）内的每对连续字符之间插入空格。换句话说，空格是词中符号之间的分隔符。

```python
raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}  # 原始的token频率字典，键是带下划线的字符串，值是频率

token_freqs = {}  # 新的token频率字典，将每个带下划线的字符串键转换为用空格分隔的字符作为新键，并保留相同的频率值

for token, freq in raw_token_freqs.items():
    token_freqs[' '.join(list(token))] = raw_token_freqs[token]  # 将原始字典中的每个键转换为用空格分隔的字符，并将对应的频率存储在新字典中

token_freqs  # 输出新的token频率字典
```

我们定义以下`get_max_freq_pair`函数，其返回词内最频繁的连续符号对，其中词来自输入词典`token_freqs`的键。

```python
def get_max_freq_pair(token_freqs):
    pairs = collections.defaultdict(int)
    for token, freq in token_freqs.items():
        symbols = token.split()
        for i in range(len(symbols) - 1):
            # “pairs”的键是两个连续符号的元组
            pairs[symbols[i], symbols[i + 1]] += freq
    return max(pairs, key=pairs.get)  # 具有最大值的“pairs”键
```

作为基于连续符号频率的贪心方法，字节对编码将使用以下`merge_symbols`函数来合并最频繁的连续符号对以产生新符号。

```python
def merge_symbols(max_freq_pair, token_freqs, symbols):
    symbols.append(''.join(max_freq_pair))  # 将频率最高的一对符号合并并加入symbols列表
    new_token_freqs = dict()
    for token, freq in token_freqs.items():
        new_token = token.replace(' '.join(max_freq_pair), ''.join(max_freq_pair))  # 将所有包含最高频率对的符号替换为合并后的符号
        new_token_freqs[new_token] = token_freqs[token]  # 更新替换后的符号的频率
    return new_token_freqs  # 返回更新后的符号频率字典
```

现在，我们对词典`token_freqs`的键迭代地执行字节对编码算法。在第一次迭代中，最频繁的连续符号对是`'t'`和`'a'`，因此字节对编码将它们合并以产生新符号`'ta'`。在第二次迭代中，字节对编码继续合并`'ta'`和`'l'`以产生另一个新符号`'tal'`。

```python
num_merges = 10  # 定义合并操作的次数为10次

for i in range(num_merges):  # 循环执行合并操作，共执行10次
    max_freq_pair = get_max_freq_pair(token_freqs)  # 获取当前频率最高的一对符号或标记
    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)  # 将获取的最高频率的符号或标记进行合并操作
    print(f'合并# {i+1}:', max_freq_pair)  # 打印每次合并操作的序号和具体合并的内容
```

在字节对编码的10次迭代之后，我们可以看到列表`symbols`现在又包含10个从其他符号迭代合并而来的符号。

```python
print(symbols)
```

对于在词典`raw_token_freqs`的键中指定的同一数据集，作为字节对编码算法的结果，数据集中的每个词现在被子词“fast_”“fast”“er_”“tall_”和“tall”分割。例如，单词“fast er_”和“tall er_”分别被分割为“fast er_”和“tall er_”。

```python
print(list(token_freqs.keys()))  # 打印 token_freqs 字典的所有键，以列表形式展示
```

请注意，字节对编码的结果取决于正在使用的数据集。我们还可以使用从一个数据集学习的子词来切分另一个数据集的单词。作为一种贪心方法，下面的`segment_BPE`函数尝试将单词从输入参数`symbols`分成可能最长的子词。

```python
def segment_BPE(tokens, symbols):
    outputs = []
    for token in tokens:
        start, end = 0, len(token)
        cur_output = []
        while start < len(token) and start < end:
            if token[start: end] in symbols:  # 如果当前子串在符号集合中
                cur_output.append(token[start: end])  # 将当前子串添加到输出列表中
                start = end  # 更新起始位置为当前结束位置
                end = len(token)  # 重置结束位置为词元的末尾
            else:
                end -= 1  # 若不在符号集合中，则缩短结束位置，尝试更短的子串
        if start < len(token):  # 如果还有剩余未处理的子串
            cur_output.append('[UNK]')  # 将剩余的子串标记为未知符号
        outputs.append(' '.join(cur_output))  # 将处理后的词元列表转换为字符串并添加到输出中
    return outputs
```

我们使用列表`symbols`中的子词（从前面提到的数据集学习）来表示另一个数据集的`tokens`。

```python
tokens = ['tallest_', 'fatter_']  # 定义一个包含两个词元的列表
print(segment_BPE(tokens, symbols))  # 调用 segment_BPE 函数对 tokens 进行分段处理，并打印结果
```

# 词的相似性和类比任务

在前面，我们在一个小的数据集上训练了一个word2vec模型，并使用它为一个输入词寻找语义相似的词。实际上，在大型语料库上预先训练的词向量可以应用于下游的自然语言处理任务。为了直观地演示大型语料库中预训练词向量的语义，让我们将预训练词向量应用到词的相似性和类比任务中。

```python
import os
import torch
from torch import nn
from d2l import torch as d2l
```

## 加载预训练词向量

以下列出维度为50、100和300的预训练GloVe嵌入，可从[GloVe网站](https://nlp.stanford.edu/projects/glove/)下载。预训练的fastText嵌入有多种语言。这里我们使用可以从[fastText网站](https://fasttext.cc/)下载300维度的英文版本（“wiki.en”）。

```python
#@save
# 定义了GloVe词向量预训练模型的下载链接和哈希值（50维）
d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',
                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')

#@save
# 定义了GloVe词向量预训练模型的下载链接和哈希值（100维）
d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',
                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')

#@save
# 定义了GloVe词向量预训练模型的下载链接和哈希值（300维）
d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',
                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')

#@save
# 定义了英文维基百科语料库的下载链接和哈希值
d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',
                           'c1816da3821ae9f43899be655002f6c723e91b88')
```

为了加载这些预训练的GloVe和fastText嵌入，我们定义了以下`TokenEmbedding`类。

```python
#@save
class TokenEmbedding:
    """GloVe嵌入"""
    def __init__(self, embedding_name):
        self.idx_to_token, self.idx_to_vec = self._load_embedding(
            embedding_name)
        self.unknown_idx = 0
        self.token_to_idx = {token: idx for idx, token in
                             enumerate(self.idx_to_token)}  # 创建从标记到索引的映射

    def _load_embedding(self, embedding_name):
        idx_to_token, idx_to_vec = ['<unk>'], []
        data_dir = d2l.download_extract(embedding_name)
        # 使用下载和解压函数下载并提取数据
        # GloVe网站：https://nlp.stanford.edu/projects/glove/
        # fastText网站：https://fasttext.cc/
        with open(os.path.join(data_dir, 'vec.txt'), 'r', encoding='utf-8') as f:
            for line in f:
                elems = line.rstrip().split(' ')
                token, elems = elems[0], [float(elem) for elem in elems[1:]]
                # 跳过标题信息，例如fastText中的首行
                if len(elems) > 1:
                    idx_to_token.append(token)
                    idx_to_vec.append(elems)
        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec  # 添加一个全零向量作为未知词的向量表示
        return idx_to_token, torch.tensor(idx_to_vec)  # 返回标记列表和对应的词向量张量

    def __getitem__(self, tokens):
        indices = [self.token_to_idx.get(token, self.unknown_idx)
                   for token in tokens]  # 获取词语的索引，未知词默认索引为0
        vecs = self.idx_to_vec[torch.tensor(indices)]  # 根据索引获取词向量张量
        return vecs

    def __len__(self):
        return len(self.idx_to_token)  # 返回词典中词语的数量
```

下面我们加载50维GloVe嵌入（在维基百科的子集上预训练）。创建`TokenEmbedding`实例时，如果尚未下载指定的嵌入文件，则必须下载该文件。

```python
glove_6b50d = TokenEmbedding('glove.6b.50d')  # 创建一个 TokenEmbedding 对象，并使用 GloVe 的 50 维度嵌入文件初始化它
```

输出词表大小。词表包含400000个词（词元）和一个特殊的未知词元。

```
len(glove_6b50d)
```

我们可以得到词表中一个单词的索引，反之亦然。

```python
glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]
```

## 应用预训练词向量

使用加载的GloVe向量，我们将通过下面的词相似性和类比任务中来展示词向量的语义。

### 词相似度

为了根据词向量之间的余弦相似性为输入词查找语义相似的词，我们实现了以下`knn`（$k$近邻）函数。

```python
def knn(W, x, k):
    # 增加1e-9以获得数值稳定性
    cos = torch.mv(W, x.reshape(-1,)) / (
        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *
        torch.sqrt((x * x).sum()))  # 计算向量 x 与矩阵 W 中每个向量的余弦相似度
    _, topk = torch.topk(cos, k=k)  # 找出余弦相似度最大的前 k 个向量的索引
    return topk, [cos[int(i)] for i in topk]  # 返回最相似的前 k 个向量的索引和对应的余弦相似度列表
```

然后，我们使用`TokenEmbedding`的实例`embed`中预训练好的词向量来搜索相似的词。

```python
def get_similar_tokens(query_token, k, embed):
    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)
    for i, c in zip(topk[1:], cos[1:]):  # 排除输入词
        print(f'{embed.idx_to_token[int(i)]}：cosine相似度={float(c):.3f}')  # 打印相似词和对应的余弦相似度
```

`glove_6b50d`中预训练词向量的词表包含400000个词和一个特殊的未知词元。排除输入词和未知词元后，我们在词表中找到与“chip”一词语义最相似的三个词。

```python
get_similar_tokens('chip', 3, glove_6b50d)
```

下面输出与“baby”和“beautiful”相似的词。

```python
get_similar_tokens('baby', 3, glove_6b50d)
```

```python
get_similar_tokens('beautiful', 3, glove_6b50d)
```

### 词类比

除了找到相似的词，我们还可以将词向量应用到词类比任务中。例如，“man” : “woman” :: “son” : “daughter”是一个词的类比。“man”是对“woman”的类比，“son”是对“daughter”的类比。具体来说，词类比任务可以定义为：对于单词类比$a : b :: c : d$，给出前三个词$a$、$b$和$c$，找到$d$。用$\text{vec}(w)$表示词$w$的向量，为了完成这个类比，我们将找到一个词，其向量与$\text{vec}(c)+\text{vec}(b)-\text{vec}(a)$的结果最相似。

```python
def get_analogy(token_a, token_b, token_c, embed):
    vecs = embed[[token_a, token_b, token_c]]  # 获取token_a, token_b, token_c的词向量
    x = vecs[1] - vecs[0] + vecs[2]  # 计算类比词
    topk, cos = knn(embed.idx_to_vec, x, 1)  # 使用最近邻算法找到与x最相似的词向量索引
    return embed.idx_to_token[int(topk[0])]  # 返回最相似词的标记（词汇表中的词）
```

让我们使用加载的词向量来验证“male-female”类比。

```python
get_analogy('man', 'woman', 'son', glove_6b50d)
```

下面完成一个“首都-国家”的类比：“beijing” : “china” :: “tokyo” : “japan”。这说明了预训练词向量中的语义。

```python
get_analogy('beijing', 'china', 'tokyo', glove_6b50d)
```

另外，对于“bad” : “worst” :: “big” : “biggest”等“形容词-形容词最高级”的比喻，预训练词向量可以捕捉到句法信息。

```python
get_analogy('bad', 'worst', 'big', glove_6b50d)
```

为了演示在预训练词向量中捕捉到的过去式概念，我们可以使用“现在式-过去式”的类比来测试句法：“do” : “did” :: “go” : “went”。

```python
get_analogy('do', 'did', 'go', glove_6b50d)
```

# 来自Transformers的双向编码器表示（BERT）
在本章的其余部分，我们将深入了解BERT的训练前准备。当在下一节解释自然语言处理应用时，我们将说明针对下游应用的BERT微调。

```python
import torch
from torch import nn
from d2l import torch as d2l
```

## 输入表示

在自然语言处理中，有些任务（如情感分析）以单个文本作为输入，而有些任务（如自然语言推断）以一对文本序列作为输入。BERT输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT输入序列是特殊类别词元“&lt;cls&gt;”、文本序列的标记、以及特殊分隔词元“&lt;sep&gt;”的连结。当输入为文本对时，BERT输入序列是“&lt;cls&gt;”、第一个文本序列的标记、“&lt;sep&gt;”、第二个文本序列标记、以及“&lt;sep&gt;”的连结。我们将始终如一地将术语“BERT输入序列”与其他类型的“序列”区分开来。例如，一个*BERT输入序列*可以包括一个*文本序列*或两个*文本序列*。

为了区分文本对，根据输入序列学到的片段嵌入$\mathbf{e}_A$和$\mathbf{e}_B$分别被添加到第一序列和第二序列的词元嵌入中。对于单文本输入，仅使用$\mathbf{e}_A$。

下面的`get_tokens_and_segments`将一个句子或两个句子作为输入，然后返回BERT输入序列的标记及其相应的片段索引。

```python
#@save
def get_tokens_and_segments(tokens_a, tokens_b=None):
    """获取输入序列的词元及其片段索引"""
    tokens = ['<cls>'] + tokens_a + ['<sep>']  # 构建包含特殊标记的词元列表
    segments = [0] * (len(tokens_a) + 2)  # 标记为片段A（0）
    if tokens_b is not None:
        tokens += tokens_b + ['<sep>']  # 如果提供了 tokens_b，则将其加入到词元列表中
        segments += [1] * (len(tokens_b) + 1)  # 标记为片段B（1）
    return tokens, segments  # 返回生成的词元列表和对应的片段索引列表
```

BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。下面的`BERTEncoder`类使用片段嵌入和可学习的位置嵌入。

```python
#@save
class BERTEncoder(nn.Module):
    """BERT编码器"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=1000, key_size=768, query_size=768, value_size=768,
                 **kwargs):
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)  # 词元嵌入层
        self.segment_embedding = nn.Embedding(2, num_hiddens)         # 片段嵌入层
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module(f"{i}", d2l.EncoderBlock(
                key_size, query_size, value_size, num_hiddens, norm_shape,
                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
            # 添加多个编码器块到序列中

        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,
                                                      num_hiddens))

    def forward(self, tokens, segments, valid_lens):
        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data[:, :X.shape[1], :]  # 添加位置嵌入
        for blk in self.blks:
            X = blk(X, valid_lens)  # 通过每个编码器块处理X
        return X
```

假设词表大小为10000，为了演示`BERTEncoder`的前向推断，让我们创建一个实例并初始化它的参数。

```python
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4  # 词汇表大小、隐藏单元数、前馈网络隐藏层大小、注意力头数
norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2  # 归一化层形状、前馈网络输入大小、编码器层数、dropout概率
encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,
                      ffn_num_hiddens, num_heads, num_layers, dropout)
# 创建一个BERT编码器实例，使用上述参数初始化
```

我们将`tokens`定义为长度为8的2个输入序列，其中每个词元是词表的索引。使用输入`tokens`的`BERTEncoder`的前向推断返回编码结果，其中每个词元由向量表示，其长度由超参数`num_hiddens`定义。此超参数通常称为Transformer编码器的*隐藏大小*（隐藏单元数）。

```python
tokens = torch.randint(0, vocab_size, (2, 8))  # 生成形状为(2, 8)的随机整数张量，范围在[0, vocab_size)之间，作为输入的token
segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])  # 创建形状为(2, 8)的张量，表示两个序列的segment信息
encoded_X = encoder(tokens, segments, None)  # 通过BERT编码器处理tokens和segments，返回编码后的结果
encoded_X.shape  # 输出编码后的结果的形状
```

## 预训练任务

`BERTEncoder`的前向推断给出了输入文本的每个词元和插入的特殊标记“&lt;cls&gt;”及“&lt;seq&gt;”的BERT表示。接下来，我们将使用这些表示来计算预训练BERT的损失函数。预训练包括以下两个任务：掩蔽语言模型和下一句预测。

### 掩蔽语言模型（Masked Language Modeling）
我们实现了下面的`MaskLM`类来预测BERT预训练的掩蔽语言模型任务中的掩蔽标记。预测使用单隐藏层的多层感知机（`self.mlp`）。在前向推断中，它需要两个输入：`BERTEncoder`的编码结果和用于预测的词元位置。输出是这些位置的预测结果。

```python
#@save
class MaskLM(nn.Module):
    """BERT的掩蔽语言模型任务"""
    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),  # 多层感知机，输入维度为num_inputs，输出维度为num_hiddens
                                 nn.ReLU(),  # ReLU激活函数
                                 nn.LayerNorm(num_hiddens),  # 层归一化，归一化维度为num_hiddens
                                 nn.Linear(num_hiddens, vocab_size))  # 线性变换层，将num_hiddens维度映射到vocab_size维度

    def forward(self, X, pred_positions):
        num_pred_positions = pred_positions.shape[1]  # 获取预测位置的数量
        pred_positions = pred_positions.reshape(-1)  # 将预测位置展平为一维张量
        batch_size = X.shape[0]  # 获取批次大小
        batch_idx = torch.arange(0, batch_size)  # 创建一个张量，表示批次索引，如[0, 1, ... , batch_size-1]
        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)  # 将批次索引重复num_pred_positions次，以匹配预测位置的数量
        masked_X = X[batch_idx, pred_positions]  # 根据预测位置从输入X中获取对应的值，即掩蔽语言模型的输入
        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))  # 将masked_X重新形状为(batch_size, num_pred_positions, -1)，-1表示自动推断维度
        mlm_Y_hat = self.mlp(masked_X)  # 使用多层感知机处理masked_X，得到预测的掩蔽语言模型输出
        return mlm_Y_hat  # 返回预测的掩蔽语言模型输出
```

为了演示`MaskLM`的前向推断，我们创建了其实例`mlm`并对其进行了初始化。回想一下，来自`BERTEncoder`的正向推断`encoded_X`表示2个BERT输入序列。我们将`mlm_positions`定义为在`encoded_X`的任一输入序列中预测的3个指示。`mlm`的前向推断返回`encoded_X`的所有掩蔽位置`mlm_positions`处的预测结果`mlm_Y_hat`。对于每个预测，结果的大小等于词表的大小。

```python
mlm = MaskLM(vocab_size, num_hiddens)  # 初始化掩蔽语言模型实例，vocab_size为词汇表大小，num_hiddens为隐藏单元数量
mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])  # 定义掩蔽语言模型需要预测的位置张量，形状为(2, 3)
mlm_Y_hat = mlm(encoded_X, mlm_positions)  # 使用掩蔽语言模型mlm对encoded_X进行预测，得到预测输出mlm_Y_hat
print(mlm_Y_hat.shape)  # 输出mlm_Y_hat的形状，应为(2, 3, vocab_size)，其中vocab_size为词汇表大小
```

通过掩码下的预测词元`mlm_Y`的真实标签`mlm_Y_hat`，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失。

```python
mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])  # 定义掩蔽语言模型的目标张量mlm_Y，形状为(2, 3)
loss = nn.CrossEntropyLoss(reduction='none')  # 初始化交叉熵损失函数loss，设置为不进行损失的汇总操作
mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))  # 计算掩蔽语言模型的损失mlm_l，
                                                                      # 将mlm_Y_hat和mlm_Y分别展平后作为参数传入损失函数
print(mlm_l.shape)  # 输出mlm_l的形状，应为(6, vocab_size)，其中vocab_size为词汇表大小
```

### 下一句预测（Next Sentence Prediction）

尽管掩蔽语言建模能够编码双向上下文来表示单词，但它不能显式地建模文本对之间的逻辑关系。为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二元分类任务——*下一句预测*。在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。

下面的`NextSentencePred`类使用单隐藏层的多层感知机来预测第二个句子是否是BERT输入序列中第一个句子的下一个句子。由于Transformer编码器中的自注意力，特殊词元“&lt;cls&gt;”的BERT表示已经对输入的两个句子进行了编码。因此，多层感知机分类器的输出层（`self.output`）以`X`作为输入，其中`X`是多层感知机隐藏层的输出，而MLP隐藏层的输入是编码后的“&lt;cls&gt;”词元。

```python
#@save
class NextSentencePred(nn.Module):
    """BERT的下一句预测任务"""

    def __init__(self, num_inputs, **kwargs):
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.Linear(num_inputs, 2)  # 初始化一个线性层作为输出层，输入维度为num_inputs，输出维度为2

    def forward(self, X):
        # X的形状：(batchsize, num_hiddens)，输入X经过线性层输出
        return self.output(X)
```

我们可以看到，`NextSentencePred`实例的前向推断返回每个BERT输入序列的二分类预测。

```python
encoded_X = torch.flatten(encoded_X, start_dim=1)  # 对encoded_X进行展平操作，起始维度为1

# NSP的输入形状:(batchsize，num_hiddens)
nsp = NextSentencePred(encoded_X.shape[-1])  # 创建NextSentencePred实例nsp，输入维度为encoded_X的最后一个维度
nsp_Y_hat = nsp(encoded_X)  # 使用nsp对encoded_X进行前向传播得到预测结果nsp_Y_hat
nsp_Y_hat.shape  # 输出nsp_Y_hat的形状
```

还可以计算两个二元分类的交叉熵损失。

```python
nsp_y = torch.tensor([0, 1])  # 创建目标张量nsp_y，表示两个样本的真实标签
nsp_l = loss(nsp_Y_hat, nsp_y)  # 使用交叉熵损失函数计算nsp_Y_hat和nsp_y之间的损失值nsp_l
nsp_l.shape  # 输出nsp_l的形状
```

## 整合代码

在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。现在我们可以通过实例化三个类`BERTEncoder`、`MaskLM`和`NextSentencePred`来定义`BERTModel`类。前向推断返回编码后的BERT表示`encoded_X`、掩蔽语言模型预测`mlm_Y_hat`和下一句预测`nsp_Y_hat`。

```python
#@save
class BERTModel(nn.Module):
    """BERT模型"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
             ffn_num_hiddens, num_heads, num_layers, dropout,
             max_len=1000, key_size=768, query_size=768, value_size=768,
             hid_in_features=768, mlm_in_features=768,
             nsp_in_features=768):
        super(BERTModel, self).__init__()

        # 初始化BERT编码器
        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
                                   ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
                                   dropout, max_len=max_len, key_size=key_size,
                                   query_size=query_size, value_size=value_size)

        # 隐藏层
        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
                                    nn.Tanh())

        # 掩码语言建模
        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)

        # 下一句预测
        self.nsp = NextSentencePred(nsp_in_features)


    def forward(self, tokens, segments, valid_lens=None,
                pred_positions=None):
        # 编码输入序列
        encoded_X = self.encoder(tokens, segments, valid_lens)

        # 如果有掩码预测位置，执行掩码语言建模
        if pred_positions is not None:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        else:
            mlm_Y_hat = None

        # 下一句预测的隐藏层输入为第一个位置的隐藏状态（<cls>标记）
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))

        # 返回编码后的序列、掩码语言建模的预测、以及下一句预测的预测
        return encoded_X, mlm_Y_hat, nsp_Y_hat

```

# 用于预训练BERT的数据集
为了预训练实现的BERT模型，我们需要以理想的格式生成数据集，以便于两个预训练任务：遮蔽语言模型和下一句预测。一方面，最初的BERT模型是在两个庞大的图书语料库和英语维基百科的合集上预训练的，但它很难吸引这本书的大多数读者。另一方面，现成的预训练BERT模型可能不适合医学等特定领域的应用。因此，在定制的数据集上对BERT进行预训练变得越来越流行。为了方便BERT预训练的演示，我们使用了较小的语料库WikiText-2 。

与用于预训练word2vec的PTB数据集相比，WikiText-2（1）保留了原来的标点符号，适合于下一句预测；（2）保留了原来的大小写和数字；（3）大了一倍以上。

```python
import os
import random
import torch
from d2l import torch as d2l
```

在WikiText-2数据集中，每行代表一个段落，其中在任意标点符号及其前面的词元之间插入空格。保留至少有两句话的段落。为了简单起见，我们仅使用句号作为分隔符来拆分句子。我们将更复杂的句子拆分技术的讨论留在本节末尾的练习中。

```python
#@save
d2l.DATA_HUB['wikitext-2'] = (
    'https://s3.amazonaws.com/research.metamind.io/wikitext/'
    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')

#@save
def _read_wiki(data_dir):
    file_name = os.path.join(data_dir, 'wiki.train.tokens')
    with open(file_name, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    # 读取每行数据并进行处理
    paragraphs = [line.strip().lower().split(' . ')
                  for line in lines if len(line.split(' . ')) >= 2]
    random.shuffle(paragraphs)  # 打乱段落顺序
    return paragraphs  # 返回处理后的段落列表
```

## 为预训练任务定义辅助函数

在下文中，我们首先为BERT的两个预训练任务实现辅助函数。这些辅助函数将在稍后将原始文本语料库转换为理想格式的数据集时调用，以预训练BERT。

### 生成下一句预测任务的数据

`_get_next_sentence`函数生成二分类任务的训练样本。

```python
#@save
def _get_next_sentence(sentence, next_sentence, paragraphs):
    if random.random() < 0.5:
        is_next = True  # 以50%的概率选择下一个句子作为接续句
    else:
        # paragraphs是三重列表的嵌套
        next_sentence = random.choice(random.choice(paragraphs))  # 随机选择一个段落中的句子作为接续句
        is_next = False
    return sentence, next_sentence, is_next
```

下面的函数通过调用`_get_next_sentence`函数从输入`paragraph`生成用于下一句预测的训练样本。这里`paragraph`是句子列表，其中每个句子都是词元列表。自变量`max_len`指定预训练期间的BERT输入序列的最大长度。

```python
#@save
def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):
    nsp_data_from_paragraph = []
    for i in range(len(paragraph) - 1):
        tokens_a, tokens_b, is_next = _get_next_sentence(
            paragraph[i], paragraph[i + 1], paragraphs)
        # 考虑1个'<cls>'词元和2个'<sep>'词元
        if len(tokens_a) + len(tokens_b) + 3 > max_len:
            continue  # 如果tokens_a和tokens_b的长度加上特殊词元超过max_len，则跳过该样本
        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)
        nsp_data_from_paragraph.append((tokens, segments, is_next))
    return nsp_data_from_paragraph
```

### 生成遮蔽语言模型任务的数据

为了从BERT输入序列生成遮蔽语言模型的训练样本，我们定义了以下`_replace_mlm_tokens`函数。在其输入中，`tokens`是表示BERT输入序列的词元的列表，`candidate_pred_positions`是不包括特殊词元的BERT输入序列的词元索引的列表（特殊词元在遮蔽语言模型任务中不被预测），以及`num_mlm_preds`指示预测的数量（选择15%要预测的随机词元）。在定义遮蔽语言模型任务之后，在每个预测位置，输入可以由特殊的“掩码”词元或随机词元替换，或者保持不变。最后，该函数返回可能替换后的输入词元、发生预测的词元索引和这些预测的标签。

```python
#@save
def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,
                        vocab):
    # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元
    mlm_input_tokens = [token for token in tokens]  # 复制输入的词元
    pred_positions_and_labels = []  # 用于存储预测位置和标签的列表
    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测
    random.shuffle(candidate_pred_positions)  # 打乱候选预测位置
    for mlm_pred_position in candidate_pred_positions:  # 遍历候选预测位置
        if len(pred_positions_and_labels) >= num_mlm_preds:  # 如果已处理的预测位置数量达到要求
            break  # 退出循环
        masked_token = None  # 初始化被遮蔽的词元
        # 80%的时间：将词替换为“<mask>”词元
        if random.random() < 0.8:  # 80%的概率
            masked_token = '<mask>'  # 替换为“<mask>”词元
        else:
            # 10%的时间：保持词不变
            if random.random() < 0.5:  # 50%的概率
                masked_token = tokens[mlm_pred_position]  # 保持词元不变
            # 10%的时间：用随机词替换该词
            else:
                masked_token = random.choice(vocab.idx_to_token)  # 用随机词替换
        mlm_input_tokens[mlm_pred_position] = masked_token  # 替换词元
        pred_positions_and_labels.append(
            (mlm_pred_position, tokens[mlm_pred_position]))  # 记录预测位置和标签
    return mlm_input_tokens, pred_positions_and_labels  # 返回修改后的词元和预测位置及标签
```

通过调用前述的`_replace_mlm_tokens`函数，以下函数将BERT输入序列（`tokens`）作为输入，并返回输入词元的索引、发生预测的词元索引以及这些预测的标签索引。

```python
#@save
def _get_mlm_data_from_tokens(tokens, vocab):
    candidate_pred_positions = []
    # 遍历词元列表，获取可以预测的位置（不包括特殊词元如'<cls>', '<sep>'）
    for i, token in enumerate(tokens):
        if token in ['<cls>', '<sep>']:  # 跳过特殊词元
            continue
        candidate_pred_positions.append(i)  # 记录可以预测的位置索引

    # 确定要预测的随机词元数量，约为总词元数量的15%
    num_mlm_preds = max(1, round(len(tokens) * 0.15))

    # 调用_replace_mlm_tokens函数，生成遮蔽语言模型任务的输入词元、预测位置及其标签
    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(
        tokens, candidate_pred_positions, num_mlm_preds, vocab)

    # 根据预测位置的索引排序预测位置及其标签
    pred_positions_and_labels = sorted(pred_positions_and_labels,
                                       key=lambda x: x[0])
    pred_positions = [v[0] for v in pred_positions_and_labels]  # 提取预测位置索引
    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]  # 提取预测标签

    # 返回MLM任务的输入词元、预测位置索引、预测标签
    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]
```

## 将文本转换为预训练数据集

现在我们几乎准备好为BERT预训练定制一个`Dataset`类。在此之前，我们仍然需要定义辅助函数`_pad_bert_inputs`来将特殊的“&lt;mask&gt;”词元附加到输入。它的参数`examples`包含来自两个预训练任务的辅助函数`_get_nsp_data_from_paragraph`和`_get_mlm_data_from_tokens`的输出。

```python
#@save
def _pad_bert_inputs(examples, max_len, vocab):
    max_num_mlm_preds = round(max_len * 0.15)  # 最大MLM预测数量

    # 初始化存储各种输入数据的列表
    all_token_ids, all_segments, valid_lens = [], [], []
    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []
    nsp_labels = []

    # 遍历输入的样本
    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:
        # 将token_ids填充至max_len长度，并转换为PyTorch张量
        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long))
        
        # 将segments填充至max_len长度（通常为0），并转换为PyTorch张量
        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))
        
        # 记录有效长度（不包括'<pad>'的部分），转换为PyTorch张量
        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))
        
        # 将预测位置填充至max_num_mlm_preds长度，并转换为PyTorch张量
        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.long))
        
        # 设置遮蔽语言模型权重，已预测部分为1.0，其余为0.0，并转换为PyTorch张量
        all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.float32))
        
        # 将遮蔽语言模型标签填充至max_num_mlm_preds长度，并转换为PyTorch张量
        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))
        
        # 记录下一句预测标签，并转换为PyTorch张量
        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))

    # 返回所有填充后的数据，作为BERT模型的输入
    return (all_token_ids, all_segments, valid_lens, all_pred_positions,
            all_mlm_weights, all_mlm_labels, nsp_labels)
```

将用于生成两个预训练任务的训练样本的辅助函数和用于填充输入的辅助函数放在一起，我们定义以下`_WikiTextDataset`类为用于预训练BERT的WikiText-2数据集。通过实现`__getitem__ `函数，我们可以任意访问WikiText-2语料库的一对句子生成的预训练样本（遮蔽语言模型和下一句预测）样本。

最初的BERT模型使用词表大小为30000的WordPiece嵌入。WordPiece的词元化方法是对原有的字节对编码算法稍作修改。为简单起见，我们使用`d2l.tokenize`函数进行词元化。出现次数少于5次的不频繁词元将被过滤掉。

```python
#@save
class _WikiTextDataset(torch.utils.data.Dataset):
    def __init__(self, paragraphs, max_len):
        # 输入paragraphs[i]是代表段落的句子字符串列表；
        # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表
        # 分词处理每个段落
        paragraphs = [d2l.tokenize(
            paragraph, token='word') for paragraph in paragraphs]

        # 将段落中的句子合并到一个列表中
        sentences = [sentence for paragraph in paragraphs
                     for sentence in paragraph]

        # 构建词汇表，设定最小词频为5，并预留特殊标记
        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[
            '<pad>', '<mask>', '<cls>', '<sep>'])

        # 获取下一句子预测任务的数据
        examples = []
        for paragraph in paragraphs:
            examples.extend(_get_nsp_data_from_paragraph(
                paragraph, paragraphs, self.vocab, max_len))
        # 获取遮蔽语言模型任务的数据
        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)
                      + (segments, is_next))
                     for tokens, segments, is_next in examples]
        # 填充输入
        (self.all_token_ids, self.all_segments, self.valid_lens,
         self.all_pred_positions, self.all_mlm_weights,
         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(
            examples, max_len, self.vocab)

        def __getitem__(self, idx):
        # 获取指定索引位置的数据并返回
            return (self.all_token_ids[idx],       # 所有token的ID列表中的第idx个元素
                    self.all_segments[idx],         # 所有segment的列表中的第idx个元素
                    self.valid_lens[idx],           # valid_lens列表中的第idx个元素
                    self.all_pred_positions[idx],   # 所有预测位置列表中的第idx个元素
                    self.all_mlm_weights[idx],      # 所有MLM权重列表中的第idx个元素
                    self.all_mlm_labels[idx],       # 所有MLM标签列表中的第idx个元素
                    self.nsp_labels[idx])           # NSP标签列表中的第idx个元素

        def __len__(self):
            # 返回数据集的总长度
            return len(self.all_token_ids)         # 所有token的ID列表的长度

```

通过使用`_read_wiki`函数和`_WikiTextDataset`类，我们定义了下面的`load_data_wiki`来下载并生成WikiText-2数据集，并从中生成预训练样本。

```python
#@save
def load_data_wiki(batch_size, max_len):
    """加载WikiText-2数据集"""
    num_workers = d2l.get_dataloader_workers()  # 获取数据加载器的工作进程数
#     data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')  # 下载并解压WikiText-2数据集
    data_dir = "D:\\tools\\env\\d2l\\d2l-zh\\pytorch\\test\\data\\wikitext-2"
    paragraphs = _read_wiki(data_dir)  # 从数据目录中读取段落文本
    train_set = _WikiTextDataset(paragraphs, max_len)  # 创建WikiText数据集对象
    train_iter = torch.utils.data.DataLoader(train_set, batch_size,
                                             shuffle=True, num_workers=num_workers)  # 创建数据加载器迭代器
    return train_iter, train_set.vocab  # 返回训练数据迭代器和词汇表对象
```

将批量大小设置为512，将BERT输入序列的最大长度设置为64，我们打印出小批量的BERT预训练样本的形状。注意，在每个BERT输入序列中，为遮蔽语言模型任务预测$10$（$64 \times 0.15$）个位置。

```python
batch_size, max_len = 512, 64  # 设置批量大小和最大序列长度
train_iter, vocab = load_data_wiki(batch_size, max_len)  # 加载WikiText-2数据集

# 迭代训练数据加载器 train_iter 中的第一个批次数据
for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,
     mlm_Y, nsp_y) in train_iter:
    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,
          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,
          nsp_y.shape)  # 打印每个张量的形状信息
    break  # 只展示第一个批次的数据形状
```

最后，我们来看一下词量。即使在过滤掉不频繁的词元之后，它仍然比PTB数据集的大两倍以上。

```python
len(vocab)
```

# 预训练BERT

我们将在本节中在WikiText-2数据集上对BERT进行预训练。

```python
import torch
from torch import nn
from d2l import torch as d2l
```

首先，我们加载WikiText-2数据集作为小批量的预训练样本，用于遮蔽语言模型和下一句预测。批量大小是512，BERT输入序列的最大长度是64。注意，在原始BERT模型中，最大长度是512。

```python
batch_size, max_len = 512, 64
train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)
```

## 预训练BERT

原始BERT有两个不同模型尺寸的版本。基本模型（$\text{BERT}_{\text{BASE}}$）使用12层（Transformer编码器块），768个隐藏单元（隐藏大小）和12个自注意头。大模型（$\text{BERT}_{\text{LARGE}}$）使用24层，1024个隐藏单元和16个自注意头。值得注意的是，前者有1.1亿个参数，后者有3.4亿个参数。为了便于演示，我们定义了一个小的BERT，使用了2层、128个隐藏单元和2个自注意头。

```python
net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],
                    ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,
                    num_layers=2, dropout=0.2, key_size=128, query_size=128,
                    value_size=128, hid_in_features=128, mlm_in_features=128,
                    nsp_in_features=128)  # 创建一个 BERT 模型实例，设置了模型的各项超参数
devices = d2l.try_all_gpus()  # 尝试获取所有可用的 GPU 设备
loss = nn.CrossEntropyLoss()  # 定义交叉熵损失函数，用于训练中的分类任务
```

在定义训练代码实现之前，我们定义了一个辅助函数`_get_batch_loss_bert`。给定训练样本，该函数计算遮蔽语言模型和下一句子预测任务的损失。请注意，BERT预训练的最终损失是遮蔽语言模型损失和下一句预测损失的和。

```python
#@save
def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,  # 定义一个函数来计算BERT模型的损失函数
                         segments_X, valid_lens_x,  # 分割的X值以及有效长度
                         pred_positions_X, mlm_weights_X,  # 预测位置和遮蔽语言模型权重
                         mlm_Y, nsp_y):  # mlm_Y和nsp_y
    # 前向传播
    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,  # 使用网络进行前向传播，获取预测的mlm_Y_hat和nsp_Y_hat
                                  valid_lens_x.reshape(-1),  # 重塑有效长度
                                  pred_positions_X)  # 预测位置X
    # 计算遮蔽语言模型损失
    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\  # 计算遮蔽语言模型的损失
    mlm_weights_X.reshape(-1, 1)  # 乘以遮蔽语言模型权重
    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)  # 对遮蔽语言模型损失求和并标准化
    # 计算下一句子预测任务的损失
    nsp_l = loss(nsp_Y_hat, nsp_y)  # 计算下一句子预测任务的损失
    l = mlm_l + nsp_l  # 总损失为遮蔽语言模型损失和下一句子预测任务损失之和
    return mlm_l, nsp_l, l  # 返回遮蔽语言模型损失、下一句子预测任务损失和总损失
```

通过调用上述两个辅助函数，下面的`train_bert`函数定义了在WikiText-2（`train_iter`）数据集上预训练BERT（`net`）的过程。训练BERT可能需要很长时间。以下函数的输入`num_steps`指定了训练的迭代步数，而不是指定训练的轮数。

```python
def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])  # 使用多GPU并行化模型，并将模型移到主设备上
    trainer = torch.optim.Adam(net.parameters(), lr=0.01)  # 使用Adam优化器进行模型参数优化
    step, timer = 0, d2l.Timer()  # 初始化步数和计时器
    animator = d2l.Animator(xlabel='step', ylabel='loss',  # 创建动画对象，用于可视化训练过程中的损失变化
                            xlim=[1, num_steps], legend=['mlm', 'nsp'])
    metric = d2l.Accumulator(4)  # 创建累加器，用于累积遮蔽语言模型损失、下一句预测任务损失、句子对数量和计数
    num_steps_reached = False  # 初始化达到指定步数的标志位

    while step < num_steps and not num_steps_reached:
        for tokens_X, segments_X, valid_lens_x, pred_positions_X, \
            mlm_weights_X, mlm_Y, nsp_y in train_iter:
            tokens_X = tokens_X.to(devices[0])  # 将tokens_X移到指定设备上
            segments_X = segments_X.to(devices[0])  # 将segments_X移到指定设备上
            valid_lens_x = valid_lens_x.to(devices[0])  # 将valid_lens_x移到指定设备上
            pred_positions_X = pred_positions_X.to(devices[0])  # 将pred_positions_X移到指定设备上
            mlm_weights_X = mlm_weights_X.to(devices[0])  # 将mlm_weights_X移到指定设备上
            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])  # 将mlm_Y和nsp_y移到指定设备上

            trainer.zero_grad()  # 梯度清零
            timer.start()  # 计时器开始计时

            mlm_l, nsp_l, l = _get_batch_loss_bert(  # 调用_get_batch_loss_bert函数计算损失
                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,
                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)

            l.backward()  # 反向传播计算梯度
            trainer.step()  # 更新模型参数

            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)  # 累加遮蔽语言模型损失、下一句预测任务损失、句子对数量和计数
            timer.stop()  # 计时器停止计时

            animator.add(step + 1,  # 在动画器中添加数据点，显示当前损失情况
                         (metric[0] / metric[3], metric[1] / metric[3]))

            step += 1  # 更新步数
            if step == num_steps:  # 如果达到指定的步数
                num_steps_reached = True  # 设置标志位为True
                break

    # 打印遮蔽语言模型损失、下一句预测任务损失、每秒处理的句子对数量和使用的设备
    print(f'MLM loss {metric[0] / metric[3]:.3f}, '
          f'NSP loss {metric[1] / metric[3]:.3f}')
    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '
          f'{str(devices)}')
```

在预训练过程中，我们可以绘制出遮蔽语言模型损失和下一句预测损失。

```python
train_bert(train_iter, net, loss, len(vocab), devices, 50)
```

## 用BERT表示文本

在预训练BERT之后，我们可以用它来表示单个文本、文本对或其中的任何词元。下面的函数返回`tokens_a`和`tokens_b`中所有词元的BERT（`net`）表示。

```python
def get_bert_encoding(net, tokens_a, tokens_b=None):
    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)  # 获取tokens和segments
    
    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)  # 将tokens转换为token IDs，并移到指定设备上
    
    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)  # 将segments转换为PyTorch张量，并移到指定设备上
    
    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)  # 计算tokens的有效长度，并转换为PyTorch张量，并移到指定设备上
    
    encoded_X, _, _ = net(token_ids, segments, valid_len)  # 使用BERT模型编码输入tokens，返回编码结果
    
    return encoded_X  # 返回BERT编码结果
```

考虑“a crane is flying”这句话。回想一下讨论的BERT的输入表示。插入特殊标记“&lt;cls&gt;”（用于分类）和“&lt;sep&gt;”（用于分隔）后，BERT输入序列的长度为6。因为零是“&lt;cls&gt;”词元，`encoded_text[:, 0, :]`是整个输入语句的BERT表示。为了评估一词多义词元“crane”，我们还打印出了该词元的BERT表示的前三个元素。

```python
tokens_a = ['a', 'crane', 'is', 'flying']
encoded_text = get_bert_encoding(net, tokens_a)  # 获得输入tokens_a的BERT编码结果

# 词元：'<cls>','a','crane','is','flying','<sep>'
encoded_text_cls = encoded_text[:, 0, :]  # 提取编码后的'<cls>'词元的BERT向量表示
encoded_text_crane = encoded_text[:, 2, :]  # 提取编码后的'crane'词元的BERT向量表示

# 输出编码结果的形状、'<cls>'词元的形状以及'crane'词元前三个元素的值
encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]
```

现在考虑一个句子“a crane driver came”和“he just left”。类似地，`encoded_pair[:, 0, :]`是来自预训练BERT的整个句子对的编码结果。注意，多义词元“crane”的前三个元素与上下文不同时的元素不同。这支持了BERT表示是上下文敏感的。

```python
tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']
encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)
# 词元：'<cls>','a','crane','driver','came','<sep>','he','just',
# 'left','<sep>'
encoded_pair_cls = encoded_pair[:, 0, :]
encoded_pair_crane = encoded_pair[:, 2, :]
encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]
```

