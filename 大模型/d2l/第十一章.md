# 优化和深度学习

```python
%matplotlib inline
import numpy as np  # 导入NumPy库
import torch  # 导入PyTorch库
from mpl_toolkits import mplot3d  # 导入matplotlib的3D绘图工具
from d2l import torch as d2l  # 导入d2l库中的PyTorch模块
```

为了说明上述不同的目标，引入两个概念*风险*和*经验风险*。经验风险是训练数据集的平均损失，而风险则是整个数据群的预期损失。下面我们定义了两个函数：风险函数`f`和经验风险函数`g`。假设我们只有有限的训练数据。因此，这里的`g`不如`f`平滑。

```python
def f(x):  # 风险函数
    return x * torch.cos(np.pi * x)

def g(x):  # 经验风险函数
    return f(x) + 0.2 * torch.cos(5 * np.pi * x)
```

下图说明，训练数据集的最低经验风险可能与最低风险（泛化误差）不同。

```python
def annotate(text, xy, xytext):  #@save
    # 添加注释到图形上
    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
                           arrowprops=dict(arrowstyle='->'))

x = torch.arange(0.5, 1.5, 0.01)
d2l.set_figsize((4.5, 2.5))
d2l.plot(x, [f(x), g(x)], 'x', 'risk')
# 标注经验风险的最小值
annotate('min of\nempirical risk', (1.0, -1.2), (0.5, -1.1))
# 标注风险的最小值
annotate('min of risk', (1.1, -1.05), (0.95, -0.5))
```

## 深度学习中的优化挑战
深度学习优化存在许多挑战。其中最令人烦恼的是局部最小值、鞍点和梯度消失。
### 局部最小值
对于任何目标函数$f(x)$，如果在$x$处对应的$f(x)$值小于在$x$附近任意其他点的$f(x)$值，那么$f(x)$可能是局部最小值。如果$f(x)$在$x$处的值是整个域中目标函数的最小值，那么$f(x)$是全局最小值。
例如，给定函数
$$f(x) = x \cdot \text{cos}(\pi x) \text{ for } -1.0 \leq x \leq 2.0,$$
我们可以近似该函数的局部最小值和全局最小值。

```python
x = torch.arange(-1.0, 2.0, 0.01)  # 创建一个 torch 数组，范围从 -1.0 到 2.0，步长为 0.01
d2l.plot(x, [f(x), ], 'x', 'f(x)')  # 绘制 f(x) 函数的图像，用 'x' 标记数据点，横轴标签为 'f(x)'
annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))  # 在图上标注局部最小值，箭头从 (-0.3, -0.25) 指向 (-0.77, -1.0)
annotate('global minimum', (1.1, -0.95), (0.6, 0.8))  # 在图上标注全局最小值，箭头从 (1.1, -0.95) 指向 (0.6, 0.8)
```

深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数*局部*最优，而不是*全局*最优。只有一定程度的噪声可能会使参数跳出局部最小值。事实上，这是小批量随机梯度下降的有利特性之一。在这种情况下，小批量上梯度的自然变化能够将参数从局部极小值中跳出。
### 鞍点
除了局部最小值之外，鞍点是梯度消失的另一个原因。*鞍点*（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数$f(x) = x^3$。它的一阶和二阶导数在$x=0$时消失。这时优化可能会停止，尽管它不是最小值。

```python
x = torch.arange(-2.0, 2.0, 0.01)  # 创建一个 torch 数组，范围从 -2.0 到 2.0，步长为 0.01
d2l.plot(x, [x**3], 'x', 'f(x)')  # 绘制 f(x) = x^3 函数的图像，用 'x' 标记数据点，横轴标签为 'f(x)'
annotate('saddle point', (0, -0.2), (-0.52, -5.0))  # 在图上标注鞍点，箭头从 (0, -0.2) 指向 (-0.52, -5.0)
```

如下例所示，较高维度的鞍点甚至更加隐蔽。考虑这个函数$f(x, y) = x^2 - y^2$。它的鞍点为$(0, 0)$。这是关于$y$的最大值，也是关于$x$的最小值。此外，它看起来像个马鞍，这就是鞍点的名字由来。

```python
x, y = torch.meshgrid(
    torch.linspace(-1.0, 1.0, 101), torch.linspace(-1.0, 1.0, 101))
# 创建一个二维网格，x 和 y 分别取自 -1.0 到 1.0 的 101 个均匀间隔点
z = x**2 - y**2  # 计算二元函数 z = x^2 - y^2

ax = d2l.plt.figure().add_subplot(111, projection='3d')  # 创建一个三维子图
ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})  # 绘制 z = x^2 - y^2 的三维线框图
ax.plot([0], [0], [0], 'rx')  # 在 (0, 0, 0) 处用红色 'x' 标记点

ticks = [-1, 0, 1]  # 设置刻度
d2l.plt.xticks(ticks)  # 设置 x 轴刻度
d2l.plt.yticks(ticks)  # 设置 y 轴刻度
ax.set_zticks(ticks)  # 设置 z 轴刻度

d2l.plt.xlabel('x')  # 设置 x 轴标签
d2l.plt.ylabel('y')  # 设置 y 轴标签
```

### 梯度消失

可能遇到的最隐蔽问题是梯度消失。回想一下我们常用的激活函数及其衍生函数。例如，假设我们想最小化函数$f(x) = \tanh(x)$，然后我们恰好从$x = 4$开始。正如我们所看到的那样，$f$的梯度接近零。更具体地说，$f'(x) = 1 - \tanh^2(x)$，因此是$f'(4) = 0.0013$。因此，在我们取得进展之前，优化将会停滞很长一段时间。事实证明，这是在引入ReLU激活函数之前训练深度学习模型相当棘手的原因之一。

```python
x = torch.arange(-2.0, 5.0, 0.01)  # 创建一个 torch 数组，范围从 -2.0 到 5.0，步长为 0.01
d2l.plot(x, [torch.tanh(x)], 'x', 'f(x)')  # 绘制 f(x) = tanh(x) 函数的图像，用 'x' 标记数据点，横轴标签为 'f(x)'
annotate('vanishing gradient', (4, 1), (2, 0.0))  # 在图上标注梯度消失问题，箭头从 (4, 1) 指向 (2, 0.0)
```

# 凸性

```python
%matplotlib inline
import numpy as np  # 导入 NumPy 库，用于数值计算
import torch  # 导入 PyTorch 库，用于张量计算
from mpl_toolkits import mplot3d  # 导入 mplot3d 模块，用于三维绘图
from d2l import torch as d2l  # 导入 d2l 库中的 torch 模块，一般用于深度学习教学
```

### 凸函数

现在我们有了凸集，我们可以引入*凸函数*（convex function）$f$。给定一个凸集$\mathcal{X}$，如果对于所有$x, x' \in \mathcal{X}$和所有$\lambda \in [0, 1]$，函数$f: \mathcal{X} \to \mathbb{R}$是凸的，我们可以得到

$$\lambda f(x) + (1-\lambda) f(x') \geq f(\lambda x + (1-\lambda) x').$$

为了说明这一点，让我们绘制一些函数并检查哪些函数满足要求。下面我们定义一些函数，包括凸函数和非凸函数。

```python
f = lambda x: 0.5 * x**2  # 定义凸函数 f(x) = 0.5 * x^2
g = lambda x: torch.cos(np.pi * x)  # 定义非凸函数 g(x) = cos(pi * x)
h = lambda x: torch.exp(0.5 * x)  # 定义凸函数 h(x) = exp(0.5 * x)

x, segment = torch.arange(-2, 2, 0.01), torch.tensor([-1.5, 1])
d2l.use_svg_display()  # 使用 SVG 格式显示图形
_, axes = d2l.plt.subplots(1, 3, figsize=(9, 3))  # 创建 1 行 3 列的子图，并设置图形大小为 (9, 3)
for ax, func in zip(axes, [f, g, h]):
    d2l.plot([x, segment], [func(x), func(segment)], axes=ax)  # 绘制函数图像到各自的子图中
```

## 性质
### 局部极小值是全局极小值
例如，对于凸函数$f(x) = (x-1)^2$，有一个局部最小值$x=1$，它也是全局最小值

```python
f = lambda x: (x - 1) ** 2  # 定义函数 f(x) = (x - 1)^2
d2l.set_figsize()  # 设置图形大小为默认大小
d2l.plot([x, segment], [f(x), f(segment)], 'x', 'f(x)')
```

### 凸函数的下水平集是凸的
### 凸性和二阶导数

# 梯度下降
## 一维梯度下降
下面我们来展示如何实现梯度下降。为了简单起见，我们选用目标函数$f(x)=x^2$。尽管我们知道$x=0$时$f(x)$能取得最小值，但我们仍然使用这个简单的函数来观察$x$的变化。

```python
%matplotlib inline
import numpy as np  # 导入NumPy库并使用别名np
import torch  # 导入PyTorch库
from d2l import torch as d2l  # 从d2l库中导入torch模块，并使用别名d2l
```

```python
def f(x):  # 目标函数
    return x ** 2

def f_grad(x):  # 目标函数的梯度(导数)
    return 2 * x
```

接下来，我们使用$x=10$作为初始值，并假设$\eta=0.2$。使用梯度下降法迭代$x$共10次，我们可以看到，$x$的值最终将接近最优解。

```python
def gd(eta, f_grad):
    x = 10.0  # 初始化 x 的初始值为 10.0
    results = [x]  # 用于存储每次迭代后的 x 值的列表，初始添加初始值 10.0
    for i in range(10):
        x -= eta * f_grad(x)  # 根据梯度下降法更新 x 的值
        results.append(float(x))  # 将更新后的 x 值添加到结果列表中
    print(f'epoch 10, x: {x:f}')  # 打印迭代结束后的 x 值，保留小数点后的精度
    return results  # 返回所有迭代过程中的 x 值列表

results = gd(0.2, f_grad)  # 调用 gd 函数进行梯度下降优化，并将结果存储在 results 变量中
```

对进行$x$优化的过程可以绘制如下。

```python
def show_trace(results, f):
    n = max(abs(min(results)), abs(max(results)))  # 计算结果列表中的最大绝对值，用于确定绘图范围
    f_line = torch.arange(-n, n, 0.01)  # 生成一个从 -n 到 n 的范围内，步长为 0.01 的 torch 数组
    d2l.set_figsize()  # 设置图形的大小
    d2l.plot([f_line, results], [[f(x) for x in f_line], [
        f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])  # 使用d2l库中的plot函数绘制图形，包括函数曲线和追踪结果

show_trace(results, f)  # 调用show_trace函数显示优化过程的轨迹图
```

### 学习率
*学习率*（learning rate）决定目标函数能否收敛到局部最小值，以及何时收敛到最小值。学习率$\eta$可由算法设计者设置。请注意，如果我们使用的学习率太小，将导致$x$的更新非常缓慢，需要更多的迭代。例如，考虑同一优化问题中$\eta = 0.05$的进度。如下所示，尽管经过了10个步骤，我们仍然离最优解很远。

```python
show_trace(gd(0.05, f_grad), f)  # 0.05 为学习率
```

相反，如果我们使用过高的学习率，$x$的迭代不能保证降低$f(x)$的值。例如，当学习率为$\eta=1.1$时，$x$超出了最优解$x=0$并逐渐发散。

```python
show_trace(gd(1.1, f_grad), f)
```

### 局部最小值

为了演示非凸函数的梯度下降，考虑函数$f(x) = x \cdot \cos(cx)$，其中$c$为某常数。这个函数有无穷多个局部最小值。根据我们选择的学习率，我们最终可能只会得到许多解的一个。下面的例子说明了（不切实际的）高学习率如何导致较差的局部最小值。

```python
c = torch.tensor(0.15 * np.pi)

def f(x):  # 目标函数
    return x * torch.cos(c * x)

def f_grad(x):  # 目标函数的梯度
    return torch.cos(c * x) - c * x * torch.sin(c * x)

show_trace(gd(2, f_grad), f)   # 学习率为2
```

## 多元梯度下降
我们构造一个目标函数$f(\mathbf{x})=x_1^2+2x_2^2$，并有二维向量$\mathbf{x} = [x_1, x_2]^\top$作为输入，标量作为输出。梯度由$\nabla f(\mathbf{x}) = [2x_1, 4x_2]^\top$给出。我们将从初始位置$[-5, -2]$通过梯度下降观察$\mathbf{x}$的轨迹。

我们还需要两个辅助函数：第一个是update函数，并将其应用于初始值20次；第二个函数会显示$\mathbf{x}$的轨迹。

```python
def train_2d(trainer, steps=20, f_grad=None):  #@save
    """用定制的训练机优化2D目标函数"""
    # 初始参数和状态变量
    x1, x2, s1, s2 = -5, -2, 0, 0
    results = [(x1, x2)]  # 记录每次迭代后的参数值
    for i in range(steps):
        if f_grad:
            # 如果提供了梯度函数，则使用trainer进行参数更新
            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)
        else:
            # 如果没有提供梯度函数，则直接使用trainer进行参数更新
            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)
        results.append((x1, x2))  # 将更新后的参数值记录下来
    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')  # 输出最终迭代结果
    return results  # 返回所有迭代过程中的参数值记录
```

```python
def show_trace_2d(f, results):  #@save
    """显示优化过程中2D变量的轨迹"""
    d2l.set_figsize()  # 设置图的尺寸
    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')  # 绘制优化过程中参数变化的轨迹
    x1, x2 = torch.meshgrid(torch.arange(-5.5, 1.0, 0.1),
                          torch.arange(-3.0, 1.0, 0.1), indexing='ij')
    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')  # 绘制目标函数的等高线图
    d2l.plt.xlabel('x1')  # 设置x轴标签
    d2l.plt.ylabel('x2')  # 设置y轴标签
```

接下来，我们观察学习率$\eta = 0.1$时优化变量$\mathbf{x}$的轨迹。可以看到，经过20步之后，$\mathbf{x}$的值接近其位于$[0, 0]$的最小值。虽然进展相当顺利，但相当缓慢。

```python
def f_2d(x1, x2):  # 目标函数：定义一个简单的二维目标函数
    return x1 ** 2 + 2 * x2 ** 2

def f_2d_grad(x1, x2):  # 目标函数的梯度：计算目标函数的梯度
    return (2 * x1, 4 * x2)

def gd_2d(x1, x2, s1, s2, f_grad):  # 梯度下降优化器：使用梯度下降法优化目标函数
    g1, g2 = f_grad(x1, x2)
    return (x1 - eta * g1, x2 - eta * g2, 0, 0)

eta = 0.1
show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))  # 显示优化过程中的参数轨迹
```

## 自适应方法
如果我们可以自动确定$\eta$，或者完全不必选择学习率，会怎么样？除了考虑目标函数的值和梯度、还考虑它的曲率的二阶方法可以帮我们解决这个问题。虽然由于计算代价的原因，这些方法不能直接应用于深度学习，但它们为如何设计高级优化算法提供了有用的思维直觉，这些算法可以模拟下面概述的算法的许多理想特性。
### 牛顿法
给定一个凸双曲余弦函数$c$，其中$c$为某些常数，我们可以看到经过几次迭代后，得到了$x=0$处的全局最小值。

```python
c = torch.tensor(0.5)

def f(x):  # 目标函数：定义一个双曲余弦函数作为目标函数
    return torch.cosh(c * x)

def f_grad(x):  # 目标函数的梯度：计算目标函数在x处的梯度
    return c * torch.sinh(c * x)

def f_hess(x):  # 目标函数的Hessian矩阵：计算目标函数在x处的Hessian矩阵
    return c**2 * torch.cosh(c * x)

def newton(eta=1):   # 默认学习率为 1
    x = 10.0
    results = [x]
    for i in range(10):
        x -= eta * f_grad(x) / f_hess(x)  # 使用牛顿法更新x的值
        results.append(float(x))
    print('epoch 10, x:', x)  # 输出最终迭代结果
    return results

show_trace(newton(), f)  # 显示优化过程中的参数轨迹
```

现在让我们考虑一个非凸函数，比如$f(x) = x \cos(c x)$，$c$为某些常数。请注意在牛顿法中，我们最终将除以Hessian。这意味着如果二阶导数是负的，$f$的值可能会趋于增加。这是这个算法的致命缺陷！让我们看看实践中会发生什么。

```python
c = torch.tensor(0.15 * np.pi)

def f(x):  # 目标函数
    return x * torch.cos(c * x)

def f_grad(x):  # 目标函数的梯度
    return torch.cos(c * x) - c * x * torch.sin(c * x)

def f_hess(x):  # 目标函数的Hessian
    return - 2 * c * torch.sin(c * x) - x * c**2 * torch.cos(c * x)

show_trace(newton(), f)
```

这发生了惊人的错误。我们怎样才能修正它？一种方法是用取Hessian的绝对值来修正，另一个策略是重新引入学习率。这似乎违背了初衷，但不完全是——拥有二阶信息可以使我们在曲率较大时保持谨慎，而在目标函数较平坦时则采用较大的学习率。让我们看看在学习率稍小的情况下它是如何生效的，比如$\eta = 0.5$。如我们所见，我们有了一个相当高效的算法。

```python
show_trace(newton(0.5), f)
```

# 随机梯度下降

```python
%matplotlib inline
import math
import torch
from d2l import torch as d2l
```

## 随机梯度更新
平均而言，随机梯度是对梯度的良好估计。现在，我们将把它与梯度下降进行比较，方法是向梯度添加均值为0、方差为1的随机噪声，以模拟随机梯度下降。

```python
def f(x1, x2):  # 目标函数
    return x1 ** 2 + 2 * x2 ** 2

def f_grad(x1, x2):  # 目标函数的梯度
    return 2 * x1, 4 * x2
```

```python
def sgd(x1, x2, s1, s2, f_grad):
    # 计算梯度
    g1, g2 = f_grad(x1, x2)
    # 模拟有噪声的梯度
    g1 += torch.normal(0.0, 1, (1,)).item()  # 加上噪声
    g2 += torch.normal(0.0, 1, (1,)).item()  # 加上噪声
    # 计算学习率
    eta_t = eta * lr()
    # 返回更新后的变量和零值
    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)
```

```python
def constant_lr():
    return 1  # 始终返回学习速率为1

eta = 0.1  # 设定初始学习速率
lr = constant_lr  # 使用常数学习速率

# 调用 d2l.show_trace_2d 显示二维优化过程的轨迹
# 这里假设 f 和 f_grad 是已经定义的函数，sgd 是优化算法，steps=50 指定优化步数
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))
```

## 动态学习率
$$
\begin{aligned}
    \eta(t) & = \eta_i \text{ if } t_i \leq t \leq t_{i+1}  && \text{分段常数} \\
    \eta(t) & = \eta_0 \cdot e^{-\lambda t} && \text{指数衰减} \\
    \eta(t) & = \eta_0 \cdot (\beta t + 1)^{-\alpha} && \text{多项式衰减}
\end{aligned}
$$
让我们看看指数衰减在实践中是什么样子。

```python
def exponential_lr():
    global t  # 使用全局变量 t
    t += 1  # 每次调用增加 t 的值
    return math.exp(-0.1 * t)  # 返回指数衰减的学习率

t = 1  # 初始 t 值
lr = exponential_lr  # 使用指数衰减学习率

# 调用 d2l.show_trace_2d 显示二维优化过程的轨迹
# 这里假设 f 和 f_grad 是已经定义的函数，sgd 是优化算法，steps=1000 指定优化步数
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))
```

正如预期的那样，参数的方差大大减少。但是，这是以未能收敛到最优解$\mathbf{x} = (0, 0)$为代价的。即使经过1000个迭代步骤，我们仍然离最优解很远。事实上，该算法根本无法收敛。另一方面，如果我们使用多项式衰减，其中学习率随迭代次数的平方根倒数衰减，那么仅在50次迭代之后，收敛就会更好。

```python
def polynomial_lr():
    global t  # 使用全局变量 t
    t += 1  # 每次调用增加 t 的值
    return (1 + 0.1 * t) ** (-0.5)  # 返回多项式衰减的学习率

t = 1  # 初始 t 值
lr = polynomial_lr  # 使用多项式衰减学习率

# 调用 d2l.show_trace_2d 显示二维优化过程的轨迹
# 这里假设 f 和 f_grad 是已经定义的函数，sgd 是优化算法，steps=50 指定优化步数
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))
```

# 小批量随机梯度下降

```python
%matplotlib inline
import numpy as np  # 导入 NumPy 库，并使用别名 np
import torch  # 导入 PyTorch 库
from torch import nn  # 从 PyTorch 中导入神经网络模块 nn
from d2l import torch as d2l  # 从 D2L 库中导入 PyTorch 版本，并使用别名 d2l

timer = d2l.Timer()  # 使用 D2L 库中的计时器对象
A = torch.zeros(256, 256)  # 创建一个 256x256 的全零张量 A
B = torch.randn(256, 256)  # 创建一个 256x256 的随机张量 B，随机数来自标准正态分布
C = torch.randn(256, 256)  # 创建一个 256x256 的随机张量 C，随机数来自标准正态分布
```

按元素分配只需遍历分别为$\mathbf{B}$和$\mathbf{C}$的所有行和列，即可将该值分配给$\mathbf{A}$。

```python
# 逐元素计算 A = BC 的乘积
timer.start()  # 启动计时器，开始计时

for i in range(256):
    for j in range(256):
        A[i, j] = torch.dot(B[i, :], C[:, j])  # 计算 B[i,:] 和 C[:,j] 的点积，并将结果赋值给 A[i,j]

timer.stop()  # 停止计时器，结束计时
```

更快的策略是执行按列分配。

```python
# 逐列计算 A = BC 的乘积
timer.start()  # 启动计时器，开始计时

for j in range(256):
    A[:, j] = torch.mv(B, C[:, j])  # 使用 torch.mv 计算 B 与 C[:,j] 的矩阵乘法，并将结果赋值给 A 的第 j 列

timer.stop()  # 停止计时器，结束计时
```

最有效的方法是在一个区块中执行整个操作。让我们看看它们各自的操作速度是多少。

```python
# 一次性计算 A = BC
timer.start()  # 启动计时器，开始计时

A = torch.mm(B, C)  # 使用 torch.mm 一次性计算 B 与 C 的矩阵乘法，并将结果赋值给 A

timer.stop()  # 停止计时器，结束计时

# 计算性能指标 Gigaflops
gigaflops = [2 / t for t in timer.times]  # 计算每种方法的每秒十亿次浮点数运算数（Gigaflops）
print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '
      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')
```

## 小批量
在实践中我们选择一个足够大的小批量，它可以提供良好的计算效率同时仍适合GPU的内存。下面，我们来看看这些高效的代码。在里面我们执行相同的矩阵-矩阵乘法，但是这次我们将其一次性分为64列的“小批量”。

```python
# 使用分块计算 A = BC 的乘积
timer.start()  # 启动计时器，开始计时

for j in range(0, 256, 64):
    A[:, j:j+64] = torch.mm(B, C[:, j:j+64])  # 使用 torch.mm 计算 B 与 C[:, j:j+64] 的矩阵乘法，并将结果赋值给 A 的相应区块

timer.stop()  # 停止计时器，结束计时

# 计算性能指标 Gigaflops
gigaflops = 2 / timer.times[3]  # 计算每秒十亿次浮点数运算数（Gigaflops）
print(f'performance in Gigaflops: block {gigaflops:.3f}')
```

## 读取数据集

让我们来看看如何从数据中有效地生成小批量。下面我们使用NASA开发的测试机翼的数据集[不同飞行器产生的噪声](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise)来比较这些优化算法。为方便起见，我们只使用前$1,500$样本。数据已作预处理：我们移除了均值并将方差重新缩放到每个坐标为$1$。

```python
#@save
d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',
                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')

#@save
def get_data_ch11(batch_size=10, n=1500):
    # 下载 'airfoil' 数据集并将其加载为 NumPy 数组
    data = np.genfromtxt(d2l.download('airfoil'),
                         dtype=np.float32, delimiter='\t')
    # 将数据转换为 PyTorch 张量，并进行标准化处理
    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))
    # 加载数据并创建数据迭代器
    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),
                               batch_size, is_train=True)
    # 返回数据迭代器和特征数目减一的值
    return data_iter, data.shape[1]-1
```

## 从零开始实现

我们在这里将它的输入参数变得更加通用，主要是为了方便本章后面介绍的其他优化算法也可以使用同样的输入。具体来说，我们添加了一个状态输入`states`并将超参数放在字典`hyperparams`中。此外，我们将在训练函数里对各个小批量样本的损失求平均，因此优化算法中的梯度不需要除以批量大小。

```python
def sgd(params, states, hyperparams):
    # 对每个参数进行梯度下降更新
    for p in params:
        # 更新参数值：p = p - learning_rate * gradient
        p.data.sub_(hyperparams['lr'] * p.grad)
        # 清零梯度，以便下一轮计算
        p.grad.data.zero_()
```

下面实现一个通用的训练函数，以方便本章后面介绍的其他优化算法使用。它初始化了一个线性回归模型，然后可以使用小批量随机梯度下降以及后续小节介绍的其他算法来训练模型。

```python
#@save
def train_ch11(trainer_fn, states, hyperparams, data_iter,
               feature_dim, num_epochs=2):
    # 初始化模型参数
    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),
                     requires_grad=True)  # 初始化权重 w
    b = torch.zeros((1), requires_grad=True)  # 初始化偏置 b
    # 定义线性回归模型和平方损失函数
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    # 动画显示训练过程中的损失变化
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[0, num_epochs], ylim=[0.22, 0.35])
    n, timer = 0, d2l.Timer()
    # 训练模型
    for _ in range(num_epochs):
        for X, y in data_iter:
            # 计算当前批量的平均损失
            l = loss(net(X), y).mean()
            # 梯度反向传播
            l.backward()
            # 调用优化算法更新模型参数
            trainer_fn([w, b], states, hyperparams)
            n += X.shape[0]
            # 每处理完200个样本，更新动画显示的损失
            if n % 200 == 0:
                timer.stop()
                animator.add(n/X.shape[0]/len(data_iter),
                             (d2l.evaluate_loss(net, data_iter, loss),))
                timer.start()
    # 打印最终的损失值和每轮训练的平均时间
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
    return timer.cumsum(), animator.Y[0]
```

让我们来看看批量梯度下降的优化是如何进行的。这可以通过将小批量设置为1500（即样本总数）来实现。因此，模型参数每个迭代轮数只迭代一次。

```python
def train_sgd(lr, batch_size, num_epochs=2):
    data_iter, feature_dim = get_data_ch11(batch_size)  # 获取数据迭代器和特征维度
    return train_ch11(
        sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)  # 使用SGD优化算法训练模型

gd_res = train_sgd(1, 1500, 10)  # 使用学习率为1，批量大小为1500，训练10个epoch
```

当批量大小为1时，优化使用的是随机梯度下降。为了简化实现，我们选择了很小的学习率。在随机梯度下降的实验中，每当一个样本被处理，模型参数都会更新。在这个例子中，这相当于每个迭代轮数有1500次更新。可以看到，目标函数值的下降在1个迭代轮数后就变得较为平缓。尽管两个例子在一个迭代轮数内都处理了1500个样本，但实验中随机梯度下降的一个迭代轮数耗时更多。这是因为随机梯度下降更频繁地更新了参数，而且一次处理单个观测值效率较低。

```python
sgd_res = train_sgd(0.005, 1)  # 使用学习率为0.005，批量大小为1进行训练
```

最后，当批量大小等于100时，我们使用小批量随机梯度下降进行优化。每个迭代轮数所需的时间比随机梯度下降和批量梯度下降所需的时间短。

```python
mini1_res = train_sgd(0.4, 100)  # 使用学习率为0.4，批量大小为100进行训练
```

将批量大小减少到10，每个迭代轮数的时间都会增加，因为每批工作负载的执行效率变得更低。

```python
mini2_res = train_sgd(0.05, 10)  # 使用学习率为0.05，批量大小为10进行训练
```

现在我们可以比较前四个实验的时间与损失。可以看出，尽管在处理的样本数方面，随机梯度下降的收敛速度快于梯度下降，但与梯度下降相比，它需要更多的时间来达到同样的损失，因为逐个样本来计算梯度并不那么有效。小批量随机梯度下降能够平衡收敛速度和计算效率。大小为10的小批量比随机梯度下降更有效；大小为100的小批量在运行时间上甚至优于梯度下降。

```python
d2l.set_figsize([6, 3])  # 设置图形大小为6x3
d2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),
         'time (sec)', 'loss', xlim=[1e-2, 10],
         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])  # 绘制损失随时间变化的曲线图，横轴为时间（秒），纵轴为损失值，横轴范围从1e-2到10
d2l.plt.gca().set_xscale('log')  # 设置横轴为对数刻度
```

## 简洁实现

下面用深度学习框架自带算法实现一个通用的训练函数，我们将在本章中其它小节使用它。

```python
#@save
def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):
    # 初始化模型
    net = nn.Sequential(nn.Linear(5, 1))
    def init_weights(m):
        if type(m) == nn.Linear:
            torch.nn.init.normal_(m.weight, std=0.01)
    net.apply(init_weights)

    optimizer = trainer_fn(net.parameters(), **hyperparams)  # 初始化优化器
    loss = nn.MSELoss(reduction='none')  # 定义损失函数为均方误差
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                            xlim=[0, num_epochs], ylim=[0.22, 0.35])  # 创建动画记录器
    n, timer = 0, d2l.Timer()
    for _ in range(num_epochs):  # 迭代每个epoch
        for X, y in data_iter:  # 迭代每个小批量数据
            optimizer.zero_grad()  # 梯度清零
            out = net(X)  # 进行前向传播
            y = y.reshape(out.shape)  # 调整标签形状以匹配输出
            l = loss(out, y)  # 计算损失
            l.mean().backward()  # 反向传播求梯度
            optimizer.step()  # 更新模型参数
            n += X.shape[0]  # 计算累计训练样本数
            if n % 200 == 0:
                timer.stop()
                # 添加动画记录，计算验证集上的损失
                animator.add(n/X.shape[0]/len(data_iter),
                             (d2l.evaluate_loss(net, data_iter, loss) / 2,))
                timer.start()
    # 打印最终损失和每个epoch的平均时间
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')
```

下面使用这个训练函数，复现之前的实验。

```python
data_iter, _ = get_data_ch11(10)  # 获取数据迭代器，_表示不需要标签数据
trainer = torch.optim.SGD  # 定义优化器为随机梯度下降（SGD）
train_concise_ch11(trainer, {'lr': 0.01}, data_iter)  # 调用训练函数进行模型训练
```

# 动量法
### 泄漏平均值
### 条件不佳的问题
$$f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$

与之前一样，$f$在$(0, 0)$有最小值，该函数在$x_1$的方向上*非常*平坦。让我们看看在这个新函数上执行梯度下降时会发生什么

```python
%matplotlib inline  # 使用matplotlib显示图形，并在输出中显示图形

import torch  # 导入PyTorch库
from d2l import torch as d2l  # 导入d2l工具包，并使用PyTorch的实现

eta = 0.4  # 定义学习率eta

def f_2d(x1, x2):  # 定义二维函数f(x1, x2)
    return 0.1 * x1 ** 2 + 2 * x2 ** 2

def gd_2d(x1, x2, s1, s2):  # 定义二维梯度下降函数gd_2d
    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)

d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))  # 调用d2l中的函数显示二维函数的轨迹和梯度下降路径
```

从构造来看，$x_2$方向的梯度比水平$x_1$方向的梯度大得多，变化也快得多。因此，我们陷入两难：如果选择较小的学习率，我们会确保解不会在$x_2$方向发散，但要承受在$x_1$方向的缓慢收敛。相反，如果学习率较高，我们在$x_1$方向上进展很快，但在$x_2$方向将会发散。下面的例子说明了即使学习率从$0.4$略微提高到$0.6$，也会发生变化。$x_1$方向上的收敛有所改善，但整体来看解的质量更差了。

```python
eta = 0.6  # 更新学习率eta

d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))  # 调用d2l中的函数显示更新后的二维函数轨迹和梯度下降路径
```

### 动量法

*动量法*（momentum）使我们能够解决上面描述的梯度下降问题。观察上面的优化轨迹，我们可能会直觉到计算过去的平均梯度效果会很好。毕竟，在$x_1$方向上，这将聚合非常对齐的梯度，从而增加我们在每一步中覆盖的距离。相反，在梯度振荡的$x_2$方向，由于相互抵消了对方的振荡，聚合梯度将减小步长大小。使用$\mathbf{v}_t$而不是梯度$\mathbf{g}_t$可以生成以下更新等式：
$$
\begin{aligned}
\mathbf{v}_t &\leftarrow \beta \mathbf{v}_{t-1} + \mathbf{g}_{t, t-1}, \\
\mathbf{x}_t &\leftarrow \mathbf{x}_{t-1} - \eta_t \mathbf{v}_t.
\end{aligned}
$$
请注意，对于$\beta = 0$，我们恢复常规的梯度下降。在深入研究它的数学属性之前，让我们快速看一下算法在实验中的表现如何。

```python
def momentum_2d(x1, x2, v1, v2):
    v1 = beta * v1 + 0.2 * x1  # 更新v1，加入动量因子beta
    v2 = beta * v2 + 4 * x2  # 更新v2，加入动量因子beta
    return x1 - eta * v1, x2 - eta * v2, v1, v2

eta, beta = 0.6, 0.5  # 定义学习率eta和动量因子beta

d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))  # 调用d2l中的函数显示使用动量的二维函数轨迹和优化路径
```

正如所见，尽管学习率与我们以前使用的相同，动量法仍然很好地收敛了。让我们看看当降低动量参数时会发生什么。将其减半至$\beta = 0.25$会导致一条几乎没有收敛的轨迹。尽管如此，它比没有动量时解将会发散要好得多。

```python
eta, beta = 0.6, 0.25  # 更新学习率eta和动量因子beta

d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))  # 调用d2l中的函数显示更新后的二维函数轨迹和优化路径
```

请注意，我们可以将动量法与随机梯度下降，特别是小批量随机梯度下降结合起来。唯一的变化是，在这种情况下，我们将梯度$\mathbf{g}_{t, t-1}$替换为$\mathbf{g}_t$。为了方便起见，我们在时间$t=0$初始化$\mathbf{v}_0 = 0$。
### 有效样本权重
换句话说，不同于在梯度下降或者随机梯度下降中取步长$\eta$，我们选取步长$\frac{\eta}{1-\beta}$，同时处理潜在表现可能会更好的下降方向。这是集两种好处于一身的做法。为了说明$\beta$的不同选择的权重效果如何，请参考下面的图表。

```python
d2l.set_figsize()  # 设置图表大小

betas = [0.95, 0.9, 0.6, 0]
for beta in betas:
    x = torch.arange(40).detach().numpy()  # 创建一个包含40个元素的张量，并转换为NumPy数组
    d2l.plt.plot(x, beta ** x, label=f'beta = {beta:.2f}')  # 绘制指数衰减曲线，标签中显示beta的值

d2l.plt.xlabel('time')  # 设置x轴标签为时间
d2l.plt.legend();  # 显示图例
```

## 实际实验
### 从零开始实现
相比于小批量随机梯度下降，动量方法需要维护一组辅助变量，即速度。它与梯度以及优化问题的变量具有相同的形状。在下面的实现中，我们称这些变量为`states`。

```python
def init_momentum_states(feature_dim):
    v_w = torch.zeros((feature_dim, 1))  # 初始化权重的动量状态为零张量
    v_b = torch.zeros(1)  # 初始化偏置的动量状态为零张量
    return (v_w, v_b)  # 返回权重和偏置的动量状态元组
```

```python
# 带有动量的随机梯度下降优化
def sgd_momentum(params, states, hyperparams):
    for p, v in zip(params, states):
        with torch.no_grad():  # 使用torch.no_grad()上下文管理器，确保以下代码不会计算梯度
            v[:] = hyperparams['momentum'] * v + p.grad  # 计算动量更新公式
            p[:] -= hyperparams['lr'] * v  # 应用带动量的随机梯度下降更新参数
        p.grad.data.zero_()  # 清零梯度数据，准备下一轮计算
```

让我们看看它在实验中是如何运作的。

```python
def train_momentum(lr, momentum, num_epochs=2):
    # 使用带有动量的随机梯度下降算法进行训练
    d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),
                   {'lr': lr, 'momentum': momentum}, data_iter,
                   feature_dim, num_epochs)

data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)  # 获取数据迭代器和特征维度
train_momentum(0.02, 0.5)  # 调用训练函数，设置学习率为0.02，动量为0.5，训练默认2个周期
```

当我们将动量超参数`momentum`增加到0.9时，它相当于有效样本数量增加到$\frac{1}{1 - 0.9} = 10$。我们将学习率略微降至$0.01$，以确保可控。

```python
train_momentum(0.01, 0.9)  # 调用训练函数，设置学习率为0.01，动量为0.9，训练默认2个周期
```

降低学习率进一步解决了任何非平滑优化问题的困难，将其设置为$0.005$会产生良好的收敛性能。

```python
train_momentum(0.005, 0.9)   # 调用训练函数，设置学习率为0.005，动量为0.9，训练默认2个周期
```

### 简洁实现

由于深度学习框架中的优化求解器早已构建了动量法，设置匹配参数会产生非常类似的轨迹。

```python
trainer = torch.optim.SGD  # 设置优化器为随机梯度下降（SGD）
d2l.train_concise_ch11(trainer, {'lr': 0.005, 'momentum': 0.9}, data_iter)  # 使用优化器训练模型，设置学习率为0.005，动量为0.9，使用给定的数据迭代器
```

## 理论分析
### 二次凸函数
### 标量函数

```python
lambdas = [0.1, 1, 10, 19]  # 设置不同的 lambda 值列表
eta = 0.1  # 设置 学习率为 0.1
d2l.set_figsize((6, 4))  # 设置图形大小为 (6, 4)
for lam in lambdas:
    t = torch.arange(20).detach().numpy()  # 生成从 0 到 19 的整数序列，并转换为 numpy 数组
    d2l.plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')  # 绘制曲线，其中 x 轴为时间 t，y 轴为 (1 - eta * lambda) 的 t 次方
d2l.plt.xlabel('time')  # 设置 x 轴标签为 'time'
d2l.plt.legend();  # 显示图例
```

# AdaGrad算法
## 稀疏特征和学习率
## 预处理
## 算法
眼下让我们先看看它在二次凸问题中的表现如何。我们仍然以同一函数为例：

$$f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$

我们将使用与之前相同的学习率来实现AdaGrad算法，即$\eta = 0.4$。可以看到，自变量的迭代轨迹较平滑。但由于$\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。

```python
%matplotlib inline
import math
import torch
from d2l import torch as d2l

def adagrad_2d(x1, x2, s1, s2):
    eps = 1e-6  # 设置小常数 eps 避免除以零
    g1, g2 = 0.2 * x1, 4 * x2  # 计算梯度 g1 和 g2
    s1 += g1 ** 2  # 更新累积平方梯度 s1
    s2 += g2 ** 2  # 更新累积平方梯度 s2
    x1 -= eta / math.sqrt(s1 + eps) * g1  # 更新变量 x1
    x2 -= eta / math.sqrt(s2 + eps) * g2  # 更新变量 x2
    return x1, x2, s1, s2

def f_2d(x1, x2):
    return 0.1 * x1 ** 2 + 2 * x2 ** 2  # 定义二维函数 f(x1, x2)

eta = 0.4  # 设置学习率 eta
d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))  # 显示优化路径
```

我们将学习率提高到$2$，可以看到更好的表现。这已经表明，即使在无噪声的情况下，学习率的降低可能相当剧烈，我们需要确保参数能够适当地收敛。

```python
eta = 2  # 设置学习率 eta 为 2
d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))  # 显示优化路径，使用 Adagrad 算法优化 f_2d 函数
```

## 从零开始实现

同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。

```python
def init_adagrad_states(feature_dim):
    s_w = torch.zeros((feature_dim, 1))  # 初始化权重的累积平方梯度为零向量
    s_b = torch.zeros(1)  # 初始化偏置的累积平方梯度为零标量
    return (s_w, s_b)

def adagrad(params, states, hyperparams):
    eps = 1e-6   # 设置小常数 eps 避免除以零
    for p, s in zip(params, states):
        with torch.no_grad():
            s[:] += torch.square(p.grad)  # 累积平方梯度更新
            p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)  # 参数更新公式
        p.grad.data.zero_()  # 梯度清零，准备下一次迭代
```

与 11.5中的实验相比，这里使用更大的学习率来训练模型。

```python
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)  # 获取数据迭代器和特征维度

# 使用Adagrad优化算法进行模型训练
d2l.train_ch11(adagrad, init_adagrad_states(feature_dim),
               {'lr': 0.1}, data_iter, feature_dim);
```

## 简洁实现

我们可直接使用深度学习框架中提供的AdaGrad算法来训练模型。

```python
trainer = torch.optim.Adagrad  # 使用Adagrad优化器

# 使用简洁实现的方式进行模型训练
d2l.train_concise_ch11(trainer, {'lr': 0.1}, data_iter)
```

# RMSProp算法

```python
import math
import torch
from d2l import torch as d2l
```

```python
d2l.set_figsize()  # 设置图表尺寸
gammas = [0.95, 0.9, 0.8, 0.7]  # 不同的 gamma 值列表
for gamma in gammas:
    x = torch.arange(40).detach().numpy()  # 创建一个包含0到39的张量并转换为NumPy数组
    d2l.plt.plot(x, (1-gamma) * gamma ** x, label=f'gamma = {gamma:.2f}')  # 绘制曲线，gamma的函数
d2l.plt.xlabel('time');  # 设置x轴标签为'time'

```

## 从零开始实现

和之前一样，我们使用二次函数$f(\mathbf{x})=0.1x_1^2+2x_2^2$来观察RMSProp算法的轨迹。当我们使用学习率为0.4的Adagrad算法时，变量在算法的后期阶段移动非常缓慢，因为学习率衰减太快。RMSProp算法中不会发生这种情况，因为$\eta$是单独控制的。

```python
def rmsprop_2d(x1, x2, s1, s2):
    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6
    s1 = gamma * s1 + (1 - gamma) * g1 ** 2  # 更新 s1
    s2 = gamma * s2 + (1 - gamma) * g2 ** 2  # 更新 s2
    x1 -= eta / math.sqrt(s1 + eps) * g1  # 更新 x1
    x2 -= eta / math.sqrt(s2 + eps) * g2  # 更新 x2
    return x1, x2, s1, s2

def f_2d(x1, x2):
    return 0.1 * x1 ** 2 + 2 * x2 ** 2

eta, gamma = 0.4, 0.9  # 学习率 eta 和衰减参数 gamma
d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))  # 显示优化轨迹
```

接下来，我们在深度网络中实现RMSProp算法。

```python
def init_rmsprop_states(feature_dim):
    s_w = torch.zeros((feature_dim, 1))  # 初始化权重参数的状态
    s_b = torch.zeros(1)  # 初始化偏置参数的状态
    return (s_w, s_b)  # 返回初始化的状态元组
```

```python
def rmsprop(params, states, hyperparams):
    gamma, eps = hyperparams['gamma'], 1e-6
    
    # 遍历每个参数和其对应的状态
    for p, s in zip(params, states):
        with torch.no_grad():
            # 更新状态 s，使用RMSProp的状态更新公式
            s[:] = gamma * s + (1 - gamma) * torch.square(p.grad)
            
            # 更新参数 p，使用学习率除以状态 s 的平方根，并乘以梯度的负数
            p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)
        
        # 清零参数的梯度，以便下一次迭代
        p.grad.data.zero_()
```

我们将初始学习率设置为0.01，加权项$\gamma$设置为0.9。也就是说，$\mathbf{s}$累加了过去的$1/(1-\gamma) = 10$次平方梯度观测值的平均值。

```python
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)

# 初始化 RMSProp 的状态
init_states = init_rmsprop_states(feature_dim)

# 使用 RMSProp 进行训练
d2l.train_ch11(rmsprop, init_states, {'lr': 0.01, 'gamma': 0.9}, data_iter, feature_dim);
```

## 简洁实现

我们可直接使用深度学习框架中提供的RMSProp算法来训练模型。

```python
trainer = torch.optim.RMSprop  # 定义优化器为RMSprop

# 使用d2l库中的train_concise_ch11函数进行训练
d2l.train_concise_ch11(trainer, {'lr': 0.01, 'alpha': 0.9}, data_iter)
```

# Adadelta
## 代码实现

Adadelta需要为每个变量维护两个状态变量，即$\mathbf{s}_t$和$\Delta\mathbf{x}_t$。这将产生以下实现。

```python
%matplotlib inline  # 在Jupyter Notebook中使用matplotlib的内联显示模式

import torch  # 导入PyTorch库
from d2l import torch as d2l  # 从d2l.torch模块导入d2l

def init_adadelta_states(feature_dim):
    s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)  # 初始化权重和偏置的状态变量
    delta_w, delta_b = torch.zeros((feature_dim, 1)), torch.zeros(1)  # 初始化权重和偏置的增量变量
    return ((s_w, delta_w), (s_b, delta_b))  # 返回权重和偏置的状态和增量

def adadelta(params, states, hyperparams):
    rho, eps = hyperparams['rho'], 1e-5  # 从超参数中获取rho和eps的值
    for p, (s, delta) in zip(params, states):
        with torch.no_grad():
            # In-place updates via [:]
            s[:] = rho * s + (1 - rho) * torch.square(p.grad)  # 更新状态s
            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad  # 计算更新步长g
            p[:] -= g  # 执行参数更新
            delta[:] = rho * delta + (1 - rho) * g * g  # 更新增量delta
        p.grad.data.zero_()  # 清零梯度数据
```

对于每次参数更新，选择$\rho = 0.9$相当于10个半衰期。由此我们得到：

```python
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)  # 调用d2l库中的get_data_ch11函数获取数据迭代器和特征维度

# 使用train_ch11函数进行模型训练，传入优化函数adadelta、初始化状态函数init_adadelta_states返回的状态、超参数rho、数据迭代器、特征维度
d2l.train_ch11(adadelta, init_adadelta_states(feature_dim), {'rho': 0.9}, data_iter, feature_dim)
```

为了简洁实现，我们只需使用高级API中的Adadelta算法。

```python
trainer = torch.optim.Adadelta  # 定义Adadelta优化器，但这里没有实例化，只是将优化器类赋给trainer变量

# 使用d2l库中的train_concise_ch11函数进行模型训练，
# 传入优化器类trainer（此处应为实例化的对象，而非类本身）、超参数rho的字典、数据迭代器data_iter
d2l.train_concise_ch11(trainer, {'rho': 0.9}, data_iter)
```

# Adam算法
## 实现

从头开始实现Adam算法并不难。为方便起见，我们将时间步$t$存储在`hyperparams`字典中。除此之外，一切都很简单。

```python
%matplotlib inline  # 在Jupyter Notebook中显示matplotlib图形

import torch  # 导入PyTorch库
from d2l import torch as d2l  # 从d2l库中导入torch模块，起别名为d2l

def init_adam_states(feature_dim):
    v_w, v_b = torch.zeros((feature_dim, 1)), torch.zeros(1)  # 初始化权重的一阶和二阶矩估计为零向量和零标量
    s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)  # 初始化权重的二阶矩估计为零向量和零标量
    return ((v_w, s_w), (v_b, s_b))  # 返回权重和偏置的一阶和二阶矩估计的元组

def adam(params, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-6
    for p, (v, s) in zip(params, states):
        with torch.no_grad():
            v[:] = beta1 * v + (1 - beta1) * p.grad  # 更新v的指数加权移动平均值
            s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad)  # 更新s的指数加权移动平均值
            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])  # 计算偏差校正后的v
            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])  # 计算偏差校正后的s
            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)  # 更新参数p
        p.grad.data.zero_()  # 梯度清零
    hyperparams['t'] += 1  # 更新时间步计数器t

```

现在，我们用以上Adam算法来训练模型，这里我们使用$\eta = 0.01$的学习率。

```python
data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)  # 获取数据迭代器和特征维度
d2l.train_ch11(adam, init_adam_states(feature_dim),
               {'lr': 0.01, 't': 1}, data_iter, feature_dim);  # 使用Adam优化算法进行训练，初始化Adam状态，设置学习率为0.01和时间步t为1
```

此外，我们可以用深度学习框架自带算法应用Adam算法，这里我们只需要传递配置参数。

```python
trainer = torch.optim.Adam  # 使用PyTorch中的Adam优化器作为训练器
d2l.train_concise_ch11(trainer, {'lr': 0.01}, data_iter)  # 使用Adam优化器以学习率0.01对数据迭代器进行精简训练
```

## Yogi

```python
def yogi(params, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-3
    for p, (v, s) in zip(params, states):
        with torch.no_grad():
            v[:] = beta1 * v + (1 - beta1) * p.grad  # 更新一阶矩估计
            s[:] = s + (1 - beta2) * torch.sign(
                torch.square(p.grad) - s) * torch.square(p.grad)  # 更新二阶矩估计
            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])  # 一阶矩估计的偏差修正
            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])  # 二阶矩估计的偏差修正
            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)
                                                       + eps)  # 更新模型参数
        p.grad.data.zero_()  # 梯度清零
    hyperparams['t'] += 1  # 更新时间步

data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)  # 获取数据迭代器和特征维度
d2l.train_ch11(yogi, init_adam_states(feature_dim),
               {'lr': 0.01, 't': 1}, data_iter, feature_dim);  # 使用Yogi优化算法进行训练，初始化Yogi状态，设置学习率为0.01和时间步t为1
```

# 学习率调度器
## 一个简单的问题

我们从一个简单的问题开始，这个问题可以轻松计算，但足以说明要义。为此，我们选择了一个稍微现代化的LeNet版本（激活函数使用`relu`而不是`sigmoid`，汇聚层使用最大汇聚层而不是平均汇聚层），并应用于Fashion-MNIST数据集。此外，我们混合网络以提高性能。由于大多数代码都是标准的，我们只介绍基础知识，而不做进一步的详细讨论。

```python
%matplotlib inline
import math
import torch
from torch import nn
from torch.optim import lr_scheduler
from d2l import torch as d2l

def net_fn():
    model = nn.Sequential(
        nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),  # 第一层卷积和ReLU激活函数
        nn.MaxPool2d(kernel_size=2, stride=2),  # 最大池化层
        nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),  # 第二层卷积和ReLU激活函数
        nn.MaxPool2d(kernel_size=2, stride=2),  # 最大池化层
        nn.Flatten(),  # 扁平化层，将多维数据展平成一维
        nn.Linear(16 * 5 * 5, 120), nn.ReLU(),  # 全连接层和ReLU激活函数
        nn.Linear(120, 84), nn.ReLU(),  # 全连接层和ReLU激活函数
        nn.Linear(84, 10))  # 输出层，输出类别数

    return model

loss = nn.CrossEntropyLoss()  # 交叉熵损失函数
device = d2l.try_gpu()  # 尝试使用GPU，如果没有则使用CPU

batch_size = 256   # 批量大小
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)  # 加载Fashion MNIST数据集

# 代码几乎与d2l.train_ch6定义在卷积神经网络一章LeNet一节中的相同
def train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
          scheduler=None):
    net.to(device)  # 将模型移动到设备（GPU或CPU）
    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])  # 创建动画记录训练过程中的损失和准确率

    for epoch in range(num_epochs):
        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples（累计器）
        for i, (X, y) in enumerate(train_iter):
            net.train()  # 设置模型为训练模式
            trainer.zero_grad()  # 梯度清零
            X, y = X.to(device), y.to(device)  # 将数据移到设备上
            y_hat = net(X)  # 前向传播计算预测值
            l = loss(y_hat, y)  # 计算损失
            l.backward()  # 反向传播计算梯度
            trainer.step()  # 更新模型参数
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])  # 累加训练损失、准确率和样本数
            train_loss = metric[0] / metric[2]  # 平均训练损失
            train_acc = metric[1] / metric[2]  # 平均训练准确率
            if (i + 1) % 50 == 0:
                animator.add(epoch + i / len(train_iter),
                             (train_loss, train_acc, None))  # 记录动画中的训练损失和准确率

        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)  # 在测试集上评估模型准确率
        animator.add(epoch + 1, (None, None, test_acc))  # 记录动画中的测试准确率

        if scheduler:
            if scheduler.__module__ == lr_scheduler.__name__:
                # 使用PyTorch内置的学习率调度器
                scheduler.step()
            else:
                # 使用自定义的学习率调度器
                for param_group in trainer.param_groups:
                    param_group['lr'] = scheduler(epoch)

    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')  # 打印最终的训练损失和准确率
```

让我们来看看如果使用默认设置，调用此算法会发生什么。例如设学习率为$0.3$并训练$30$次迭代。留意在超过了某点、测试准确度方面的进展停滞时，训练准确度将如何继续提高。两条曲线之间的间隙表示过拟合。

```python
lr, num_epochs = 0.3, 30  # 学习率和训练轮数
net = net_fn()  # 初始化神经网络模型
trainer = torch.optim.SGD(net.parameters(), lr=lr)  # 定义优化器，使用随机梯度下降(SGD)算法
train(net, train_iter, test_iter, num_epochs, loss, trainer, device)  # 开始训练模型 
```

## 学习率调度器

我们可以在每个迭代轮数（甚至在每个小批量）之后向下调整学习率。例如，以动态的方式来响应优化的进展情况。

```python
lr = 0.1  # 设置学习率为0.1
trainer.param_groups[0]["lr"] = lr  # 更新优化器中第一个参数组的学习率
print(f'learning rate is now {trainer.param_groups[0]["lr"]:.2f}')  # 打印更新后的学习率
```

更通常而言，我们应该定义一个调度器。当调用更新次数时，它将返回学习率的适当值。让我们定义一个简单的方法，将学习率设置为$\eta = \eta_0 (t + 1)^{-\frac{1}{2}}$。

```python
class SquareRootScheduler:
    def __init__(self, lr=0.1):
        self.lr = lr  # 初始化学习率

    def __call__(self, num_update):
        return self.lr * pow(num_update + 1.0, -0.5)  # 根据更新次数调整学习率的计算方法
```

让我们在一系列值上绘制它的行为。

```python
scheduler = SquareRootScheduler(lr=0.1)  # 初始化学习率调度器对象
d2l.plot(torch.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])  # 绘制学习率随更新次数变化的图表
```

现在让我们来看看这对在Fashion-MNIST数据集上的训练有何影响。我们只是提供调度器作为训练算法的额外参数。

```python
net = net_fn()  # 初始化神经网络模型
trainer = torch.optim.SGD(net.parameters(), lr)  # 定义随机梯度下降(SGD)优化器
train(net, train_iter, test_iter, num_epochs, loss, trainer, device,
      scheduler)  # 使用定义好的网络模型、数据迭代器、损失函数、优化器和调度器进行训练
```

## 策略
### 单因子调度器
多项式衰减的一种替代方案是乘法衰减，即$\eta_{t+1} \leftarrow \eta_t \cdot \alpha$其中$\alpha \in (0, 1)$。为了防止学习率衰减到一个合理的下界之下，更新方程经常修改为$\eta_{t+1} \leftarrow \mathop{\mathrm{max}}(\eta_{\mathrm{min}}, \eta_t \cdot \alpha)$。

```python
class FactorScheduler:
    def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.1):
        self.factor = factor  # 衰减因子
        self.stop_factor_lr = stop_factor_lr  # 停止衰减的学习率下限
        self.base_lr = base_lr  # 初始学习率

    def __call__(self, num_update):
        self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)  # 根据衰减因子调整学习率
        return self.base_lr

scheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0)  # 初始化衰减学习率调度器对象
d2l.plot(torch.arange(50), [scheduler(t) for t in range(50)])  # 绘制学习率随更新次数变化的图表
```

接下来，我们将使用内置的调度器，但在这里仅解释它们的功能。
### 多因子调度器
训练深度网络的常见策略之一是保持学习率为一组分段的常量，并且不时地按给定的参数对学习率做乘法衰减。具体地说，给定一组降低学习率的时间点，例如$s = \{5, 10, 20\}$，每当$t \in s$时，降低$\eta_{t+1} \leftarrow \eta_t \cdot \alpha$。假设每步中的值减半，我们可以按如下方式实现这一点。

```python
net = net_fn()  # 初始化神经网络模型
trainer = torch.optim.SGD(net.parameters(), lr=0.5)  # 定义随机梯度下降(SGD)优化器
scheduler = lr_scheduler.MultiStepLR(trainer, milestones=[15, 30], gamma=0.5)  # 多步学习率调度器

def get_lr(trainer, scheduler):
    lr = scheduler.get_last_lr()[0]  # 获取当前学习率
    trainer.step()  # 执行优化器的步骤，更新模型参数
    scheduler.step()  # 更新学习率调度器状态
    return lr

d2l.plot(torch.arange(num_epochs), [get_lr(trainer, scheduler) for t in range(num_epochs)])
```

这种分段恒定学习率调度背后的直觉是，让优化持续进行，直到权重向量的分布达到一个驻点。此时，我们才将学习率降低，以获得更高质量的代理来达到一个良好的局部最小值。下面的例子展示了如何使用这种方法产生更好的解决方案。

```python
train(net, train_iter, test_iter, num_epochs, loss, trainer, device, scheduler)  
# 调用训练函数，传入网络、训练数据迭代器、测试数据迭代器、训练轮数、损失函数、优化器、设备、学习率调度器
```

### 余弦调度器

余弦调度器所依据的观点是：我们可能不想在一开始就太大地降低学习率，而且可能希望最终能用非常小的学习率来“改进”解决方案。这产生了一个类似于余弦的调度，函数形式如下所示，学习率的值在$t \in [0, T]$之间。
$$\eta_t = \eta_T + \frac{\eta_0 - \eta_T}{2} \left(1 + \cos(\pi t/T)\right)$$
这里$\eta_0$是初始学习率，$\eta_T$是当$T$时的目标学习率。此外，对于$t > T$，我们只需将值固定到$\eta_T$而不再增加它。在下面的示例中，我们设置了最大更新步数$T = 20$。

```python
class CosineScheduler:
    def __init__(self, max_update, base_lr=0.01, final_lr=0,
               warmup_steps=0, warmup_begin_lr=0):
        self.base_lr_orig = base_lr  # 原始基础学习率
        self.max_update = max_update  # 最大更新步数
        self.final_lr = final_lr  # 最终学习率
        self.warmup_steps = warmup_steps  # 热身步数
        self.warmup_begin_lr = warmup_begin_lr  # 热身开始学习率
        self.max_steps = self.max_update - self.warmup_steps  # 最大步数

    def get_warmup_lr(self, epoch):
        increase = (self.base_lr_orig - self.warmup_begin_lr) \
                       * float(epoch) / float(self.warmup_steps)
        return self.warmup_begin_lr + increase  # 返回热身阶段的学习率

    def __call__(self, epoch):
        if epoch < self.warmup_steps:
            return self.get_warmup_lr(epoch)  # 如果在热身阶段，返回热身学习率
        if epoch <= self.max_update:
            self.base_lr = self.final_lr + (
                self.base_lr_orig - self.final_lr) * (1 + math.cos(
                math.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2
        return self.base_lr  # 返回当前学习率

scheduler = CosineScheduler(max_update=20, base_lr=0.3, final_lr=0.01)   # 学习率调度器
d2l.plot(torch.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])  # 绘制学习率调度器的变化曲线
```

在计算机视觉的背景下，这个调度方式可能产生改进的结果。但请注意，如下所示，这种改进并不一定成立。

```python
net = net_fn()  # 创建神经网络
trainer = torch.optim.SGD(net.parameters(), lr=0.3)  # 使用随机梯度下降作为优化器，学习率为0.3
scheduler = CosineScheduler(max_update=20, base_lr=0.3, final_lr=0.01)  # 创建余弦学习率调度器
train(net, train_iter, test_iter, num_epochs, loss, trainer, device, scheduler)  # 调用训练函数，传入神经网络、训练数据迭代器、测试数据迭代器、训练轮数、损失函数、优化器、设备和学习率调度器
```

