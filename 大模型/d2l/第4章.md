- 常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。

## 多层感知机的从零开始实现

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

'''初始化模型参数'''
# 单隐藏层
# 定义神经网络的输入、输出和隐藏层大小
num_inputs, num_outputs, num_hiddens = 784, 10, 256

# 定义第一个全连接层的权重和偏置，并将其作为可学习的参数
W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)  # 权重初始化为随机值，形状为(num_inputs, num_hiddens)，并设置需要计算梯度
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))  # 偏置初始化为零向量，长度为num_hiddens，并设置需要计算梯度

# 定义第二个全连接层的权重和偏置，并将其作为可学习的参数
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)  # 权重初始化为随机值，形状为(num_hiddens, num_outputs)，并设置需要计算梯度
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))  # 偏置初始化为零向量，长度为num_outputs，并设置需要计算梯度

# 将所有的参数放入列表中，方便后续使用优化器进行优化
params = [W1, b1, W2, b2]


'''自定义 ReLu 激活函数'''
def relu(X):
    a = torch.zeros_like(X)  # 创建一个与输入张量 X 相同大小的零张量 a
    return torch.max(X, a)   # 返回 X 与 a 逐元素比较后的最大值张量，即 ReLU 激活函数的结果
  

'''模型'''
def net(X):
    X = X.reshape((-1, num_inputs))  # 将输入张量 X 重新调整形状为 (-1, num_inputs)，其中 -1 表示自动计算维度
    H = relu(X @ W1 + b1)            # 计算第一层隐藏层的输出，使用 ReLU 激活函数
                                    # 这里的 @ 表示矩阵乘法
    return (H @ W2 + b2)            # 返回神经网络输出层的结果，未经过激活函数处理

  
'''损失函数'''
# 定义交叉熵损失函数，reduction='none'表示不进行降维或聚合，返回每个样本的损失值
loss = nn.CrossEntropyLoss(reduction='none')


'''训练'''
num_epochs, lr = 10, 0.1  # 设置训练的总轮数和学习率
updater = torch.optim.SGD(params, lr=lr)  # 使用 SGD 优化器，传入参数和学习率

# 调用函数 train_ch3 进行模型训练
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)


'''预测'''
predictions = d2l.predict_ch3(net, test_iter)  # 使用训练好的模型net对测试集test_iter进行预测
```

## 多层感知机的简洁实现

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(nn.Flatten(),               # 将输入展平为向量
                    nn.Linear(784, 256),        # 输入大小为784，输出大小为256的全连接层
                    nn.ReLU(),                  # ReLU激活函数
                    nn.Linear(256, 10))         # 输入大小为256，输出大小为10的全连接层（输出层）

def init_weights(m):
    if type(m) == nn.Linear:                    # 如果模块是线性层
        nn.init.normal_(m.weight, std=0.01)     # 使用正态分布初始化权重，标准差为0.01

net.apply(init_weights);                        # 对模型net应用初始化函数init_weights


batch_size, lr, num_epochs = 256, 0.1, 10       # 设置批量大小、学习率和训练轮数
loss = nn.CrossEntropyLoss(reduction='none')    # 使用交叉熵损失函数，不对每个样本的损失求平均
trainer = torch.optim.SGD(net.parameters(), lr=lr)  # 使用随机梯度下降优化器SGD，传入模型net的参数和学习率lr


train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  # 加载Fashion-MNIST数据集并设置批量大小为batch_size
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)  # 使用d2l库中的train_ch3函数训练模型net，
                                                                      # 使用训练数据集train_iter和测试数据集test_iter，
                                                                      # 使用损失函数loss，训练轮数num_epochs，优化器trainer
    
    
d2l.predict_ch3(net, test_iter)  # 预测
```

## 暂退法

### 从零开始实现

```python
import torch
from torch import nn
from d2l import torch as d2l

def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1  # 确保丢弃率在合理范围内
    if dropout == 1:
        return torch.zeros_like(X)  # 如果丢弃率为1，则全部丢弃
    if dropout == 0:
        return X  # 如果丢弃率为0，则不进行丢弃

    # 生成与 X 相同形状的随机掩码，并缩放以保持期望值
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)
  
  
# 定义模型的输入维度、输出维度以及两个隐藏层的神经元数目
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256


dropout1, dropout2 = 0.2, 0.5  # 定义两个dropout的概率

class Net(nn.Module):
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
                 is_training=True):
        super(Net, self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        # 定义神经网络的各层结构
        self.lin1 = nn.Linear(num_inputs, num_hiddens1)  # 第一个全连接层
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)  # 第二个全连接层
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)  # 输出层
        self.relu = nn.ReLU()  # 激活函数使用ReLU

    def forward(self, X):
        # 前向传播函数定义
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        # 只有在训练模型时才使用dropout
        if self.training == True:
            # 在第一个全连接层之后添加一个dropout层
            H1 = dropout_layer(H1, dropout1)
        H2 = self.relu(self.lin2(H1))
        if self.training == True:
            # 在第二个全连接层之后添加一个dropout层
            H2 = dropout_layer(H2, dropout2)
        out = self.lin3(H2)
        return out

# 创建网络实例
net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)


num_epochs, lr, batch_size = 10, 0.5, 256  # 定义训练轮数、学习率和批量大小
loss = nn.CrossEntropyLoss(reduction='none')  # 定义损失函数为交叉熵损失，不进行平均或求和
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  # 加载Fashion MNIST数据集的训练和测试迭代器
trainer = torch.optim.SGD(net.parameters(), lr=lr)  # 定义随机梯度下降优化器，用于更新网络参数
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)  # 调用训练函数进行模型训练
```

### 简洁实现

```python
net = nn.Sequential(
    nn.Flatten(),  # 将输入展平为一维向量，适合全连接层处理
    nn.Linear(784, 256),  # 第一个全连接层，输入大小为784，输出大小为256
    nn.ReLU(),  # ReLU激活函数，增加网络的非线性特性
    nn.Dropout(dropout1),  # 在第一个全连接层后添加一个Dropout层，用于防止过拟合

    nn.Linear(256, 256),  # 第二个全连接层，输入大小为256，输出大小为256
    nn.ReLU(),  # ReLU激活函数，增加网络的非线性特性
    nn.Dropout(dropout2),  # 在第二个全连接层后添加一个Dropout层，用于防止过拟合

    nn.Linear(256, 10)  # 输出层，输入大小为256，输出大小为10（对应10个类别）
)

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)  # 对所有全连接层的权重进行正态分布初始化，标准差为0.01

net.apply(init_weights);  # 对网络应用初始化权重的函数，确保所有全连接层的权重被正确初始化


# 创建一个随机梯度下降（SGD）优化器，用于更新神经网络中的参数
trainer = torch.optim.SGD(net.parameters(), lr=lr)

# 调用自定义的训练函数 train_ch3 进行模型训练
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## 实战Kaggle比赛：预测房价

```python
import hashlib  # 导入哈希库
import os  # 导入操作系统库
import tarfile  # 导入tar文件处理库
import zipfile  # 导入zip文件处理库
import requests  # 导入网络请求库

DATA_HUB = dict()		# 数据集存储字典
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'		# 数据下载的基础URL

def download(name, cache_dir=os.path.join('..', 'data')):
    """下载一个DATA_HUB中的文件，返回本地文件名"""
    assert name in DATA_HUB, f"{name} 不存在于 {DATA_HUB}"
    url, sha1_hash = DATA_HUB[name]
    
    # 确保缓存目录存在，如果不存在则创建
    os.makedirs(cache_dir, exist_ok=True)
    
    # 拼接文件名，使用URL的最后一部分作为文件名
    fname = os.path.join(cache_dir, url.split('/')[-1])
    
    # 如果文件已经存在且校验通过，则直接返回文件名
    if os.path.exists(fname):
        sha1 = hashlib.sha1()
        with open(fname, 'rb') as f:
            while True:
                data = f.read(1048576)  # 每次读取1MB数据
                if not data:
                    break
                sha1.update(data)
        if sha1.hexdigest() == sha1_hash:
            return fname  # 文件完整且未被篡改，命中缓存
    
    # 否则开始下载文件
    print(f'正在从{url}下载{fname}...')
    r = requests.get(url, stream=True, verify=True)  # 发起GET请求下载文件，开启流式传输
    with open(fname, 'wb') as f:
        f.write(r.content)  # 将下载的内容写入本地文件
    
    return fname  # 返回下载后的文件名
  
def download_extract(name, folder=None):  #@save
    """下载并解压zip/tar文件
    Args:
        name (str): 要下载的文件名，应存在于DATA_HUB中
        folder (str, optional): 解压后文件夹的名称。默认为None。
    """
    fname = download(name)  # 下载文件到本地并获取文件名
    base_dir = os.path.dirname(fname)  # 获取文件所在的目录
    data_dir, ext = os.path.splitext(fname)  # 获取文件的扩展名
    if ext == '.zip':
        fp = zipfile.ZipFile(fname, 'r')  # 打开zip文件
    elif ext in ('.tar', '.gz'):
        fp = tarfile.open(fname, 'r')  # 打开tar文件
    else:
        assert False, '只有zip/tar文件可以被解压缩'  # 如果文件不是zip或tar格式，则报错
    fp.extractall(base_dir)  # 解压文件到指定目录
    return os.path.join(base_dir, folder) if folder else data_dir  # 返回解压后文件夹的路径或解压后的数据文件夹路径

def download_all():
    """下载DATA_HUB中的所有文件"""   #@save
    for name in DATA_HUB:
        download(name)  # 循环下载DATA_HUB中的每个文件
        
        
# 如果没有安装pandas，请取消下一行的注释
# !pip install pandas
%matplotlib inline
import numpy as np  # 导入NumPy库，用于数值计算
import pandas as pd  # 导入Pandas库，用于数据处理和分析
import torch  # 导入PyTorch库，用于深度学习
from torch import nn  # 导入PyTorch中的神经网络模块
from d2l import torch as d2l  # 导入d2l-torch工具包，用于深度学习教学

DATA_HUB['kaggle_house_train'] = (  #@save
    DATA_URL + 'kaggle_house_pred_train.csv',
    '585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (  #@save
    DATA_URL + 'kaggle_house_pred_test.csv',
    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

train_data = pd.read_csv(download('kaggle_house_train'))  # 从指定的数据源下载并读取训练数据集
test_data = pd.read_csv(download('kaggle_house_test'))  # 从指定的数据源下载并读取测试数据集

print(train_data.shape)		# 打印数据集的形状
print(test_data.shape)

print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])	# 打印训练数据集的前四个样本的某些特征

# 将训练数据和测试数据的特征（排除第一列和最后一列）合并成一个新的 DataFrame all_features
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:])) 

# 若无法获得测试数据，则可根据训练数据计算均值和标准差
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index

# 对所有数值特征进行标准化：减去均值并除以标准差
all_features[numeric_features] = all_features[numeric_features].apply(
    lambda x: (x - x.mean()) / x.std())

# 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0
all_features[numeric_features] = all_features[numeric_features].fillna(0)

# “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征
all_features = pd.get_dummies(all_features, dummy_na=True)
all_features.shape

n_train = train_data.shape[0]  # 获取训练集样本数量
train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)  # 将训练集特征转换为PyTorch张量
test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)  # 将测试集特征转换为PyTorch张量
train_labels = torch.tensor(
    train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)  # 将训练集标签转换为PyTorch张量


loss = nn.MSELoss()  # 定义均方误差损失函数

in_features = train_features.shape[1]  # 获取输入特征的数量

def get_net():
    net = nn.Sequential(nn.Linear(in_features, 1))  # 定义一个包含单个线性层的神经网络模型
    return net  # 返回定义好的神经网络模型
  
  
def log_rmse(net, features, labels):
    # 为了在取对数时进一步稳定值，将小于1的预测值截断为1
    clipped_preds = torch.clamp(net(features), 1, float('inf'))  # 使用神经网络进行预测，并将小于1的预测值截断为1
    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))  # 计算对数均方根误差
    return rmse.item()  # 返回对数均方根误差的标量数值
  
  
def train(net, train_features, train_labels, test_features, test_labels,
          num_epochs, learning_rate, weight_decay, batch_size):
    train_ls, test_ls = [], []  # 初始化训练集和测试集的损失列表
    train_iter = d2l.load_array((train_features, train_labels), batch_size)  # 创建训练数据迭代器
    optimizer = torch.optim.Adam(net.parameters(),  # 使用Adam优化算法进行参数优化
                                 lr=learning_rate,  # 学习率
                                 weight_decay=weight_decay)  # 权重衰减（L2正则化）
    
    for epoch in range(num_epochs):  # 迭代训练 num_epochs 次
        for X, y in train_iter:  # 从训练数据迭代器中获取每个小批量的特征 X 和标签 y
            optimizer.zero_grad()  # 梯度清零，避免梯度累加
            l = loss(net(X), y)  # 计算当前小批量的损失
            l.backward()  # 反向传播，计算梯度
            optimizer.step()  # 更新参数
            
        # 计算当前 epoch 后的训练集和测试集上的对数均方根误差，并记录到列表中
        train_ls.append(log_rmse(net, train_features, train_labels))
        if test_labels is not None:
            test_ls.append(log_rmse(net, test_features, test_labels))
    
    return train_ls, test_ls  # 返回训练集和测试集每个 epoch 的对数均方根误差列表


  
def get_k_fold_data(k, i, X, y):
    assert k > 1  # 确保k大于1，因为k-fold交叉验证需要至少2折

    fold_size = X.shape[0] // k  # 计算每折的样本数量

    X_train, y_train = None, None
    X_valid, y_valid = None, None

    for j in range(k):
        idx = slice(j * fold_size, (j + 1) * fold_size)  # 当前折的索引范围

        X_part, y_part = X[idx, :], y[idx]  # 获取当前折的数据和标签

        if j == i:
            X_valid, y_valid = X_part, y_part  # 当前折作为验证集
        elif X_train is None:
            X_train, y_train = X_part, y_part  # 第一个折作为训练集
        else:
            X_train = torch.cat([X_train, X_part], 0)  # 其他折拼接为训练集
            y_train = torch.cat([y_train, y_part], 0)  # 其他折拼接为训练集

    return X_train, y_train, X_valid, y_valid

  
  
def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,
           batch_size):
    train_l_sum, valid_l_sum = 0, 0
    for i in range(k):
        data = get_k_fold_data(k, i, X_train, y_train)  # 获取第i折的训练和验证数据
        net = get_net()  # 获取神经网络模型
        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,
                                   weight_decay, batch_size)  # 训练神经网络
        train_l_sum += train_ls[-1]  # 累加训练集最后一个epoch的损失
        valid_l_sum += valid_ls[-1]  # 累加验证集最后一个epoch的损失
        if i == 0:
            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],
                     xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],
                     legend=['train', 'valid'], yscale='log')  # 绘制第一折的训练和验证损失曲线
        print(f'折{i + 1}，训练log rmse{float(train_ls[-1]):f}, '
              f'验证log rmse{float(valid_ls[-1]):f}')  # 打印每折的训练和验证损失
    return train_l_sum / k, valid_l_sum / k  # 返回平均训练和验证损失
  
  
k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64
train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,
                          weight_decay, batch_size)
print(f'{k}-折验证: 平均训练log rmse: {float(train_l):f}, '
      f'平均验证log rmse: {float(valid_l):f}')


def train_and_pred(train_features, test_features, train_labels, test_data,
                   num_epochs, lr, weight_decay, batch_size):
    net = get_net()   # 初始化神经网络模型
    train_ls, _ = train(net, train_features, train_labels, None, None,   # 训练模型，获取训练过程中的损失值列表和空验证集的占位损失值列表
                        num_epochs, lr, weight_decay, batch_size)
    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',    # 绘制训练过程中的损失值曲线
             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')
    print(f'训练log rmse：{float(train_ls[-1]):f}')  # 打印最后一个epoch的训练损失值
    
    # 将网络应用于测试集，获取预测结果
    preds = net(test_features).detach().numpy()
    
    # 将预测结果重新格式化，准备导出到Kaggle
    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])
    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)  # 构建提交所需的DataFrame，包含ID和预测的SalePrice
    submission.to_csv('submission.csv', index=False)   # 将结果保存为CSV文件，注意不保存索引列
    
    
train_and_pred(train_features, test_features, train_labels, test_data,
               num_epochs, lr, weight_decay, batch_size)
```

