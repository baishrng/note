# 5.1 层和块

```python
import torch  # 导入PyTorch库
from torch import nn  # 导入神经网络模块
from torch.nn import functional as F  # 导入神经网络函数模块（使用别名F）

# 定义一个神经网络模型，使用Sequential容器定义
net = nn.Sequential(
    nn.Linear(20, 256),  # 输入层到隐藏层的线性变换（全连接层）
    nn.ReLU(),           # 激活函数ReLU
    nn.Linear(256, 10)   # 隐藏层到输出层的线性变换（全连接层）
)

X = torch.rand(2, 20)  # 创建一个随机输入张量，形状为(2, 20)
net(X)  # 将输入张量X输入到网络net中进行前向传播计算
```

## 自定义块

```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
```

## 顺序块

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
      
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```

## 在前向传播函数中执行代码

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)   # 使用常量参数进行ReLU操作
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:   # 控制流，直到条件不再满足
            X /= 2
        return X.sum()
      
net = FixedHiddenMLP()
net(X)
```

## 混合搭配各种组合块的方法

```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 定义一个包含两个隐藏层的嵌套MLP
        self.net = nn.Sequential(
            nn.Linear(20, 64),  # 第一个隐藏层：输入维度20，输出维度64
            nn.ReLU(),          # ReLU激活函数
            nn.Linear(64, 32),  # 第二个隐藏层：输入维度64，输出维度32
            nn.ReLU()           # ReLU激活函数
        )
        self.linear = nn.Linear(32, 16)  # 最后的线性层：输入维度32，输出维度16

    def forward(self, X):
        # 通过嵌套MLP进行前向传播
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

# 5.2参数管理

```python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))  # 创建一个大小为(2, 4)的随机张量X

output = net(X)  # 将张量X传递给定义的神经网络net进行前向传播
```

## **参数访问**

```python
print(net[2].state_dict())
```

### [**目标参数**]

```python
print(type(net[2].bias))  # 打印net中第三个模块（nn.Linear(8, 1)）的偏置项的数据类型
print(net[2].bias)        # 打印net中第三个模块（nn.Linear(8, 1)）的偏置项
print(net[2].bias.data)   # 打印net中第三个模块（nn.Linear(8, 1)）的偏置项的数据

net[2].weight.grad == None
```

### [**一次性访问所有参数**]

```python
# 输出net[0]（即nn.Linear(4, 8)）的命名参数及其形状
print(*[(name, param.shape) for name, param in net[0].named_parameters()])

# 输出整个神经网络net的所有命名参数及其形状
print(*[(name, param.shape) for name, param in net.named_parameters()])

net.state_dict()['2.bias'].data
```

	### [**从嵌套块收集参数**]

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),  # 第一个线性层将大小为4的输入映射到大小为8的输出，然后使用ReLU激活函数
                         nn.Linear(8, 4), nn.ReLU())  # 第二个线性层将大小为8的输入映射回大小为4的输出，然后再次使用ReLU激活函数

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 在循环中添加名为'block i'的block1()模块
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))  # rgnet包含block2()返回的嵌套模块和一个线性层，将大小为4的输入映射到大小为1的输出
rgnet(X)  # 对输入X执行rgnet的前向传播计算

print(rgnet)

rgnet[0][1][0].bias.data
```

## 参数初始化
### [**内置初始化**]

```python
def init_normal(m):
    if type(m) == nn.Linear:  # 如果是线性层
        nn.init.normal_(m.weight, mean=0, std=0.01)  # 使用正态分布初始化权重，均值为0，标准差为0.01
        nn.init.zeros_(m.bias)  # 使用零初始化偏置
net.apply(init_normal)  # 对网络应用初始化函数
net[0].weight.data[0], net[0].bias.data[0]  # 输出第一层的第一个权重和偏置


def init_constant(m):
    if type(m) == nn.Linear:  # 如果是线性层
        nn.init.constant_(m.weight, 1)  # 使用常数初始化权重，值为1
        nn.init.zeros_(m.bias)  # 使用零初始化偏置
net.apply(init_constant)  # 对网络应用初始化函数
net[0].weight.data[0], net[0].bias.data[0]  # 输出第一层的第一个权重和偏置


def init_xavier(m):
    if type(m) == nn.Linear:  # 如果是线性层
        nn.init.xavier_uniform_(m.weight)  # 使用Xavier均匀初始化权重
def init_42(m):
    if type(m) == nn.Linear:  # 如果是线性层
        nn.init.constant_(m.weight, 42)  # 使用常数初始化权重，值为42

net[0].apply(init_xavier)  # 对网络的第一层应用Xavier均匀初始化
net[2].apply(init_42)  # 对网络的第三层应用常数初始化，值为42
print(net[0].weight.data[0])  # 打印第一层的第一个权重
print(net[2].weight.data)  # 打印第三层的所有权重
```

### [**自定义初始化**]

```python
def my_init(m):
    if type(m) == nn.Linear:  # 如果是线性层
        print("Init", *[(name, param.shape) for name, param in m.named_parameters()][0])  # 打印初始化信息和第一个参数的形状
        nn.init.uniform_(m.weight, -10, 10)  # 使用均匀分布初始化权重，范围为-10到10
        m.weight.data *= m.weight.data.abs() >= 5  # 将权重数据绝对值大于等于5的元素保留，其它置零(有点东西啊，先计算后面的)

net.apply(my_init)  # 对网络应用自定义初始化函数
net[0].weight[:2]  # 输出第一层权重的前两个元素


net[0].weight.data[:] += 1  # 将网络第一层的权重张量中的每个元素都加1
net[0].weight.data[0, 0] = 42  # 将第一层权重张量的第一个元素设置为42
net[0].weight.data[0]  # 输出第一层权重张量的第一个元素及其余元素
```

## [**参数绑定**]

```python
# 给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)  # 使用输入X进行前向传播

# 检查共享层参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])  # 打印True或False，检查第二个和第四个层的权重是否相等

net[2].weight.data[0, 0] = 100  # 修改第二个共享层的权重的第一个元素为100

# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])  # 再次打印True或False，检查修改后的权重是否反映在第四个层中
```

# 自定义层
## 不带参数的层

```python
import torch
import torch.nn.functional as F
from torch import nn


class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    # 前向传播函数
    def forward(self, X):
        return X - X.mean()   # 返回输入X减去其均值的结果
```

```python
layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))
```

```python
net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())  # 创建一个包含线性层和自定义的CenteredLayer的序列网络
```

```python
Y = net(torch.rand(4, 8))  # 使用net对大小为(4, 8)的随机张量进行前向传播，得到输出Y
Y.mean()  # 计算Y张量的均值
```

## **带参数的层**

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))  # 定义权重参数，形状为(in_units, units)
        self.bias = nn.Parameter(torch.randn(units,))  # 定义偏置参数，形状为(units,)

    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data  # 计算线性变换
        return F.relu(linear)  # 返回经过ReLU激活函数后的结果
```

```python
linear = MyLinear(5, 3)
linear.weight
```

```python
linear(torch.rand(2, 5))
```

```python
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(2, 64))
```

# 读写文件

## **加载和保存张量**

```python
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)  # 创建一个张量 x，包含从0到3的整数
torch.save(x, 'x-file')  # 将张量 x 保存到文件 'x-file' 中
```

```python
x2 = torch.load('x-file')  # 加载名为 'x-file' 的文件中的张量数据
x2  # 输出加载后的张量 x2
```

```python
y = torch.zeros(4)  # 创建一个全零张量 y，大小为(4,)
torch.save([x, y],'x-files')  # 将张量 x 和 y 组成的列表保存到文件 'x-files' 中
x2, y2 = torch.load('x-files')  # 加载 'x-files' 文件中的数据，并将其分别赋值给 x2 和 y2
(x2, y2)  # 输出加载后的张量 x2 和 y2
```

```python
mydict = {'x': x, 'y': y}  # 创建一个字典 mydict，包含张量 x 和 y
torch.save(mydict, 'mydict')  # 将字典 mydict 保存到文件 'mydict' 中
mydict2 = torch.load('mydict')  # 加载 'mydict' 文件中的数据，赋值给 mydict2
mydict2  # 输出加载后的字典 mydict2
```

## **加载和保存模型参数**

```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 定义输入大小为20，输出大小为256的全连接层
        self.output = nn.Linear(256, 10)  # 定义输入大小为256，输出大小为10的全连接层

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))  # 在隐藏层后应用ReLU激活函数，然后传递到输出层

net = MLP()  # 创建一个MLP模型实例
X = torch.randn(size=(2, 20))  # 生成大小为(2, 20)的随机输入张量X
Y = net(X)  # 通过网络进行前向传播，计算输出张量Y
```

```python
torch.save(net.state_dict(), 'mlp.params')  # 将模型 net 的参数保存到文件 'mlp.params' 中
```

```python
clone = MLP()  # 创建一个新的MLP模型实例 clone
clone.load_state_dict(torch.load('mlp.params'))  # 加载保存在 'mlp.params' 文件中的模型参数到 clone 中
clone.eval()  # 设置 clone 模型为评估模式，这将禁用 dropout 层和批标准化层的影响
```

```python
Y_clone = clone(X)
Y_clone == Y
```

# GPU
## **计算设备**

```python
import torch
from torch import nn

# 创建三个不同的设备对象并打印它们
torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')
```

```python
torch.cuda.device_count()  # 返回当前系统中可用的 CUDA 设备数量
```

```python
def try_gpu(i=0):  #@save
    """如果存在，则返回gpu(i)，否则返回cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')  # 返回指定索引的 CUDA 设备
    return torch.device('cpu')  # 如果没有找到对应的 CUDA 设备，则返回 CPU 设备

def try_all_gpus():  #@save
    """返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""
    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]  # 返回所有可用的 CUDA 设备列表
    return devices if devices else [torch.device('cpu')]  # 如果没有找到 CUDA 设备，则返回 CPU 设备

try_gpu(), try_gpu(10), try_all_gpus()  # 测试 try_gpu 和 try_all_gpus 函数的返回值
```

## 张量与GPU

```python
x = torch.tensor([1, 2, 3])
x.device  # 返回张量 x 当前所在的设备，通常是 'cpu'，除非显式指定放在 GPU 上
```

### **存储在GPU上**

```python
X = torch.ones(2, 3, device=try_gpu())  # 创建一个大小为 2x3 的张量 X，并将其放在 GPU 上（如果存在的话）
X  
```

```python
Y = torch.rand(2, 3, device=try_gpu(1))  # 创建一个大小为 2x3 的随机张量 Y，并将其放在指定的第二个 GPU 上（如果存在的话）
Y  
```

## **神经网络与GPU**

```python
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())  # 将神经网络模型 net 移动到可用的 CUDA 设备上（如果存在），否则移到 CPU 上
```

```python
net(X)
```

```python
net[0].weight.data.device  # 返回神经网络模型 net 的第一个层（线性层）的权重张量所在的设备
```

