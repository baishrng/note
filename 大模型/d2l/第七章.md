# 深度卷积神经网络（AlexNet）

## AlexNet

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    # 这里使用一个11*11的更大窗口来捕捉对象。
    # 同时，步幅为4，以减少输出的高度和宽度。
    # 另外，输出通道的数目远大于LeNet
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),  # 第一层卷积：输入通道为1，输出通道为96，卷积核大小为11x11，步幅为4，填充为1
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 使用三个连续的卷积层和较小的卷积窗口。
    # 除了最后的卷积层，输出通道的数量进一步增加。
    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),  # 展平所有维度为一维
    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),  # 在训练过程中随机丢弃（dropout）神经元
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
    nn.Linear(4096, 10))
```

```python
X = torch.randn(1, 1, 224, 224)  # 创建一个随机输入张量
for layer in net:  # 遍历神经网络的每一层
    X = layer(X)  # 将输入 X 输入当前层，计算输出
    print(layer.__class__.__name__, 'output shape:\t', X.shape)  # 打印当前层的类名和输出形状
```

## 读取数据集

```python
batch_size = 128  # 批量大小为128，即每次训练和测试时使用的样本数量

# 使用d2l.load_data_fashion_mnist函数加载Fashion-MNIST数据集，
# 将批量大小设置为128，并将图像大小调整为224x224像素
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
```

## **训练AlexNet**

```python
lr, num_epochs = 0.01, 10  # 学习率和训练轮数
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())  # 使用GPU训练网络模型
```

# 使用块的网络（VGG）
## **VGG块**

```python
import torch
from torch import nn
from d2l import torch as d2l


def vgg_block(num_convs, in_channels, out_channels): # 卷积层的数量，输入通道的数量in_channels 和输出通道的数量out_channels.
    layers = []  # 创建一个空列表存放层次
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))  # 添加卷积层
        layers.append(nn.ReLU())  # 添加ReLU激活函数层
        in_channels = out_channels  # 更新输入通道数为输出通道数，以便下一层使用
    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))  # 添加最大池化层
    return nn.Sequential(*layers)  # 返回一个包含所有层次的序列模块
```

## **VGG网络**

```python
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))  # 定义卷积层堆叠结构，每个元组表示 (卷积层数量, 输出通道数)
```

```python
def vgg(conv_arch):
    conv_blks = []  # 创建一个空列表用于存放VGG块
    in_channels = 1  # 初始输入通道数为1（灰度图像）
    
    # 卷积层部分
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))  # 添加一个VGG块
        in_channels = out_channels  # 更新输入通道数为当前输出通道数，以便下一层使用
    
    return nn.Sequential(
        *conv_blks, nn.Flatten(),  # 将所有VGG块添加到序列模块中，然后添加展平层
        # 全连接层部分
        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),  # 添加全连接层、ReLU激活函数和Dropout层
        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),  # 添加全连接层、ReLU激活函数和Dropout层
        nn.Linear(4096, 10))  # 添加最终的全连接层输出10个类别

net = vgg(conv_arch)  # 创建一个VGG网络实例
```

```python
X = torch.randn(size=(1, 1, 224, 224))  # 创建一个输入张量X，大小为1x1x224x224（批量大小x通道数x高度x宽度）
for blk in net:
    X = blk(X)  # 通过网络的每个块处理输入X
    print(blk.__class__.__name__, 'output shape:\t', X.shape)  # 打印每个块的类名和输出形状
```

## 训练模型

```python
ratio = 4  # 定义缩放比例为4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]  # 缩小卷积层结构，每个通道数除以ratio
net = vgg(small_conv_arch)  # 使用缩小后的卷积层结构创建一个新的VGG网络实例
```

```python
lr, num_epochs, batch_size = 0.05, 10, 128  # 学习率、训练轮数和批量大小的设置
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)  # 加载Fashion MNIST数据集，设置批量大小为128，图像大小调整为224x224
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())  # 使用加载的数据集对定义好的VGG网络进行训练，训练轮数为10，学习率为0.05，尝试使用GPU加速训练
```

# 网络中的网络（NiN）
## **NiN块**

```python
import torch
from torch import nn
from d2l import torch as d2l  # 导入PyTorch模块和d2l库中的torch部分


def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    # 定义一个NiN块，包含三个卷积层和ReLU激活函数
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),  # 第一个卷积层
        nn.ReLU(),  # 第一个ReLU激活函数
        nn.Conv2d(out_channels, out_channels, kernel_size=1),  # 第二个卷积层，kernel_size=1为1x1卷积
        nn.ReLU(),  # 第二个ReLU激活函数
        nn.Conv2d(out_channels, out_channels, kernel_size=1),  # 第三个卷积层，kernel_size=1为1x1卷积
        nn.ReLU())  # 第三个ReLU激活函数
```

```python
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),  # 第一个NiN块：输入通道数为1，输出通道数为96，卷积核大小为11x11，步幅为4，填充为0
    nn.MaxPool2d(3, stride=2),  # 最大池化层：池化窗口大小为3x3，步幅为2
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),  # 第二个NiN块：输入通道数为96，输出通道数为256，卷积核大小为5x5，步幅为1，填充为2
    nn.MaxPool2d(3, stride=2),  # 最大池化层：池化窗口大小为3x3，步幅为2
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),  # 第三个NiN块：输入通道数为256，输出通道数为384，卷积核大小为3x3，步幅为1，填充为1
    nn.MaxPool2d(3, stride=2),  # 最大池化层：池化窗口大小为3x3，步幅为2
    nn.Dropout(0.5),  # Dropout层：丢弃概率为0.5
    # 标签类别数是10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),  # 第四个NiN块：输入通道数为384，输出通道数为10，卷积核大小为3x3，步幅为1，填充为1，用于输出类别预测
    nn.AdaptiveAvgPool2d((1, 1)),  # 自适应平均池化层：输出形状为(1, 1)
    # 将四维的输出转成二维的输出，其形状为(批量大小,10)
    nn.Flatten()  # 将四维的输出转换为二维的输出，形状为(批量大小, 10)
)
```

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
```

```python
lr, num_epochs, batch_size = 0.1, 10, 128  # 学习率、迭代次数和批量大小设定
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)  # 加载Fashion-MNIST数据集
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())  # 调用训练函数进行模型训练
```

# 含并行连结的网络（GoogLeNet）
## **Inception块**

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

class Inception(nn.Module):
    # c1--c4是每条路径的输出通道数
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 线路1，单1x1卷积层
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # 线路2，1x1卷积层后接3x3卷积层
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # 线路3，1x1卷积层后接5x5卷积层
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # 线路4，3x3最大汇聚层后接1x1卷积层
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        # 分别通过各条线路并计算输出
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # 在通道维度上连结各条线路的输出
        return torch.cat((p1, p2, p3, p4), dim=1)
```

## **GoogLeNet模型**

```python
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),  # 第一层：7x7的卷积核，输入通道数为1，输出通道数为64，步长为2，填充为3
                   nn.ReLU(),  # 第二层：ReLU激活函数，对卷积层的输出进行非线性变换
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))  # 第三层：3x3的最大池化层，步长为2，填充为1，用于减小特征图的空间尺寸
```

```python
b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),  # 第一层：1x1的卷积核，输入通道数为64，输出通道数为64
                   nn.ReLU(),  # 第二层：ReLU激活函数，对第一层卷积层的输出进行非线性变换
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),  # 第三层：3x3的卷积核，输入通道数为64，输出通道数为192，填充为1
                   nn.ReLU(),  # 第四层：ReLU激活函数，对第三层卷积层的输出进行非线性变换
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))  # 第五层：3x3的最大池化层，步长为2，填充为1，用于减小特征图的空间尺寸
```

```python
b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),  
                   # 第一层：使用自定义的Inception模块，输入通道数为192，输出通道数为64，具体结构为Inception(64, (96, 128), (16, 32), 32)
                   
                   Inception(256, 128, (128, 192), (32, 96), 64),  
                   # 第二层：使用自定义的Inception模块，输入通道数为256，输出通道数为128，具体结构为Inception(128, (128, 192), (32, 96), 64)
                   
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))  
                   # 第三层：3x3的最大池化层，步长为2，填充为1，用于减小特征图的空间尺寸
```

```python
b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),  
                   # 第一层：使用自定义的Inception模块，输入通道数为480，输出通道数为192，具体结构为Inception(192, (96, 208), (16, 48), 64)
                   
                   Inception(512, 160, (112, 224), (24, 64), 64),  
                   # 第二层：使用自定义的Inception模块，输入通道数为512，输出通道数为160，具体结构为Inception(160, (112, 224), (24, 64), 64)
                   
                   Inception(512, 128, (128, 256), (24, 64), 64),  
                   # 第三层：使用自定义的Inception模块，输入通道数为512，输出通道数为128，具体结构为Inception(128, (128, 256), (24, 64), 64)
                   
                   Inception(512, 112, (144, 288), (32, 64), 64),  
                   # 第四层：使用自定义的Inception模块，输入通道数为512，输出通道数为112，具体结构为Inception(112, (144, 288), (32, 64), 64)
                   
                   Inception(528, 256, (160, 320), (32, 128), 128),  
                   # 第五层：使用自定义的Inception模块，输入通道数为528，输出通道数为256，具体结构为Inception(256, (160, 320), (32, 128), 128)
                   
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))  
                   # 第六层：3x3的最大池化层，步长为2，填充为1，用于减小特征图的空间尺寸
```

```python
b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),  
                   # 第一层：使用自定义的Inception模块，输入通道数为832，输出通道数为256，具体结构为Inception(256, (160, 320), (32, 128), 128)
                   
                   Inception(832, 384, (192, 384), (48, 128), 128),  
                   # 第二层：使用自定义的Inception模块，输入通道数为832，输出通道数为384，具体结构为Inception(384, (192, 384), (48, 128), 128)
                   
                   nn.AdaptiveAvgPool2d((1,1)),  
                   # 自适应平均池化层，将输入特征图的大小自适应地池化为1x1的大小
                   
                   nn.Flatten())  # 将输入展平成一维向量
                   

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))  
# 将之前定义的多个阶段（b1到b4）、b5和一个全连接层组合成一个序列模型net。最后的全连接层将输入维度为1024降维到输出维度为10，用于分类任务。
```

```python
# 创建一个大小为 (1, 1, 96, 96) 的随机张量作为输入 X
X = torch.rand(size=(1, 1, 96, 96))

# 逐层遍历神经网络 net，并对输入 X 进行前向传播
for layer in net:
    X = layer(X)
    # 打印每一层的类名和输出张量的形状
    print(layer.__class__.__name__, 'output shape:\t', X.shape)
```

## **训练模型**

```python
# 设置学习率、训练周期数和批量大小
lr, num_epochs, batch_size = 0.1, 10, 128

# 加载 Fashion-MNIST 数据集，设置批量大小为 batch_size，将图像大小调整为 96x96
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)

# 使用 train_ch6 函数对神经网络 net 进行训练
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

# 批量规范化
## **从零实现**

```python
import torch
from torch import nn
from d2l import torch as d2l


def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。
            # 这里我们需要保持X的形状以便后面可以做广播运算
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # 训练模式下，用当前的均值和方差做标准化
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 更新移动平均的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # 缩放和移位
    return Y, moving_mean.data, moving_var.data
```

```python
class BatchNorm(nn.Module):
    # num_features：完全连接层的输出数量或卷积层的输出通道数。
    # num_dims：2表示完全连接层，4表示卷积层
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # 非模型参数的变量初始化为0和1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # 如果X不在内存上，将moving_mean和moving_var
        # 复制到X所在显存上
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的moving_mean和moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
```

##  使用批量规范化层的 LeNet

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5),          # 1通道输入，6通道输出的卷积层，卷积核大小为5
    BatchNorm(6, num_dims=4),                # 卷积层后的批标准化
    nn.Sigmoid(),                            # Sigmoid激活函数

    nn.AvgPool2d(kernel_size=2, stride=2),   # 核大小为2，步长为2的平均池化层

    nn.Conv2d(6, 16, kernel_size=5),         # 6通道输入，16通道输出的第二个卷积层，卷积核大小为5
    BatchNorm(16, num_dims=4),               # 第二个卷积层后的批标准化
    nn.Sigmoid(),                            # Sigmoid激活函数

    nn.AvgPool2d(kernel_size=2, stride=2),   # 核大小为2，步长为2的平均池化层
    nn.Flatten(),                            # 将输出展平，准备进入全连接层

    nn.Linear(16*4*4, 120),                  # 16*4*4个输入特征，120个输出特征的全连接层
    BatchNorm(120, num_dims=2),              # 全连接层后的批标准化
    nn.Sigmoid(),                            # Sigmoid激活函数

    nn.Linear(120, 84),                      # 120个输入特征，84个输出特征的全连接层
    BatchNorm(84, num_dims=2),               # 全连接层后的批标准化
    nn.Sigmoid(),                            # Sigmoid激活函数

    nn.Linear(84, 10)                        # 84个输入特征，10个输出类别的输出层
)
```

```python
lr, num_epochs, batch_size = 1.0, 10, 256  # 学习率、训练轮数、批量大小
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  # 加载Fashion MNIST数据集
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())  # 使用自定义的训练函数train_ch6进行模型训练，并尝试在GPU上训练
```

```python
net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))  # 获取第二层BatchNorm层的gamma和beta参数，并展平为一维张量 
```

## **简明实现**

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5),      # 1个输入通道，6个输出通道，卷积核大小为5x5
    nn.BatchNorm2d(6),                   # 第一个卷积层后的批标准化，6个通道
    nn.Sigmoid(),                        # Sigmoid激活函数

    nn.AvgPool2d(kernel_size=2, stride=2),  # 平均池化层，池化核大小为2x2，步幅为2

    nn.Conv2d(6, 16, kernel_size=5),     # 6个输入通道（来自前一层），16个输出通道，卷积核大小为5x5
    nn.BatchNorm2d(16),                  # 第二个卷积层后的批标准化，16个通道
    nn.Sigmoid(),                        # Sigmoid激活函数

    nn.AvgPool2d(kernel_size=2, stride=2),  # 平均池化层，池化核大小为2x2，步幅为2
    nn.Flatten(),                        # 将卷积层的输出展平为一维张量

    nn.Linear(256, 120),                 # 全连接层：输入特征数256，输出特征数120
    nn.BatchNorm1d(120),                 # 第一个全连接层后的批标准化，120个特征
    nn.Sigmoid(),                        # Sigmoid激活函数

    nn.Linear(120, 84),                  # 全连接层：输入特征数120，输出特征数84
    nn.BatchNorm1d(84),                  # 第二个全连接层后的批标准化，84个特征
    nn.Sigmoid(),                        # Sigmoid激活函数

    nn.Linear(84, 10)                    # 输出层：输入特征数84，输出10个类别（用于分类任务）
)
```

```python
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
# 使用d2l工具包中的train_ch6函数训练神经网络模型，传入模型net、训练集迭代器train_iter、测试集迭代器test_iter、训练轮数num_epochs、学习率lr，并尝试在GPU上运行
```

# 残差网络（ResNet）
## **残差块**

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l


class Residual(nn.Module):  #@save
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)  # 第一个卷积层
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)  # 第二个卷积层
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)  # 用于调整维度的1x1卷积层
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)  # 第一个批量归一化层
        self.bn2 = nn.BatchNorm2d(num_channels)  # 第二个批量归一化层

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))  # 第一个卷积层后接批量归一化和ReLU激活函数
        Y = self.bn2(self.conv2(Y))  # 第二个卷积层后接批量归一化
        if self.conv3:
            X = self.conv3(X)  # 如果有1x1卷积层，则用它调整维度
        Y += X  # 将原始输入与输出相加（残差连接）
        return F.relu(Y)  # 返回ReLU激活后的输出
```

```python
blk = Residual(3, 3)  # 创建一个输入通道数为3，输出通道数为3的残差块实例
X = torch.rand(4, 3, 6, 6)  # 创建一个随机张量作为输入，形状为[4, 3, 6, 6]
Y = blk(X)  # 对输入张量X应用残差块blk，得到输出张量Y
Y.shape  # 打印输出张量Y的形状
```

```python
blk = Residual(3, 6, use_1x1conv=True, strides=2)  # 创建一个输入通道数为3，输出通道数为6的残差块实例，使用1x1卷积并设置步幅为2
blk(X).shape  # 对输入张量X应用残差块blk，然后打印输出张量的形状
```

## **ResNet模型**

```python
b1 = nn.Sequential(
    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),  # 第一个卷积层：输入通道数为1，输出通道数为64，卷积核大小为7x7，步幅为2，填充为3
    nn.BatchNorm2d(64),  # 批量归一化层，针对输出通道数为64的卷积层
    nn.ReLU(),  # ReLU激活函数
    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 最大池化层：池化核大小为3x3，步幅为2，填充为1
)
```

```python
def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels, num_channels,
                                use_1x1conv=True, strides=2))  # 添加一个残差块，输入通道数为input_channels，输出通道数为num_channels，使用1x1卷积，步幅为2（输出通道翻倍，高和宽减半）
        else:
            blk.append(Residual(num_channels, num_channels))  # 添加一个残差块，输入和输出通道数都为num_channels
    return blk
```

```python
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))  # 创建一个包含两个残差块的序列模型，每个块的输入输出通道数均为64，第一个块使用1x1卷积和步幅为2
b3 = nn.Sequential(*resnet_block(64, 128, 2))  # 创建一个包含两个残差块的序列模型，每个块的输入通道数为64，输出通道数为128
b4 = nn.Sequential(*resnet_block(128, 256, 2))  # 创建一个包含两个残差块的序列模型，每个块的输入通道数为128，输出通道数为256
b5 = nn.Sequential(*resnet_block(256, 512, 2))  # 创建一个包含两个残差块的序列模型，每个块的输入通道数为256，输出通道数为512
```

```python
net = nn.Sequential(b1, b2, b3, b4, b5,  # 将 b1, b2, b3, b4, b5 这五个残差块序列模型依次堆叠
                    nn.AdaptiveAvgPool2d((1,1)),  # 自适应平均池化层，输出大小为 (1, 1)
                    nn.Flatten(),  # 将输入展平成一维张量
                    nn.Linear(512, 10))  # 全连接层，将输入维度为512的特征映射到维度为10的输出
```

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
```

## **训练模型**

```python
lr, num_epochs, batch_size = 0.05, 10, 256  # 学习率 lr 设为 0.05，训练轮数 num_epochs 设为 10，批量大小 batch_size 设为 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)  # 加载Fashion-MNIST数据集，设置批量大小为256，并将图像大小调整为96x96像素
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())  # 使用train_ch6函数训练模型net，传入训练数据迭代器train_iter和测试数据迭代器test_iter，学习率lr，训练轮数num_epochs和GPU设备（如果可用）
```

# 稠密连接网络（DenseNet）
## **稠密块体**
DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构（参见 :numref:`sec_resnet`中的练习）。
我们首先实现一下这个架构。

```python
import torch  # 导入PyTorch库
from torch import nn  # 从PyTorch库中导入神经网络模块
from d2l import torch as d2l  # 从d2l库中导入torch模块并命名为d2l


# 卷积块
def conv_block(input_channels, num_channels):
    return nn.Sequential(  # 返回一个包含卷积块操作的序列容器
        nn.BatchNorm2d(input_channels), nn.ReLU(),  # 批量归一化、ReLU激活函数
        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))  # 二维卷积层
```

一个*稠密块*由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。

```python
# 稠密块
class DenseBlock(nn.Module):
    def __init__(self, num_convs, input_channels, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(
                num_channels * i + input_channels, num_channels))
        self.net = nn.Sequential(*layer)  # 创建包含多个卷积块的网络序列

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # 连接通道维度上每个块的输入和输出
            X = torch.cat((X, Y), dim=1)  # 在通道维度上拼接输入X和卷积块输出Y
        return X  # 返回DenseBlock的输出

```

在下面的例子中，我们[**定义一个**]有2个输出通道数为10的(**`DenseBlock`**)。
使用通道数为3的输入时，我们会得到通道数为$3+2\times 10=23$的输出。
卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为*增长率*（growth rate）。

```python
blk = DenseBlock(2, 3, 10)  # 创建一个包含2个卷积块、输入通道数为3、输出通道数为10的DenseBlock实例
X = torch.randn(4, 3, 8, 8)  # 创建一个形状为(4, 3, 8, 8)的随机张量作为输入X
Y = blk(X)  # 将输入X传入DenseBlock中进行前向传播得到输出Y
Y.shape  # 输出Y的形状
```

## **过渡层**

由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。
而过渡层可以用来控制模型复杂度。
它通过$1\times 1$卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。

```python
# 过渡层
def transition_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels),  # 批量规范化层，输入通道数为input_channels
        nn.ReLU(),  # ReLU激活函数
        nn.Conv2d(input_channels, num_channels, kernel_size=1),  # 1x1卷积层，将input_channels通道转换为num_channels通道
        nn.AvgPool2d(kernel_size=2, stride=2)  # 2x2的平均池化层，减少空间分辨率
    )
```

对上一个例子中稠密块的输出[**使用**]通道数为10的[**过渡层**]。
此时输出的通道数减为10，高和宽均减半。

```python
blk = transition_block(23, 10)  # 创建一个转换块实例，输入通道数为23，输出通道数为10
blk(Y).shape
```

## **DenseNet模型**

我们来构造DenseNet模型。DenseNet首先使用同ResNet一样的单卷积层和最大汇聚层。

```python
b1 = nn.Sequential(
    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),  # 7x7卷积层，输入通道数1，输出通道数64，步幅为2，填充为3
    nn.BatchNorm2d(64),  # 批量归一化层，对64个通道进行归一化
    nn.ReLU(),  # ReLU激活函数
    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 3x3最大池化层，步幅为2，填充为1
)
```

接下来，类似于ResNet使用的4个残差块，DenseNet使用的是4个稠密块。
与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。
这里我们设成4，从而与 :numref:`sec_resnet`的ResNet-18保持一致。
稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。

在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。

```python
# num_channels为当前的通道数，growth_rate为卷积层的输出通道数
num_channels, growth_rate = 64, 32
num_convs_in_dense_blocks = [4, 4, 4, 4]  # 每个稠密中块卷积层的数量
blks = []
for i, num_convs in enumerate(num_convs_in_dense_blocks):
    blks.append(DenseBlock(num_convs, num_channels, growth_rate))
    # 上一个稠密块的输出通道数
    num_channels += num_convs * growth_rate
    # 在稠密块之间添加一个过渡层，使通道数量减半
    if i != len(num_convs_in_dense_blocks) - 1:
        blks.append(transition_block(num_channels, num_channels // 2))
        num_channels = num_channels // 2
```

与ResNet类似，最后接上全局汇聚层和全连接层来输出结果。

```python
net = nn.Sequential(
    b1, *blks,  # 添加前面定义的模块列表（包括稠密块和转换块）
    nn.BatchNorm2d(num_channels), nn.ReLU(),  # 批量归一化和ReLU激活函数
    nn.AdaptiveAvgPool2d((1, 1)),  # 自适应平均池化到大小为 (1, 1)
    nn.Flatten(),  # 展平操作，准备输入全连接层
    nn.Linear(num_channels, 10)  # 全连接层，输出10个类别的预测结果
)
```

## **训练模型**

由于这里使用了比较深的网络，本节里我们将输入高和宽从224降到96来简化计算。

```python
lr, num_epochs, batch_size = 0.1, 10, 256  # 学习率、训练周期数和批量大小
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)  # 加载Fashion-MNIST数据集，将高和宽降为96
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())  # 使用train_ch6函数训练模型，使用GPU加速（如果可用）
```

