# 图像增广

大型数据集是成功应用深度神经网络的先决条件。图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。可以说，图像增广技术对于AlexNet的成功是必不可少的。本节将讨论这项广泛应用于计算机视觉的技术。

```python
%matplotlib inline
import torch  # 导入PyTorch库
import torchvision  # 导入PyTorch视觉工具库
from torch import nn  # 从PyTorch中导入神经网络模块
from d2l import torch as d2l  # 导入d2l工具包中的torch模块，通常用于深度学习教学
```

## 常用的图像增广方法

在对常用图像增广方法的探索时，我们将使用下面这个尺寸为$400\times 500$的图像作为示例。

```python
d2l.set_figsize()  # 设置图形大小，通常用于d2l工具包中的绘图函数
img = d2l.Image.open('../img/cat1.jpg')  # 打开并加载图像文件'../img/cat1.jpg'
d2l.plt.imshow(img);  # 使用matplotlib显示图像
```

大多数图像增广方法都具有一定的随机性。为了便于观察图像增广的效果，我们下面定义辅助函数`apply`。此函数在输入图像`img`上多次运行图像增广方法`aug`并显示所有结果。

```python
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    Y = [aug(img) for _ in range(num_rows * num_cols)]  # 对输入的图像img应用增强函数aug，生成列表Y
    d2l.show_images(Y, num_rows, num_cols, scale=scale)  # 调用d2l工具包中的show_images函数显示增强后的图像集合Y，以num_rows行、num_cols列显示，缩放比例为scale
```

### 翻转和裁剪

[**左右翻转图像**]通常不会改变对象的类别。这是最早且最广泛使用的图像增广方法之一。接下来，我们使用`transforms`模块来创建`RandomFlipLeftRight`实例，这样就各有50%的几率使图像向左或向右翻转。

```python
apply(img, torchvision.transforms.RandomHorizontalFlip())  # 调用apply函数，将img图像应用随机水平翻转增强（RandomHorizontalFlip()函数）
```

[**上下翻转图像**]不如左右图像翻转那样常用。但是，至少对于这个示例图像，上下翻转不会妨碍识别。接下来，我们创建一个`RandomFlipTopBottom`实例，使图像各有50%的几率向上或向下翻转。

```python
apply(img, torchvision.transforms.RandomVerticalFlip())  # 调用apply函数，将img图像应用随机垂直翻转增强（RandomVerticalFlip()函数）
```

在我们使用的示例图像中，猫位于图像的中间，但并非所有图像都是这样。我们解释了汇聚层可以降低卷积层对目标位置的敏感性。另外，我们可以通过对图像进行随机裁剪，使物体以不同的比例出现在图像的不同位置。这也可以降低模型对目标位置的敏感性。

下面的代码将[**随机裁剪**]一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5～2之间随机取值。然后，区域的宽度和高度都被缩放到200像素。在本节中（除非另有说明），$a$和$b$之间的随机数指的是在区间$[a, b]$中通过均匀采样获得的连续值。

```python
shape_aug = torchvision.transforms.RandomResizedCrop(
    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))  # 定义shape_aug变量为RandomResizedCrop增强函数，设置目标尺寸为(200, 200)，尺度范围为(0.1, 1)，宽高比范围为(0.5, 2)
apply(img, shape_aug)  # 调用apply函数，将img图像应用shape_aug增强
```

### 改变颜色

另一种增广方法是改变颜色。我们可以改变图像颜色的四个方面：亮度、对比度、饱和度和色调。在下面的示例中，我们[**随机更改图像的亮度**]，随机值为原始图像的50%（$1-0.5$）到150%（$1+0.5$）之间。

```python
apply(img, torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0, saturation=0, hue=0))  # 调用apply函数，将img图像应用ColorJitter增强，设置亮度增强为0.5，对比度、饱和度和色调均不进行增强
```

同样，我们可以[**随机更改图像的色调**]。

```python
apply(img, torchvision.transforms.ColorJitter(
    brightness=0, contrast=0, saturation=0, hue=0.5))  # 调用apply函数，将img图像应用ColorJitter增强，设置亮度、对比度、饱和度均不增强，仅对色调进行最多±0.5的随机调整
```

我们还可以创建一个`RandomColorJitter`实例，并设置如何同时[**随机更改图像的亮度（`brightness`）、对比度（`contrast`）、饱和度（`saturation`）和色调（`hue`）**]。

```python
color_aug = torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)  # 定义color_aug变量为ColorJitter增强函数，设置亮度、对比度、饱和度和色调均最多增强±0.5
apply(img, color_aug)  # 调用apply函数，将img图像应用color_aug增强
```

### **结合多种图像增广方法**

在实践中，我们将结合多种图像增广方法。比如，我们可以通过使用一个`Compose`实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像。

```python
augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])  # 定义augs变量为一个组合的转换序列，包括随机水平翻转、颜色增强(color_aug)和形状增强(shape_aug)
apply(img, augs)  # 调用apply函数，将img图像应用augs中定义的增强序列
```

## **使用图像增广进行训练**

让我们使用图像增广来训练模型。这里，我们使用CIFAR-10数据集，而不是我们之前使用的Fashion-MNIST数据集。这是因为Fashion-MNIST数据集中对象的位置和大小已被规范化，而CIFAR-10数据集中对象的颜色和大小差异更明显。CIFAR-10数据集中的前32个训练图像如下所示。

```python
all_images = torchvision.datasets.CIFAR10(train=True, root="../data", download=True)   # 加载CIFAR-10数据集
d2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8)   # 显示前32张图片，以4行8列的网格显示，缩放比例为0.8
```

为了在预测过程中得到确切的结果，我们通常对训练样本只进行图像增广，且在预测过程中不使用随机操作的图像增广。在这里，我们[**只使用最简单的随机左右翻转**]。此外，我们使用`ToTensor`实例将一批图像转换为深度学习框架所要求的格式，即形状为（批量大小，通道数，高度，宽度）的32位浮点数，取值范围为0～1。

```python
train_augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(),  # 添加随机水平翻转的数据增强
    torchvision.transforms.ToTensor()  # 转换为张量
])

test_augs = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()  # 转换为张量
])
```

接下来，我们[**定义一个辅助函数，以便于读取图像和应用图像增广**]。PyTorch数据集提供的`transform`参数应用图像增广来转化图像。

```python
def load_cifar10(is_train, augs, batch_size):
    # 创建 CIFAR-10 数据集对象，指定数据集根目录、是否为训练集、数据增强方式、是否需要下载
    dataset = torchvision.datasets.CIFAR10(root="../data", train=is_train,
                                           transform=augs, download=True)
    # 创建数据加载器，指定数据集对象、批量大小、是否打乱数据、使用的工作线程数
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                             shuffle=is_train, num_workers=d2l.get_dataloader_workers())
    return dataloader
```

### 多GPU训练

我们在CIFAR-10数据集上训练ResNet-18模型。接下来，我们[**定义一个函数，使用多GPU对模型进行训练和评估**]。

```python
#@save
def train_batch_ch13(net, X, y, loss, trainer, devices):
    """用多GPU进行小批量训练"""
    if isinstance(X, list):
        # 如果输入数据是列表（例如微调BERT所需），将每个元素移到第一个设备上
        X = [x.to(devices[0]) for x in X]
    else:
        X = X.to(devices[0])  # 否则，将输入数据移到第一个设备上
    y = y.to(devices[0])  # 将标签数据移到第一个设备上
    net.train()  # 设置模型为训练模式
    trainer.zero_grad()  # 梯度清零
    pred = net(X)  # 前向传播计算预测值
    l = loss(pred, y)  # 计算损失
    l.sum().backward()  # 损失求和并反向传播
    trainer.step()  # 更新模型参数
    train_loss_sum = l.sum()  # 计算损失的总和
    train_acc_sum = d2l.accuracy(pred, y)  # 计算准确率
    return train_loss_sum, train_acc_sum  # 返回训练损失总和和训练准确率
```

```python
#@save
def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
               devices=d2l.try_all_gpus()):
    """用多GPU进行模型训练"""
    timer, num_batches = d2l.Timer(), len(train_iter)
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
                            legend=['train loss', 'train acc', 'test acc'])
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])  # 使用DataParallel在多个GPU上并行化网络
    for epoch in range(num_epochs):
        metric = d2l.Accumulator(4)  # 用于累加训练损失、训练准确度以及样本数和特征数
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = train_batch_ch13(
                net, features, labels, loss, trainer, devices)
            metric.add(l, acc, labels.shape[0], labels.numel())  # 将损失、准确度以及批次的样本数和特征数累加到metric中
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[3],
                              None))  # 更新动画器中的图表数据
        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)  # 在测试集上评估准确率
        animator.add(epoch + 1, (None, None, test_acc))  # 更新动画器中的测试准确率
    print(f'loss {metric[0] / metric[2]:.3f}, train acc '
          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')  # 打印最终的训练损失、训练准确率和测试准确率
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
          f'{str(devices)}')  # 打印训练速度
```

现在，我们可以[**定义`train_with_data_aug`函数，使用图像增广来训练模型**]。该函数获取所有的GPU，并使用Adam作为训练的优化算法，将图像增广应用于训练集，最后调用刚刚定义的用于训练和评估模型的`train_ch13`函数。

```python
batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)  # 设置批量大小、设备和ResNet-18模型

def init_weights(m):
    if type(m) in [nn.Linear, nn.Conv2d]:
        nn.init.xavier_uniform_(m.weight)  # 使用Xavier初始化方法初始化线性层和卷积层的权重

net.apply(init_weights)  # 应用权重初始化函数到网络模型上

def train_with_data_aug(train_augs, test_augs, net, lr=0.001):
    train_iter = load_cifar10(True, train_augs, batch_size)  # 加载训练集并进行数据增强
    test_iter = load_cifar10(False, test_augs, batch_size)  # 加载测试集
    loss = nn.CrossEntropyLoss(reduction="none")  # 定义交叉熵损失函数
    trainer = torch.optim.Adam(net.parameters(), lr=lr)  # 使用Adam优化器来优化网络参数
    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)  # 调用多GPU训练函数进行训练
```

让我们使用基于随机左右翻转的图像增广来[**训练模型**]。

```python
train_with_data_aug(train_augs, test_augs, net)  # 使用定义好的训练数据增强方式对网络进行训练
```

# 微调
微调包括以下四个步骤。

1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即*源模型*。
1. 创建一个新的神经网络模型，即*目标模型*。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。
1. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
1. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。
当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。

## 热狗识别

让我们通过具体案例演示微调：热狗识别。我们将在一个小型数据集上微调ResNet模型。该模型已在ImageNet数据集上进行了预训练。这个小型数据集包含数千张包含热狗和不包含热狗的图像，我们将使用微调模型来识别图像中是否包含热狗。

```python
%matplotlib inline
import os  # 导入操作系统相关的库
import torch  # 导入PyTorch深度学习库
import torchvision  # 导入PyTorch官方提供的视觉库
from torch import nn  # 从PyTorch中导入神经网络模块
from d2l import torch as d2l  # 导入"d2l"工具包中的PyTorch模块
```

### 获取数据集

我们使用的[**热狗数据集来源于网络**]。该数据集包含1400张热狗的“正类”图像，以及包含尽可能多的其他食物的“负类”图像。含着两个类别的1000张图片用于训练，其余的则用于测试。

解压下载的数据集，我们获得了两个文件夹`hotdog/train`和`hotdog/test`。这两个文件夹都有`hotdog`（有热狗）和`not-hotdog`（无热狗）两个子文件夹，子文件夹内都包含相应类的图像。

```python
#@save
d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',  # 将热狗数据集的URL添加到"d2l.DATA_HUB"字典中
                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')

data_dir = d2l.download_extract('hotdog')  # 下载并解压热狗数据集到指定目录"data_dir"
```

我们创建两个实例来分别读取训练和测试数据集中的所有图像文件。

```python
train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))  # 加载训练集图像数据集
test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))  # 加载测试集图像数据集
```

下面显示了前8个正类样本图片和最后8张负类样本图片。正如所看到的，[**图像的大小和纵横比各有不同**]。

```python
hotdogs = [train_imgs[i][0] for i in range(8)]  # 提取前8张热狗类别的训练集图像
not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]  # 提取前8张非热狗类别的训练集图像
d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);  # 显示前16张图像，每行8张，放大显示
```

在训练期间，我们首先从图像中裁切随机大小和随机长宽比的区域，然后将该区域缩放为$224 \times 224$输入图像。在测试过程中，我们将图像的高度和宽度都缩放到256像素，然后裁剪中央$224 \times 224$区域作为输入。此外，对于RGB（红、绿和蓝）颜色通道，我们分别*标准化*每个通道。具体而言，该通道的每个值减去该通道的平均值，然后将结果除以该通道的标准差。

```python
# 使用RGB通道的均值和标准差，以标准化每个通道
normalize = torchvision.transforms.Normalize(
    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

train_augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(224),  # 随机裁剪为224x224大小的图像
    torchvision.transforms.RandomHorizontalFlip(),  # 随机水平翻转图像
    torchvision.transforms.ToTensor(),  # 转换为Tensor格式
    normalize  # 标准化图像
])

test_augs = torchvision.transforms.Compose([
    torchvision.transforms.Resize([256, 256]),  # 调整图像大小为256x256
    torchvision.transforms.CenterCrop(224),  # 中心裁剪为224x224大小的图像
    torchvision.transforms.ToTensor(),  # 转换为Tensor格式
    normalize  # 标准化图像
])
```

### **定义和初始化模型**

我们使用在ImageNet数据集上预训练的ResNet-18作为源模型。在这里，我们指定`pretrained=True`以自动下载预训练的模型参数。如果首次使用此模型，则需要连接互联网才能下载。

```python
pretrained_net = torchvision.models.resnet18(pretrained=True)  # 加载预训练的ResNet-18模型，使用在ImageNet数据集上预训练的权重。
```

预训练的源模型实例包含许多特征层和一个输出层`fc`。此划分的主要目的是促进对除输出层以外所有层的模型参数进行微调。下面给出了源模型的成员变量`fc`。

```python
pretrained_net.fc
```

在ResNet的全局平均汇聚层后，全连接层转换为ImageNet数据集的1000个类输出。之后，我们构建一个新的神经网络作为目标模型。它的定义方式与预训练源模型的定义方式相同，只是最终层中的输出数量被设置为目标数据集中的类数（而不是1000个）。

在下面的代码中，目标模型`finetune_net`中成员变量`features`的参数被初始化为源模型相应层的模型参数。由于模型参数是在ImageNet数据集上预训练的，并且足够好，因此通常只需要较小的学习率即可微调这些参数。

成员变量`output`的参数是随机初始化的，通常需要更高的学习率才能从头开始训练。假设`Trainer`实例中的学习率为$\eta$，我们将成员变量`output`中参数的学习率设置为$10\eta$。

```python
finetune_net = torchvision.models.resnet18(pretrained=True)  # 加载预训练的ResNet-18模型

# 替换ResNet-18模型的最后一层全连接层为具有2个输出特征的线性层
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)

# 使用Xavier初始化方法初始化新添加的全连接层的权重
nn.init.xavier_uniform_(finetune_net.fc.weight)
```

### **微调模型**

首先，我们定义了一个训练函数`train_fine_tuning`，该函数使用微调，因此可以多次调用。

```python
# 如果param_group=True，输出层中的模型参数将使用十倍的学习率
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,
                      param_group=True):
    # 加载训练和测试数据集
    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train'), transform=train_augs),
        batch_size=batch_size, shuffle=True)
    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'test'), transform=test_augs),
        batch_size=batch_size)
    
    devices = d2l.try_all_gpus()  # 尝试获取所有GPU设备
    loss = nn.CrossEntropyLoss(reduction="none")  # 使用交叉熵损失函数，不对损失进行求和
    if param_group:
        # 如果使用参数组设置，定义两个参数组：一个是ResNet-18模型除了最后一层全连接层外的所有参数，
        # 另一个是最后一层全连接层的参数，学习率为原始学习率的10倍
        params_1x = [param for name, param in net.named_parameters()
                     if name not in ["fc.weight", "fc.bias"]]
        trainer = torch.optim.SGD([
            {'params': params_1x},  # 学习率为原始学习率的1倍
            {'params': net.fc.parameters(), 'lr': learning_rate * 10}  # 学习率为原始学习率的10倍
        ], lr=learning_rate, weight_decay=0.001)
    else:
        # 否则，使用单一的学习率对所有模型参数进行优化
        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,
                                  weight_decay=0.001)
    
    # 调用d2l库中的训练函数，进行模型训练
    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
                   devices)
```

我们[**使用较小的学习率**]，通过*微调*预训练获得的模型参数。

```python
train_fine_tuning(finetune_net, 5e-5)  # (网络，学习率)
```

[**为了进行比较，**]我们定义了一个相同的模型，但是将其(**所有模型参数初始化为随机值**)。由于整个模型需要从头开始训练，因此我们需要使用更大的学习率。

```python
scratch_net = torchvision.models.resnet18()  # 加载预训练的ResNet-18模型
scratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)  # 替换最后一层全连接层为二分类线性层
train_fine_tuning(scratch_net, 5e-4, param_group=False)  # 对替换后的模型进行微调，使用单一学习率对所有参数进行优化
```

意料之中，微调模型往往表现更好，因为它的初始参数值更有效。

# 目标检测和边界框
接下来的几节将介绍几种用于目标检测的深度学习方法。我们将首先介绍目标的*位置*。

```python
%matplotlib inline
import torch
from d2l import torch as d2l
```

下面加载本节将使用的示例图像。可以看到图像左边是一只狗，右边是一只猫。它们是这张图像里的两个主要目标。

```python
d2l.set_figsize()  # 设置图形尺寸为d2l库默认的尺寸
img = d2l.plt.imread('../img/catdog.jpg')  # 读取图像文件为numpy数组
d2l.plt.imshow(img);  # 显示图像
```

## 边界框

在目标检测中，我们通常使用*边界框*（bounding box）来描述对象的空间位置。边界框是矩形的，由矩形左上角的以及右下角的$x$和$y$坐标决定。另一种常用的边界框表示方法是边界框中心的$(x, y)$轴坐标以及框的宽度和高度。

在这里，我们[**定义在这两种表示法之间进行转换的函数**]：`box_corner_to_center`从两角表示法转换为中心宽度表示法，而`box_center_to_corner`反之亦然。输入参数`boxes`可以是长度为4的张量，也可以是形状为（$n$，4）的二维张量，其中$n$是边界框的数量。

```python
#@save
def box_corner_to_center(boxes):
    """从（左上，右下）转换到（中间，宽度，高度）"""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    boxes = torch.stack((cx, cy, w, h), axis=-1)
    return boxes

#@save
def box_center_to_corner(boxes):
    """从（中间，宽度，高度）转换到（左上，右下）"""
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = torch.stack((x1, y1, x2, y2), axis=-1)
    return boxes
```

我们将根据坐标信息[**定义图像中狗和猫的边界框**]。图像中坐标的原点是图像的左上角，向右的方向为$x$轴的正方向，向下的方向为$y$轴的正方向。

```python
# bbox是边界框的英文缩写
dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]
```

我们可以通过转换两次来验证边界框转换函数的正确性。

```python
boxes = torch.tensor((dog_bbox, cat_bbox))   # 创建包含狗和猫边界框的张量
box_center_to_corner(box_corner_to_center(boxes)) == boxes   # 将边界框从角点格式转换为中心格式，然后再转换回角点格式
```

我们可以[**将边界框在图中画出**]，以检查其是否准确。画之前，我们定义一个辅助函数`bbox_to_rect`。它将边界框表示成`matplotlib`的边界框格式。

```python
#@save
def bbox_to_rect(bbox, color):
    # 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：
    # ((左上x,左上y),宽,高)
    return d2l.plt.Rectangle(
        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
        fill=False, edgecolor=color, linewidth=2)
```

在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内。

```python
fig = d2l.plt.imshow(img)  # 显示图像 img 并返回一个 Figure 对象
fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))  # 在图像上添加狗的边界框，边框颜色为蓝色
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'))  # 在图像上添加猫的边界框，边框颜色为红色
```

# 锚框
目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的*真实边界框*。不同的模型使用的区域采样方法可能不同。这里我们介绍其中的一种方法：以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框。
这些边界框被称为*锚框*（anchor box）。

首先，让我们修改输出精度，以获得更简洁的输出。

```python
%matplotlib inline
import torch
from d2l import torch as d2l

torch.set_printoptions(2)  # 设置 PyTorch 的打印选项，将浮点数的显示精度设置为小数点后两位
```

## 生成多个锚框
我们指定输入图像、尺寸列表和宽高比列表，然后此函数将返回所有的锚框。

```python
#@save
def multibox_prior(data, sizes, ratios):
    """生成以每个像素为中心具有不同形状的锚框"""
    in_height, in_width = data.shape[-2:]  # 获取输入数据的高度和宽度
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)
    boxes_per_pixel = (num_sizes + num_ratios - 1)  # 每个像素生成的锚框数量
    size_tensor = torch.tensor(sizes, device=device)  # 尺寸列表转换为张量
    ratio_tensor = torch.tensor(ratios, device=device)  # 比例列表转换为张量

    # 为了将锚点移动到像素的中心，需要设置偏移量。
    # 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5
    offset_h, offset_w = 0.5, 0.5
    steps_h = 1.0 / in_height  # 在y轴上缩放步长
    steps_w = 1.0 / in_width  # 在x轴上缩放步长

    # 生成锚框的所有中心点
    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')  # 生成中心点网格
    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)

    # 生成“boxes_per_pixel”个高和宽，
    # 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)
    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),
                   sizes[0] * torch.sqrt(ratio_tensor[1:]))) \
                   * in_height / in_width  # 处理矩形输入
    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
                   sizes[0] / torch.sqrt(ratio_tensor[1:])))
    # 除以2来获得半高和半宽
    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(
                                        in_height * in_width, 1) / 2

    # 每个中心点都将有“boxes_per_pixel”个锚框，
    # 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次
    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
                dim=1).repeat_interleave(boxes_per_pixel, dim=0)
    output = out_grid + anchor_manipulations  # 计算最终的锚框坐标
    return output.unsqueeze(0)  # 增加一个维度作为批处理维度返回
```

可以看到[**返回的锚框变量`Y`的形状**]是（批量大小，锚框的数量，4）。

```python
img = d2l.plt.imread('../img/catdog.jpg')  # 读取图片文件并存储在变量img中
h, w = img.shape[:2]  # 获取图片的高度h和宽度w

print(h, w)  # 打印图片的高度和宽度
X = torch.rand(size=(1, 3, h, w))  # 创建一个随机张量X，形状为(1, 3, h, w)，用于模拟图像输入
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])  # 使用multibox_prior函数生成先验框Y
Y.shape  # 打印Y的形状
```

将锚框变量`Y`的形状更改为(图像高度,图像宽度,以同一像素为中心的锚框的数量,4)后，我们可以获得以指定像素的位置为中心的所有锚框。在接下来的内容中，我们[**访问以（250,250）为中心的第一个锚框**]。它有四个元素：锚框左上角的$(x, y)$轴坐标和右下角的$(x, y)$轴坐标。输出中两个轴的坐标各分别除以了图像的宽度和高度。

```python
boxes = Y.reshape(h, w, 5, 4)  # 将Y重塑为形状为(h, w, 5, 4)的张量，表示每个像素位置上的5个先验框
boxes[250, 250, 0, :]  # 获取位置在(250, 250)处的第一个边框的坐标信息
```

为了[**显示以图像中以某个像素为中心的所有锚框**]，定义下面的`show_bboxes`函数来在图像上绘制多个边界框。

```python
#@save
def show_bboxes(axes, bboxes, labels=None, colors=None):
    """显示所有边界框"""
    def _make_list(obj, default_values=None):
        if obj is None:
            obj = default_values
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj

    labels = _make_list(labels)  # 将labels转换为列表形式
    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])  # 将colors转换为列表形式，如果未提供默认为['b', 'g', 'r', 'm', 'c']
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]  # 根据索引循环选择颜色
        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)  # 将边界框坐标转换为矩形对象
        axes.add_patch(rect)  # 在图中添加矩形对象
        if labels and len(labels) > i:
            text_color = 'k' if color == 'w' else 'w'  # 根据矩形颜色选择文字颜色
            axes.text(rect.xy[0], rect.xy[1], labels[i],
                      va='center', ha='center', fontsize=9, color=text_color,
                      bbox=dict(facecolor=color, lw=0))  # 在矩形上方添加标签文字
```

正如从上面代码中所看到的，变量`boxes`中$x$轴和$y$轴的坐标值已分别除以图像的宽度和高度。绘制锚框时，我们需要恢复它们原始的坐标值。因此，在下面定义了变量`bbox_scale`。现在可以绘制出图像中所有以(250,250)为中心的锚框了。如下所示，缩放比为0.75且宽高比为1的蓝色锚框很好地围绕着图像中的狗。

```python
d2l.set_figsize()  # 设置图形大小
bbox_scale = torch.tensor((w, h, w, h))  # 创建张量表示边界框缩放比例
fig = d2l.plt.imshow(img)  # 在图中显示图像

# 调用show_bboxes函数显示边界框，传入参数为：
# - fig.axes: 图的坐标轴对象
# - boxes[250, 250, :, :] * bbox_scale: 在(250, 250)位置处的所有边界框乘以缩放比例后的坐标
# - ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2', 's=0.75, r=0.5']: 每个边界框的标签
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',
             's=0.75, r=0.5'])
```

## **交并比（IoU）**
*杰卡德系数*（Jaccard）可以衡量两组之间的相似性。给定集合$\mathcal{A}$和$\mathcal{B}$，他们的杰卡德系数是他们交集的大小除以他们并集的大小：

$$J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}.$$
对于两个边界框，它们的杰卡德系数通常称为*交并比*（intersection over union，IoU），即两个边界框相交面积与相并面积之比，如 :numref:`fig_iou`所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框完全重合。

接下来部分将使用交并比来衡量锚框和真实边界框之间、以及不同锚框之间的相似度。给定两个锚框或边界框的列表，以下`box_iou`函数将在这两个列表中计算它们成对的交并比。

```python
#@save
def box_iou(boxes1, boxes2):   
    """计算两个锚框或边界框列表中成对的交并比"""
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    
    # 计算boxes1和boxes2的面积
    # boxes1的形状：(boxes1的数量, 4)
    # boxes2的形状：(boxes2的数量, 4)
    areas1 = box_area(boxes1)
    areas2 = box_area(boxes2)
    
    # 计算交集的左上角和右下角坐标
    # inter_upperlefts的形状：(boxes1的数量, boxes2的数量, 2)
    # inter_lowerrights的形状：(boxes1的数量, boxes2的数量, 2)
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
    
    # 计算交集的宽度和高度
    # inters的形状：(boxes1的数量, boxes2的数量, 2)
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)
    
    # 计算交集的面积和并集的面积
    # inter_areas的形状：(boxes1的数量, boxes2的数量)
    # union_areas的形状：(boxes1的数量, boxes2的数量)
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    union_areas = areas1[:, None] + areas2 - inter_areas
    
    # 计算交并比
    # 返回值的形状：(boxes1的数量, boxes2的数量)
    return inter_areas / union_areas
```

## 在训练数据中标注锚框
### **将真实边界框分配给锚框**

```python
#@save
def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """将最接近的真实边界框分配给锚框"""
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]
    # 计算锚框和真实边界框之间的交并比矩阵
    jaccard = box_iou(anchors, ground_truth)
    # 初始化一个张量，用于记录每个锚框分配的真实边界框索引，默认为-1表示未分配
    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,
                                  device=device)
    # 根据IoU阈值决定是否分配真实边界框给锚框
    max_ious, indices = torch.max(jaccard, dim=1)
    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)
    box_j = indices[max_ious >= iou_threshold]
    anchors_bbox_map[anc_i] = box_j
    col_discard = torch.full((num_anchors,), -1)
    row_discard = torch.full((num_gt_boxes,), -1)
    # 对于每个真实边界框，分配IoU最大的锚框
    for _ in range(num_gt_boxes):
        max_idx = torch.argmax(jaccard)
        box_idx = (max_idx % num_gt_boxes).long()
        anc_idx = (max_idx / num_gt_boxes).long()
        anchors_bbox_map[anc_idx] = box_idx
        jaccard[:, box_idx] = col_discard
        jaccard[anc_idx, :] = row_discard
    return anchors_bbox_map
```

### 标记类别和偏移量
[**给定框$A$和$B$，中心坐标分别为$(x_a, y_a)$和$(x_b, y_b)$，宽度分别为$w_a$和$w_b$，高度分别为$h_a$和$h_b$，可以将$A$的偏移量标记为：

$$\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},
\frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},
\frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},
\frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right),$$
其中常量的默认值为 $\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1$ ， $\sigma_w=\sigma_h=0.2$。这种转换在下面的 `offset_boxes` 函数中实现。

```python
#@save
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """对锚框偏移量的转换"""
    # 将锚框和分配的真实边界框转换为中心坐标表示
    c_anc = d2l.box_corner_to_center(anchors)
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    # 计算偏移量的xy部分
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]
    # 计算偏移量的wh部分
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    # 组合成最终的偏移量张量
    offset = torch.cat([offset_xy, offset_wh], axis=1)
    return offset
```

如果一个锚框没有被分配真实边界框，我们只需将锚框的类别标记为*背景*（background）。背景类别的锚框通常被称为*负类*锚框，其余的被称为*正类*锚框。我们使用真实边界框（`labels`参数）实现以下`multibox_target`函数，来[**标记锚框的类别和偏移量**]（`anchors`参数）。此函数将背景类别的索引设置为零，然后将新类别的整数索引递增一。

```python
#@save
def multibox_target(anchors, labels):
    """使用真实边界框标记锚框"""
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)  # 获取批次大小和锚框，将维度为1的批次维度去除
    batch_offset, batch_mask, batch_class_labels = [], [], []  # 初始化存储偏移量、掩码和类别标签的列表
    device, num_anchors = anchors.device, anchors.shape[0]  # 获取设备信息和锚框的数量
    for i in range(batch_size):
        label = labels[i, :, :]  # 获取当前批次中的标签
        anchors_bbox_map = assign_anchor_to_bbox(label[:, 1:], anchors, device)  # 将标签中的边界框分配给锚框，并得到映射关系
        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(1, 4)  # 创建掩码，标记哪些锚框被分配了真实边界框
        # 将类标签和分配的边界框坐标初始化为零
        class_labels = torch.zeros(num_anchors, dtype=torch.long, device=device)  # 初始化所有锚框的类别标签
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32, device=device)  # 初始化所有锚框的边界框坐标

        # 使用真实边界框来标记锚框的类别。
        # 如果一个锚框没有被分配，标记其为背景（值为零）
        indices_true = torch.nonzero(anchors_bbox_map >= 0)  # 找到被分配了真实边界框的锚框索引
        bb_idx = anchors_bbox_map[indices_true]  # 获取分配了真实边界框的锚框的索引
        class_labels[indices_true] = label[bb_idx, 0].long() + 1  # 将分配了真实边界框的锚框的类别标签设为真实边界框的类别（加1）
        assigned_bb[indices_true] = label[bb_idx, 1:]  # 将分配了真实边界框的锚框的边界框坐标设为真实边界框的坐标

        # 偏移量转换
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask  # 计算偏移量，并根据掩码选择哪些锚框需要计算偏移量
        batch_offset.append(offset.reshape(-1))  # 将偏移量展平并添加到列表中
        batch_mask.append(bbox_mask.reshape(-1))  # 将掩码展平并添加到列表中
        batch_class_labels.append(class_labels)  # 添加类别标签到列表中

    bbox_offset = torch.stack(batch_offset)  # 将偏移量列表堆叠成张量
    bbox_mask = torch.stack(batch_mask)  # 将掩码列表堆叠成张量
    class_labels = torch.stack(batch_class_labels)  # 将类别标签列表堆叠成张量

    return bbox_offset, bbox_mask, class_labels  # 返回偏移量、掩码和类别标签张量
```

### 一个例子

下面通过一个具体的例子来说明锚框标签。我们已经为加载图像中的狗和猫定义了真实边界框，其中第一个元素是类别（0代表狗，1代表猫），其余四个元素是左上角和右下角的$(x, y)$轴坐标（范围介于0和1之间）。我们还构建了五个锚框，用左上角和右下角的坐标进行标记：$A_0, \ldots, A_4$（索引从0开始）。然后我们[**在图像中绘制这些真实边界框和锚框**]。

```python
ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],  # 真实边界框的张量，每行包含类别和坐标信息
                         [1, 0.55, 0.2, 0.9, 0.88]])  # 第一行为类别0（dog）和边界框坐标，第二行为类别1（cat）和边界框坐标
anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],  # 锚框的张量，每行包含锚框的坐标信息
                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                    [0.57, 0.3, 0.92, 0.9]])

fig = d2l.plt.imshow(img)  # 使用d2l库显示图像，并获取图像对象
show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')  # 显示真实边界框，将坐标乘以比例因子，显示类别为'dog'和'cat'，颜色为黑色
show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);  # 显示锚框，将坐标乘以比例因子，显示类别为'0'到'4'
```

使用上面定义的`multibox_target`函数，我们可以[**根据狗和猫的真实边界框，标注这些锚框的分类和偏移量**]。在这个例子中，背景、狗和猫的类索引分别为0、1和2。下面我们为锚框和真实边界框样本添加一个维度。

```python
labels = multibox_target(anchors.unsqueeze(dim=0),  # 使用multibox_target函数计算锚框和真实边界框的匹配标签
                         ground_truth.unsqueeze(dim=0))  # 锚框和真实边界框的张量需扩展一个维度以匹配函数的输入要求
```

返回的结果中有三个元素，都是张量格式。第三个元素包含标记的输入锚框的类别。

让我们根据图像中的锚框和真实边界框的位置来分析下面返回的类别标签。首先，在所有的锚框和真实边界框配对中，锚框$A_4$与猫的真实边界框的IoU是最大的。因此，$A_4$的类别被标记为猫。去除包含$A_4$或猫的真实边界框的配对，在剩下的配对中，锚框$A_1$和狗的真实边界框有最大的IoU。因此，$A_1$的类别被标记为狗。接下来，我们需要遍历剩下的三个未标记的锚框：$A_0$、$A_2$和$A_3$。对于$A_0$，与其拥有最大IoU的真实边界框的类别是狗，但IoU低于预定义的阈值（0.5），因此该类别被标记为背景；对于$A_2$，与其拥有最大IoU的真实边界框的类别是猫，IoU超过阈值，所以类别被标记为猫；对于$A_3$，与其拥有最大IoU的真实边界框的类别是猫，但值低于阈值，因此该类别被标记为背景。

```python
labels[2]  # 获取计算得到的标签结果的第三个元素（索引为2）
```

返回的第二个元素是掩码（mask）变量，形状为（批量大小，锚框数的四倍）。掩码变量中的元素与每个锚框的4个偏移量一一对应。由于我们不关心对背景的检测，负类的偏移量不应影响目标函数。通过元素乘法，掩码变量中的零将在计算目标函数之前过滤掉负类偏移量。

```python
labels[1]  # 获取计算得到的标签结果的第二个元素（索引为1）
```

返回的第一个元素包含了为每个锚框标记的四个偏移值。请注意，负类锚框的偏移量被标记为零。

```
labels[0]
```

## 使用非极大值抑制预测边界框

在预测时，我们先为图像生成多个锚框，再为这些锚框一一预测类别和偏移量。一个*预测好的边界框*则根据其中某个带有预测偏移量的锚框而生成。下面我们实现了`offset_inverse`函数，该函数将锚框和偏移量预测作为输入，并[**应用逆偏移变换来返回预测的边界框坐标**]。

```python
#@save
def offset_inverse(anchors, offset_preds):
    """根据带有预测偏移量的锚框来预测边界框"""
    anc = d2l.box_corner_to_center(anchors)  # 将锚框从左上角坐标形式转换为中心坐标形式
    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]  # 计算预测边界框的中心坐标
    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]  # 计算预测边界框的宽度和高度
    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)  # 合并预测的边界框坐标和尺寸
    predicted_bbox = d2l.box_center_to_corner(pred_bbox)  # 将预测的边界框从中心坐标形式转换为左上角坐标形式
    return predicted_bbox  # 返回预测得到的边界框坐标
```

[**以下`nms`函数按降序对置信度进行排序并返回其索引**]。

```python
#@save
def nms(boxes, scores, iou_threshold):
    """使用非极大值抑制（NMS）算法对预测边界框进行过滤"""
    # 根据预测边界框的置信度分数进行排序
    B = torch.argsort(scores, dim=-1, descending=True)
    keep = []  # 用于存储保留的预测边界框的索引
    while B.numel() > 0:
        i = B[0]
        keep.append(i)
        if B.numel() == 1:
            break
        # 计算当前预测边界框与其余预测边界框的IoU（交并比）
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        # 选取IoU小于设定阈值的预测边界框进行保留
        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)
        B = B[inds + 1]  # 更新B，去掉与当前选中边界框IoU大于阈值的边界框
    return torch.tensor(keep, device=boxes.device)  # 返回保留的预测边界框的索引
```

我们定义以下`multibox_detection`函数来[**将非极大值抑制应用于预测边界框**]。这里的实现有点复杂，请不要担心。我们将在实现之后，马上用一个具体的例子来展示它是如何工作的。

```python
#@save
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """使用非极大值抑制来预测边界框"""
    device, batch_size = cls_probs.device, cls_probs.shape[0]
    anchors = anchors.squeeze(0)  # 去掉维度为1的批次维度
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    out = []
    for i in range(batch_size):
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        # 找到置信度最高的类别和对应的置信度
        conf, class_id = torch.max(cls_prob[1:], 0)
        # 根据偏移预测值计算预测边界框的位置
        predicted_bb = offset_inverse(anchors, offset_pred)
        # 使用非极大值抑制选择最终的预测边界框
        keep = nms(predicted_bb, conf, nms_threshold)

        # 找到所有的non_keep索引，并将类设置为背景
        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)
        combined = torch.cat((keep, all_idx))
        uniques, counts = combined.unique(return_counts=True)
        non_keep = uniques[counts == 1]
        all_id_sorted = torch.cat((keep, non_keep))
        class_id[non_keep] = -1  # 将非保留框的类别设置为背景
        class_id = class_id[all_id_sorted]
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # 使用pos_threshold过滤掉置信度过低的预测
        below_min_idx = (conf < pos_threshold)
        class_id[below_min_idx] = -1
        conf[below_min_idx] = 1 - conf[below_min_idx]
        # 组装预测信息：类别、置信度、边界框坐标
        pred_info = torch.cat((class_id.unsqueeze(1),
                               conf.unsqueeze(1),
                               predicted_bb), dim=1)
        out.append(pred_info)
    return torch.stack(out)
```

现在让我们[**将上述算法应用到一个带有四个锚框的具体示例中**]。为简单起见，我们假设预测的偏移量都是零，这意味着预测的边界框即是锚框。对于背景、狗和猫其中的每个类，我们还定义了它的预测概率。

```python
anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],  # 锚框的定义，每个锚框由左上角和右下角坐标组成
                        [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])

offset_preds = torch.tensor([0] * anchors.numel())  # 偏移预测，初始化为全零，与锚框数量相同

cls_probs = torch.tensor([[0] * 4,  # 背景的预测概率
                          [0.9, 0.8, 0.7, 0.1],  # 狗的预测概率
                          [0.1, 0.2, 0.3, 0.9]])  # 猫的预测概率
# 类别预测概率，每行代表一个样本的预测结果，第一行为背景，后面分别为狗和猫的预测概率
```

我们可以[**在图像上绘制这些预测边界框和置信度**]。

```python
fig = d2l.plt.imshow(img)   # 在图像上显示图像的绘图对象
show_bboxes(fig.axes, anchors * bbox_scale,
            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])   # 调用显示边界框的函数，传入绘图对象的坐标轴、缩放后的锚框以及每个锚框对应的类别和概率信息

```

现在我们可以调用`multibox_detection`函数来执行非极大值抑制，其中阈值设置为0.5。请注意，我们在示例的张量输入中添加了维度。

我们可以看到[**返回结果的形状是（批量大小，锚框的数量，6）**]。最内层维度中的六个元素提供了同一预测边界框的输出信息。第一个元素是预测的类索引，从0开始（0代表狗，1代表猫），值-1表示背景或在非极大值抑制中被移除了。第二个元素是预测的边界框的置信度。其余四个元素分别是预测边界框左上角和右下角的$(x, y)$轴坐标（范围介于0和1之间）。

```python
# 使用多框检测函数进行预测，传入扩展后的类别概率、偏移预测和锚框，并设置非极大值抑制阈值为0.5
output = multibox_detection(cls_probs.unsqueeze(dim=0),
                            offset_preds.unsqueeze(dim=0),
                            anchors.unsqueeze(dim=0),
                            nms_threshold=0.5)
output
```

删除-1类别（背景）的预测边界框后，我们可以[**输出由非极大值抑制保存的最终预测边界框**]。

```python
fig = d2l.plt.imshow(img)  # 显示图像

for i in output[0].detach().numpy():  # 遍历输出的每个预测框
    if i[0] == -1:  # 如果类别是-1，跳过
        continue
    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])  # 设置标签，显示类别和置信度
    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)  # 在图像上显示边界框
```

# 多尺度目标检测
## 多尺度锚框
在不同尺度下，我们可以生成不同数量和不同大小的锚框。直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。例如，$1 \times 1$、$1 \times 2$和$2 \times 2$的目标可以分别以4、2和1种可能的方式出现在$2 \times 2$图像上。因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。

为了演示如何在多个尺度下生成锚框，让我们先读取一张图像。它的高度和宽度分别为561和728像素。

```python
%matplotlib inline
import torch
from d2l import torch as d2l

img = d2l.plt.imread('../img/catdog.jpg')  # 读取图像文件
h, w = img.shape[:2]  # 获取图像的高度和宽度
h, w  # 打印图像的高度和宽度
```

给定特征图的宽度和高度`fmap_w`和`fmap_h`，以下函数将*均匀地*对任何输入图像中`fmap_h`行和`fmap_w`列中的像素进行采样。以这些均匀采样的像素为中心，将会生成大小为`s`（假设列表`s`的长度为1）且宽高比（`ratios`）不同的锚框。

```python
def display_anchors(fmap_w, fmap_h, s):
    d2l.set_figsize()  # 设置图形大小
    # 前两个维度上的值不影响输出
    fmap = torch.zeros((1, 10, fmap_h, fmap_w))  # 创建一个特征图张量
    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])  # 生成锚框
    bbox_scale = torch.tensor((w, h, w, h))  # 创建边界框缩放因子张量
    d2l.show_bboxes(d2l.plt.imshow(img).axes,  # 在图像上显示锚框
                    anchors[0] * bbox_scale)  # 缩放并显示锚框
```

首先，让我们考虑[**探测小目标**]。为了在显示时更容易分辨，在这里具有不同中心的锚框不会重叠：锚框的尺度设置为0.15，特征图的高度和宽度设置为4。我们可以看到，图像上4行和4列的锚框的中心是均匀分布的。

```python
display_anchors(fmap_w=4, fmap_h=4, s=[0.15])  # 调用函数，传入特征图宽度、高度和锚框大小参数
```

然后，我们[**将特征图的高度和宽度减小一半，然后使用较大的锚框来检测较大的目标**]。当尺度设置为0.4时，一些锚框将彼此重叠。

```python
display_anchors(fmap_w=2, fmap_h=2, s=[0.4])   # 调用函数，传入特征图宽度、高度和锚框大小参数
```

最后，我们进一步[**将特征图的高度和宽度减小一半，然后将锚框的尺度增加到0.8**]。此时，锚框的中心即是图像的中心。

```python
display_anchors(fmap_w=1, fmap_h=1, s=[0.8])   # 调用函数，传入特征图宽度、高度和锚框大小参数
```

# 目标检测数据集
为了快速测试目标检测模型，[**我们收集并标记了一个小型数据集**]。首先，我们拍摄了一组香蕉的照片，并生成了1000张不同角度和大小的香蕉图像。
然后，我们在一些背景图片的随机位置上放一张香蕉的图像。最后，我们在图片上为这些香蕉标记了边界框。

## **下载数据集**

包含所有图像和CSV标签文件的香蕉检测数据集可以直接从互联网下载。

```python
%matplotlib inline
import os  # 导入操作系统交互功能的库
import pandas as pd  # 导入数据分析工具pandas，并简称为pd
import torch  # 导入PyTorch深度学习框架的核心库
import torchvision  # 导入PyTorch官方的视觉工具库torchvision
from d2l import torch as d2l  # 从d2l库中导入torch接口，并简称为d2l
```

```python
#@save
d2l.DATA_HUB['banana-detection'] = (
    d2l.DATA_URL + 'banana-detection.zip',  # 数据集的下载链接
    '5de26c8fce5ccdea9f91267273464dc968d20d72')  # 数据集文件的哈希值
```

## 读取数据集

通过`read_data_bananas`函数，我们[**读取香蕉检测数据集**]。该数据集包括一个的CSV文件，内含目标类别标签和位于左上角和右下角的真实边界框坐标。

```python
#@save
def read_data_bananas(is_train=True):
    """读取香蕉检测数据集中的图像和标签"""
    data_dir = d2l.download_extract('banana-detection')  # 下载并解压香蕉检测数据集
    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
                             else 'bananas_val', 'label.csv')  # 根据是否为训练集选择相应的CSV文件
    csv_data = pd.read_csv(csv_fname)  # 读取CSV文件中的数据
    csv_data = csv_data.set_index('img_name')  # 将图像名称设置为索引
    images, targets = [], []
    for img_name, target in csv_data.iterrows():
        images.append(torchvision.io.read_image(
            os.path.join(data_dir, 'bananas_train' if is_train else
                         'bananas_val', 'images', f'{img_name}')))  # 读取图像数据
        # 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），
        # 其中所有图像都具有相同的香蕉类（索引为0）
        targets.append(list(target))  # 将每个图像的标签信息添加到targets列表中
    return images, torch.tensor(targets).unsqueeze(1) / 256  # 返回图像数据和标签数据，标签数据进行归一化处理
```

通过使用`read_data_bananas`函数读取图像和标签，以下`BananasDataset`类别将允许我们[**创建一个自定义`Dataset`实例**]来加载香蕉检测数据集。

```python
#@save
class BananasDataset(torch.utils.data.Dataset):
    """一个用于加载香蕉检测数据集的自定义数据集"""

    def __init__(self, is_train):
        self.features, self.labels = read_data_bananas(is_train)  # 调用read_data_bananas函数读取数据集
        print('read ' + str(len(self.features)) + (f' training examples' if
              is_train else f' validation examples'))  # 打印读取的数据集大小信息

    def __getitem__(self, idx):
        return (self.features[idx].float(), self.labels[idx])  # 返回第idx个样本的图像和标签数据

    def __len__(self):
        return len(self.features)  # 返回数据集的长度，即样本数量
```

最后，我们定义`load_data_bananas`函数，来[**为训练集和测试集返回两个数据加载器实例**]。对于测试集，无须按随机顺序读取它。

```python
#@save
def load_data_bananas(batch_size):
    """加载香蕉检测数据集"""
    # 创建训练集数据迭代器，每次迭代返回一个随机打乱的批量训练数据
    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),
                                             batch_size, shuffle=True)
    # 创建验证集数据迭代器，每次迭代返回一个顺序加载的批量验证数据
    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),
                                           batch_size)
    return train_iter, val_iter
```

让我们[**读取一个小批量，并打印其中的图像和标签的形状**]。图像的小批量的形状为（批量大小、通道数、高度、宽度），看起来很眼熟：它与我们之前图像分类任务中的相同。标签的小批量的形状为（批量大小，$m$，5），其中$m$是数据集的任何图像中边界框可能出现的最大数量。

小批量计算虽然高效，但它要求每张图像含有相同数量的边界框，以便放在同一个批量中。通常来说，图像可能拥有不同数量个边界框；因此，在达到$m$之前，边界框少于$m$的图像将被非法边界框填充。这样，每个边界框的标签将被长度为5的数组表示。数组中的第一个元素是边界框中对象的类别，其中-1表示用于填充的非法边界框。数组的其余四个元素是边界框左上角和右下角的（$x$，$y$）坐标值（值域在0～1之间）。对于香蕉数据集而言，由于每张图像上只有一个边界框，因此$m=1$。

```python
batch_size, edge_size = 32, 256  # 设置批量大小为32，边缘大小为256
train_iter, _ = load_data_bananas(batch_size)   # 加载香蕉检测数据集并获取训练集迭代器
batch = next(iter(train_iter))   # 获取一个批量的训练数据
batch[0].shape, batch[1].shape   # 打印批量数据的输入（图像）和输出（标签）的形状
```

## **演示**

让我们展示10幅带有真实边界框的图像。我们可以看到在所有这些图像中香蕉的旋转角度、大小和位置都有所不同。当然，这只是一个简单的人工数据集，实践中真实世界的数据集通常要复杂得多。

```python
imgs = (batch[0][0:10].permute(0, 2, 3, 1)) / 255  # 将图像数据取出并按照指定维度顺序重新排列，然后进行归一化处理
axes = d2l.show_images(imgs, 2, 5, scale=2)  # 使用d2l库展示图像，2行5列，放大倍数为2
for ax, label in zip(axes, batch[1][0:10]):
    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])  # 在图像上显示边界框，使用白色标记
```

# 单发多框检测（SSD）
## 模型
### **类别预测层**
类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入同一空间坐标（$x$、$y$）：输出特征图上（$x$、$y$）坐标的通道里包含了以输入特征图（$x$、$y$）坐标为中心生成的所有锚框的类别预测。因此输出通道数为$a(q+1)$，其中索引为$i(q+1) + j$（$0 \leq j \leq q$）的通道代表了索引为$i$的锚框有关类别索引为$j$的预测。

在下面，我们定义了这样一个类别预测层，通过参数`num_anchors`和`num_classes`分别指定了$a$和$q$。该图层使用填充为1的$3\times3$的卷积层。此卷积层的输入和输出的宽度和高度保持不变。

```python
%matplotlib inline
import torch  # 导入PyTorch库
import torchvision  # 导入torchvision库
from torch import nn  # 从PyTorch中导入神经网络模块
from torch.nn import functional as F  # 从PyTorch中导入函数模块，并命名为F
from d2l import torch as d2l  # 从d2l库中导入torch模块

# 创建一个用于分类预测的卷积层，输入通道数为num_inputs，输出通道数为num_anchors * (num_classes + 1)，卷积核大小为3x3，填充为1
def cls_predictor(num_inputs, num_anchors, num_classes):
    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),
                     kernel_size=3, padding=1)  
```

### **边界框预测层**

边界框预测层的设计与类别预测层的设计类似。唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是$q+1$个类别。

```python
# 创建一个用于边界框预测的卷积层，输入通道数为num_inputs，输出通道数为num_anchors * 4，卷积核大小为3x3，填充为1
def bbox_predictor(num_inputs, num_anchors):
    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)  
```

### **连结多尺度的预测**

单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。在不同的尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。因此，不同尺度下预测输出的形状可能会有所不同。

在以下示例中，我们为同一个小批量构建两个不同比例（`Y1`和`Y2`）的特征图，其中`Y2`的高度和宽度是`Y1`的一半。以类别预测为例，假设`Y1`和`Y2`的每个单元分别生成了$5$个和$3$个锚框。进一步假设目标类别的数量为$10$，对于特征图`Y1`和`Y2`，类别预测输出中的通道数分别为$5\times(10+1)=55$和$3\times(10+1)=33$，其中任一输出的形状是（批量大小，通道数，高度，宽度）。

```python
def forward(x, block):
    return block(x)

Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))  # 使用cls_predictor对输入张量进行前向传播，得到输出张量Y1
Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))  # 使用cls_predictor对输入张量进行前向传播，得到输出张量Y2
Y1.shape, Y2.shape  # 输出Y1和Y2的形状
```

正如我们所看到的，除了批量大小这一维度外，其他三个维度都具有不同的尺寸。为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。

通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的（批量大小，高$\times$宽$\times$通道数）的格式，以方便之后在维度$1$上的连结。

```python
def flatten_pred(pred):
    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)  # 将预测张量pred进行维度重排和展平操作，使其形状适合后续处理

def concat_preds(preds):
    return torch.cat([flatten_pred(p) for p in preds], dim=1)  # 将多个预测张量preds连接起来，并沿着第1维度（通道维度）进行拼接
```

这样一来，尽管`Y1`和`Y2`在通道数、高度和宽度方面具有不同的大小，我们仍然可以在同一个小批量的两个不同尺度上连接这两个预测输出。

```python
concat_preds([Y1, Y2]).shape  # 连接Y1和Y2，并输出连接后的张量的形状
```

### **高和宽减半块**

为了在多个尺度下检测目标，我们在下面定义了高和宽减半块`down_sample_blk`，该模块将输入特征图的高度和宽度减半。事实上，该块应用了VGG模块设计。更具体地说，每个高和宽减半块由两个填充为$1$的$3\times3$的卷积层、以及步幅为$2$的$2\times2$最大汇聚层组成。我们知道，填充为$1$的$3\times3$卷积层不改变特征图的形状。但是，其后的$2\times2$的最大汇聚层将输入特征图的高度和宽度减少了一半。对于此高和宽减半块的输入和输出特征图，因为$1\times 2+(3-1)+(3-1)=6$，所以输出中的每个单元在输入上都有一个$6\times6$的感受野。因此，高和宽减半块会扩大每个单元在其输出特征图中的感受野。

```python
def down_sample_blk(in_channels, out_channels):
    blk = []  # 创建一个空列表，用于存放网络块的层

    # 重复两次卷积、批归一化、ReLU激活函数序列
    for _ in range(2):
        blk.append(nn.Conv2d(in_channels, out_channels,
                             kernel_size=3, padding=1))  # 添加卷积层，输入通道数为in_channels，输出通道数为out_channels，卷积核大小为3x3，填充为1
        blk.append(nn.BatchNorm2d(out_channels))  # 添加批归一化层，通道数为out_channels
        blk.append(nn.ReLU())  # 添加ReLU激活函数层
        in_channels = out_channels  # 更新下一轮循环的输入通道数为当前的输出通道数

    blk.append(nn.MaxPool2d(2))  # 添加最大池化层，池化核大小为2x2，步幅默认为池化核大小
    return nn.Sequential(*blk)  # 返回一个包含所有层的序列模块
```

在以下示例中，我们构建的高和宽减半块会更改输入通道的数量，并将输入特征图的高度和宽度减半。

```python
forward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape  # 使用down_sample_blk对输入张量进行前向传播，并输出前向传播后的张量的形状
```

### **基本网络块**

基本网络块用于从输入图像中抽取特征。为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍。给定输入图像的形状为$256\times256$，此基本网络块输出的特征图形状为$32 \times 32$（$256/2^3=32$）。

```python
def base_net():
    blk = []  # 创建一个空列表，用于存放网络块的层
    num_filters = [3, 16, 32, 64]  # 定义不同层的输入和输出通道数

    # 根据num_filters列表中定义的通道数创建网络块
    for i in range(len(num_filters) - 1):
        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))  # 使用down_sample_blk函数创建每个网络块

    return nn.Sequential(*blk)  # 返回一个包含所有网络块的序列模块

forward(torch.zeros((2, 3, 256, 256)), base_net()).shape  # 对输入张量使用base_net进行前向传播，并输出前向传播后的张量的形状
```

### 完整的模型

[**完整的单发多框检测模型由五个模块组成**]。每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是基本网络块，第二个到第四个是高和宽减半块，最后一个模块使用全局最大池将高度和宽度都降到1。从技术上讲，第二到第五个区块都是多尺度特征块。

```python
def get_blk(i):
    if i == 0:
        blk = base_net()  # 如果i为0，使用base_net函数创建网络块blk
    elif i == 1:
        blk = down_sample_blk(64, 128)  # 如果i为1，使用down_sample_blk函数创建输入通道数为64，输出通道数为128的网络块blk
    elif i == 4:
        blk = nn.AdaptiveMaxPool2d((1,1))  # 如果i为4，创建一个自适应最大池化层blk，输出大小为(1,1)
    else:
        blk = down_sample_blk(128, 128)  # 对于其他i值，使用down_sample_blk函数创建输入和输出通道数均为128的网络块blk
    return blk
```

现在我们[**为每个块定义前向传播**]。与图像分类任务不同，此处的输出包括：CNN特征图`Y`；在当前尺度下根据`Y`生成的锚框；预测的这些锚框的类别和偏移量（基于`Y`）。

```python
def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):
    Y = blk(X)  # 使用blk对输入张量X进行前向传播，得到特征图Y
    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)  # 使用特征图Y生成锚框anchors
    cls_preds = cls_predictor(Y)  # 使用分类预测器cls_predictor对特征图Y进行分类预测，得到类别预测cls_preds
    bbox_preds = bbox_predictor(Y)  # 使用边界框预测器bbox_predictor对特征图Y进行边界框预测，得到边界框预测bbox_preds
    return (Y, anchors, cls_preds, bbox_preds)  # 返回特征图Y、锚框anchors、类别预测cls_preds和边界框预测bbox_preds
```

回想一下，一个较接近顶部的多尺度特征块是用于检测较大目标的，因此需要生成更大的锚框。在上面的前向传播中，在每个多尺度特征块上，我们通过调用的`multibox_prior`函数（见 :numref:`sec_anchor`）的`sizes`参数传递两个比例值的列表。在下面，0.2和1.05之间的区间被均匀分成五个部分，以确定五个模块的在不同尺度下的较小值：0.2、0.37、0.54、0.71和0.88。之后，他们较大的值由$\sqrt{0.2 \times 0.37} = 0.272$、$\sqrt{0.37 \times 0.54} = 0.447$等给出。

```python
sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],
         [0.88, 0.961]]  # 定义不同尺寸的锚框，每个尺寸以 [宽度, 高度] 的形式表示

ratios = [[1, 2, 0.5]] * 5  # 定义每个尺寸对应的长宽比，每个列表包含长宽比1、2和0.5

num_anchors = len(sizes[0]) + len(ratios[0]) - 1  # 计算总的锚框数量，每个尺寸有宽度和高度，减去默认的长宽比1
```

现在，我们就可以按如下方式[**定义完整的模型**]`TinySSD`了。

```python
class TinySSD(nn.Module):
    def __init__(self, num_classes, **kwargs):
        super(TinySSD, self).__init__(**kwargs)
        self.num_classes = num_classes
        idx_to_in_channels = [64, 128, 128, 128, 128]
        for i in range(5):
            setattr(self, f'blk_{i}', get_blk(i))  # 设置名为blk_i的模块，调用get_blk函数得到
            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],
                                                    num_anchors, num_classes))  # 设置名为cls_i的模块，调用cls_predictor函数得到
            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],
                                                      num_anchors))  # 设置名为bbox_i的模块，调用bbox_predictor函数得到

    def forward(self, X):
        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5
        for i in range(5):
            # getattr(self,'blk_%d'%i)即访问self.blk_i
            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(
                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],
                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))
        anchors = torch.cat(anchors, dim=1)  # 在维度1上拼接anchors
        cls_preds = concat_preds(cls_preds)  # 拼接cls_preds
        cls_preds = cls_preds.reshape(
            cls_preds.shape[0], -1, self.num_classes + 1)  # 调整cls_preds的形状
        bbox_preds = concat_preds(bbox_preds)  # 拼接bbox_preds
        return anchors, cls_preds, bbox_preds
```

我们[**创建一个模型实例，然后使用它**]对一个$256 \times 256$像素的小批量图像`X`(**执行前向传播**)。

如本节前面部分所示，第一个模块输出特征图的形状为$32 \times 32$。回想一下，第二到第四个模块为高和宽减半块，第五个模块为全局汇聚层。由于以特征图的每个单元为中心有$4$个锚框生成，因此在所有五个尺度下，每个图像总共生成$(32^2 + 16^2 + 8^2 + 4^2 + 1)\times 4 = 5444$个锚框。

```python
net = TinySSD(num_classes=1)  # 创建一个TinySSD模型实例，num_classes设置为1，适用于二分类任务
X = torch.zeros((32, 3, 256, 256))  # 创建一个输入张量X，形状为(32, 3, 256, 256)，表示32个3通道的256x256图像
anchors, cls_preds, bbox_preds = net(X)  # 将输入X传递给TinySSD模型net，得到输出的锚框、类别预测和边界框预测

print('output anchors:', anchors.shape)  # 打印输出的锚框的形状
print('output class preds:', cls_preds.shape)  # 打印输出的类别预测的形状
print('output bbox preds:', bbox_preds.shape)  # 打印输出的边界框预测的形状
```

## 训练模型

现在，我们将描述如何训练用于目标检测的单发多框检测模型。
### 读取数据集和初始化

首先，让我们[**读取**] (**香蕉检测数据集**)。

```python
batch_size = 32  # 设置批量大小为32
train_iter, _ = d2l.load_data_bananas(batch_size)  # 使用d2l加载香蕉数据集，并返回训练集迭代器train_iter，测试集迭代器_
```

香蕉检测数据集中，目标的类别数为1。定义好模型后，我们需要(**初始化其参数并定义优化算法**)。

```python
device, net = d2l.try_gpu(), TinySSD(num_classes=1)  # 尝试获取GPU设备（如果可用），并创建一个TinySSD模型实例，用于二分类任务
trainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)  # 使用随机梯度下降（SGD）优化器来优化TinySSD模型的参数，设置学习率为0.2和权重衰减为5e-4
```

### **定义损失函数和评价函数**

目标检测有两种类型的损失。第一种有关锚框类别的损失：我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算；第二种有关正类锚框偏移量的损失：预测偏移量是一个回归问题。但是，对于这个回归问题，我们在这里不使用平方损失，而是使用$L_1$范数损失，即预测值和真实值之差的绝对值。掩码变量`bbox_masks`令负类锚框和填充锚框不参与损失的计算。最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数。

```python
cls_loss = nn.CrossEntropyLoss(reduction='none')  # 定义分类损失函数为交叉熵损失，reduction='none'表示不对每个样本的损失求平均

bbox_loss = nn.L1Loss(reduction='none')  # 定义边界框损失函数为L1损失，reduction='none'表示不对每个样本的损失求平均

def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):
    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]
    cls = cls_loss(cls_preds.reshape(-1, num_classes),  # 计算分类损失，reshape使预测值和标签值的形状对齐，然后计算损失
                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)
    bbox = bbox_loss(bbox_preds * bbox_masks,  # 计算边界框损失，乘以掩码以过滤无效预测
                     bbox_labels * bbox_masks).mean(dim=1)
    return cls + bbox  # 返回分类损失和边界框损失的总和作为最终的损失
```

我们可以沿用准确率评价分类结果。由于偏移量使用了$L_1$范数损失，我们使用*平均绝对误差*来评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的。

```python
def cls_eval(cls_preds, cls_labels):
    return float((cls_preds.argmax(dim=-1).type(cls_labels.dtype) == cls_labels).sum())
    # 返回分类预测的准确数量，将预测的最大概率值所在的类别与真实标签比较

def bbox_eval(bbox_preds, bbox_labels, bbox_masks):
    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())
    # 返回边界框预测的总误差，考虑了边界框掩码以过滤无效区域
```

### **训练模型**

在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框（`anchors`），并预测其类别（`cls_preds`）和偏移量（`bbox_preds`）。然后，我们根据标签信息`Y`为生成的锚框标记类别（`cls_labels`）和偏移量（`bbox_labels`）。最后，我们根据类别和偏移量的预测和标注值计算损失函数。为了代码简洁，这里没有评价测试数据集。

```python
num_epochs, timer = 20, d2l.Timer()
animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                        legend=['class error', 'bbox mae'])
net = net.to(device)
for epoch in range(num_epochs):
    metric = d2l.Accumulator(4)  # 累加器，用于存储训练精度和绝对误差的和及示例数
    net.train()
    for features, target in train_iter:
        timer.start()  # 计时器开始计时
        trainer.zero_grad()  # 梯度清零
        X, Y = features.to(device), target.to(device)
        anchors, cls_preds, bbox_preds = net(X)  # 生成多尺度锚框并进行预测
        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)  # 为锚框标注类别和偏移量
        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,
                      bbox_masks)  # 计算损失函数
        l.mean().backward()  # 反向传播计算梯度
        trainer.step()  # 更新模型参数
        # 计算并累加分类错误率、示例数、bbox平均绝对误差、示例数
        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),
                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),
                   bbox_labels.numel())
    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]
    animator.add(epoch + 1, (cls_err, bbox_mae))  # 更新动画图表数据
print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')
print(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on '
      f'{str(device)}')  # 输出训练速度及设备信息
```

## **预测目标**

在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。

```python
# 读取图像并进行预处理
X = torchvision.io.read_image('../img/banana.jpg').unsqueeze(0).float()

# 将图像张量转换为 numpy 数组，用于后续处理（如果需要）
img = X.squeeze(0).permute(1, 2, 0).numpy()
```

使用下面的`multibox_detection`函数，我们可以根据锚框及其预测偏移量得到预测边界框。然后，通过非极大值抑制来移除相似的预测边界框。

```python
def predict(X):
    net.eval()  # 将神经网络设置为评估模式，不计算梯度
    anchors, cls_preds, bbox_preds = net(X.to(device))  # 使用网络进行推理，得到锚框、类别预测和边界框预测
    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)  # 对类别预测进行softmax归一化，并按照指定的维度重新排列
    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)  # 使用多框检测算法进行检测
    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]  # 选择类别预测不为-1（背景）的检测结果的索引
    return output[0, idx]  # 返回过滤后的检测结果

output = predict(X)  # 调用predict函数进行推理得到检测结果
```

最后，我们[**筛选所有置信度不低于0.9的边界框，做为最终输出**]。

```python
def display(img, output, threshold):
    d2l.set_figsize((5, 5))  # 设置图形大小为 5x5 英寸
    fig = d2l.plt.imshow(img)  # 在matplotlib中显示图像
    for row in output:
        score = float(row[1])  # 获取检测结果中的置信度分数
        if score < threshold:  # 如果置信度分数低于阈值，则跳过此检测结果
            continue
        h, w = img.shape[0:2]  # 获取图像的高度和宽度
        bbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]  # 根据检测结果中的边界框信息计算真实坐标
        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')  # 在图像上显示边界框及其置信度

display(img, output.cpu(), threshold=0.9)  # 调用display函数显示经过过滤后的检测结果，阈值为0.9
```

# 区域卷积神经网络（R-CNN）系列
## R-CNN
## Fast R-CNN
下面，我们演示了兴趣区域汇聚层的计算方法。假设卷积神经网络抽取的特征`X`的高度和宽度都是4，且只有单通道。

```python
import torch
import torchvision

X = torch.arange(16.).reshape(1, 1, 4, 4)
X
```

让我们进一步假设输入图像的高度和宽度都是40像素，且选择性搜索在此图像上生成了两个提议区域。每个区域由5个元素表示：区域目标类别、左上角和右下角的$(x, y)$坐标。

```python
rois = torch.Tensor([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])  # 定义了一个包含两个ROI的张量
```

由于`X`的高和宽是输入图像高和宽的$1/10$，因此，两个提议区域的坐标先按`spatial_scale`乘以0.1。然后，在`X`上分别标出这两个兴趣区域`X[:, :, 0:3, 0:3]`和`X[:, :, 1:4, 0:4]`。最后，在$2\times 2$的兴趣区域汇聚层中，每个兴趣区域被划分为子窗口网格，并进一步抽取相同形状$2\times 2$的特征。

```python
torchvision.ops.roi_pool(X, rois, output_size=(2, 2), spatial_scale=0.1)  # 使用ROI池化对特征图X进行操作
# X: 输入的特征图，通常是一个四维张量，表示一批图像的特征
# rois: 包含ROI（感兴趣区域）的张量，每个ROI由五个值定义：索引，左上角坐标(x1, y1)，右下角坐标(x2, y2)
# output_size: 指定ROI池化操作的输出大小，这里设为(2, 2)，表示每个ROI输出的大小为2x2
# spatial_scale: 空间比例因子，用于将输入坐标映射到特征图尺寸上的比例，这里设置为0.1
```

# 语义分割和数据集
## Pascal VOC2012 语义分割数据集

[**最重要的语义分割数据集之一是[Pascal VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)。**]下面我们深入了解一下这个数据集。

```python
%matplotlib inline
import os
import torch
import torchvision
from d2l import torch as d2l
```

数据集的tar文件大约为2GB，所以下载可能需要一段时间。提取出的数据集位于`../data/VOCdevkit/VOC2012`。

```python
#@save
# 将 VOC2012 数据集的 URL 和 SHA-1 值存储在数据集 hub 中
d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')

voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')   # 下载并解压 VOC2012 数据集到指定的目录
```

进入路径`../data/VOCdevkit/VOC2012`之后，我们可以看到数据集的不同组件。`ImageSets/Segmentation`路径包含用于训练和测试样本的文本文件，而`JPEGImages`和`SegmentationClass`路径分别存储着每个示例的输入图像和标签。此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。此外，标签中颜色相同的像素属于同一个语义类别。下面将`read_voc_images`函数定义为[**将所有输入的图像和标签读入内存**]。

```python
voc_dir = "../data/VOCdevkit/VOC2012"
#@save
def read_voc_images(voc_dir, is_train=True):
    """读取所有VOC图像并标注"""
    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',
                             'train.txt' if is_train else 'val.txt')
    mode = torchvision.io.image.ImageReadMode.RGB  # 设置图像读取模式为RGB
    with open(txt_fname, 'r') as f:
        images = f.read().split()  # 读取txt文件中的图像列表
    features, labels = [], []
    for i, fname in enumerate(images):
        # 读取图像和标签
        features.append(torchvision.io.read_image(os.path.join(
            voc_dir, 'JPEGImages', f'{fname}.jpg')))  # 读取JPEG图像
        labels.append(torchvision.io.read_image(os.path.join(
            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))  # 读取分割标签图像
    return features, labels

train_features, train_labels = read_voc_images(voc_dir, True)  # 读取训练集的VOC图像及其标签
```

下面我们[**绘制前5个输入图像及其标签**]。在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别。

```python
n = 5
imgs = train_features[0:n] + train_labels[0:n]  # 取前n个训练图像和对应的标签
imgs = [img.permute(1, 2, 0) for img in imgs]  # 调整张量维度顺序为(H, W, C)，以便于显示
d2l.show_images(imgs, 2, n);  # 使用d2l库的show_images函数展示图像，2行n列
```

接下来，我们[**列举RGB颜色值和类名**]。

```python
#@save
VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],
                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
                [0, 64, 128]]  # VOC数据集的颜色映射，用于图像分割标签的可视化编码

#@save
VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',
               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
               'diningtable', 'dog', 'horse', 'motorbike', 'person',
               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']  # VOC数据集中的类别列表，用于解码图像分割标签
```

通过上面定义的两个常量，我们可以方便地[**查找标签中每个像素的类索引**]。我们定义了`voc_colormap2label`函数来构建从上述RGB颜色值到类别索引的映射，而`voc_label_indices`函数将RGB值映射到在Pascal VOC2012数据集中的类别索引。

```python
#@save
def voc_colormap2label():
    """构建从RGB到VOC类别索引的映射"""
    # 创建一个长度为256^3的全零张量，用于存储颜色映射到类别索引的结果
    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)
    # 遍历VOC_COLORMAP中的每一个颜色映射，将其转换为对应的类别索引并存储在colormap2label中
    for i, colormap in enumerate(VOC_COLORMAP):
        colormap2label[
            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
    return colormap2label

#@save
def voc_label_indices(colormap, colormap2label):
    """将VOC标签中的RGB值映射到它们的类别索引"""
    # 将PyTorch张量转换为NumPy数组，并将通道维度从(H, W, C)转换为(C, H, W)
    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')
    # 计算每个像素的RGB值对应的索引
    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256
           + colormap[:, :, 2])
    # 使用colormap2label将RGB值映射到VOC类别索引并返回结果
    return colormap2label[idx]
```

[**例如**]，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0。

```python
y = voc_label_indices(train_labels[0], voc_colormap2label())
# 计算训练标签图像train_labels[0]中像素的类别索引
# train_labels是训练集中的标签数据，voc_label_indices函数将RGB值映射为VOC类别索引
# 返回的y是一个与train_labels[0]形状相同的数组，其中每个元素是对应像素的类别索引

# 提取y中105到114行、130到139列区域的像素的类别索引
region_indices = y[105:115, 130:140]
# VOC_CLASSES是一个定义了VOC数据集中各类别名称的列表，索引1对应的是类别标签1的名称

region_indices, VOC_CLASSES[1]
# 输出提取区域的类别索引region_indices和类别标签1对应的名称
```

### 预处理数据

在之前的实验，我们通过再缩放图像使其符合模型的输入形状。然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射可能不够精确，尤其在不同语义的分割区域。为了避免这个问题，我们将图像裁剪为固定尺寸，而不是再缩放。具体来说，我们[**使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域**]。

```python
#@save
def voc_rand_crop(feature, label, height, width):
    """随机裁剪特征和标签图像"""
    rect = torchvision.transforms.RandomCrop.get_params(
        feature, (height, width))  # 获取随机裁剪的参数
    feature = torchvision.transforms.functional.crop(feature, *rect)  # 对特征图像进行裁剪
    label = torchvision.transforms.functional.crop(label, *rect)  # 对标签图像进行裁剪
    return feature, label  # 返回裁剪后的特征和标签图像
```

```python
imgs = []
for _ in range(n):
    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)  # 调用随机裁剪函数，裁剪训练特征和标签图像

imgs = [img.permute(1, 2, 0) for img in imgs]  # 调整图像张量维度，从（C, H, W）到（H, W, C）
d2l.show_images(imgs[::2] + imgs[1::2], 2, n);  # 使用d2l库展示裁剪后的图像
```

### **自定义语义分割数据集类**

我们通过继承高级API提供的`Dataset`类，自定义了一个语义分割数据集类`VOCSegDataset`。通过实现`__getitem__`函数，我们可以任意访问数据集中索引为`idx`的输入图像及其每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的`filter`函数移除掉。此外，我们还定义了`normalize_image`函数，从而对输入图像的RGB三个通道的值分别做标准化。

```python
#@save
class VOCSegDataset(torch.utils.data.Dataset):
    """一个用于加载VOC数据集的自定义数据集"""

    def __init__(self, is_train, crop_size, voc_dir):
        self.transform = torchvision.transforms.Normalize(
            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 定义图像转换为张量的均值和标准差
        self.crop_size = crop_size  # 初始化裁剪尺寸
        features, labels = read_voc_images(voc_dir, is_train=is_train)  # 读取VOC数据集的图像和标签
        self.features = [self.normalize_image(feature)
                         for feature in self.filter(features)]  # 对特征图像进行标准化和筛选
        self.labels = self.filter(labels)  # 对标签图像进行筛选
        self.colormap2label = voc_colormap2label()  # 获取VOC数据集的颜色映射到标签的字典
        print('read ' + str(len(self.features)) + ' examples')  # 打印读取的示例数目

    def normalize_image(self, img):
        return self.transform(img.float() / 255)  # 归一化图像

    def filter(self, imgs):
        return [img for img in imgs if (
            img.shape[1] >= self.crop_size[0] and
            img.shape[2] >= self.crop_size[1])]  # 筛选尺寸大于等于裁剪尺寸的图像

    def __getitem__(self, idx):
        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],
                                       *self.crop_size)  # 随机裁剪特征和标签图像
        return (feature, voc_label_indices(label, self.colormap2label))  # 返回裁剪后的特征和标签索引

    def __len__(self):
        return len(self.features)  # 返回数据集中的样本数目
```

### **读取数据集**

我们通过自定义的`VOCSegDataset`类来分别创建训练集和测试集的实例。假设我们指定随机裁剪的输出图像的形状为$320\times 480$，下面我们可以查看训练集和测试集所保留的样本个数。

```python
crop_size = (320, 480)  # 设置裁剪尺寸为 (320, 480)

# 创建训练数据集实例
voc_train = VOCSegDataset(True, crop_size, voc_dir)

# 创建测试数据集实例
voc_test = VOCSegDataset(False, crop_size, voc_dir)
```

设批量大小为64，我们定义训练集的迭代器。打印第一个小批量的形状会发现：与图像分类或目标检测不同，这里的标签是一个三维数组。

```python
batch_size = 64  # 设置批量大小为64

train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,  # 创建一个数据加载器，使用voc_train数据集，设置批量大小为batch_size，打乱数据顺序
                                    drop_last=True,  # 如果数据集大小不能被批次大小整除，丢弃最后一个不完整的批次
                                    num_workers=d2l.get_dataloader_workers())  # 设置用于数据加载的子进程数量，加快数据加载速度


# 迭代数据加载器，打印每个批次的特征（X）和标签（Y）的形状
for X, Y in train_iter:
    print(X.shape)  # 打印特征（X）的形状
    print(Y.shape)  # 打印标签（Y）的形状
    break  # 仅打印第一个批次，然后退出循环
```

### **整合所有组件**

最后，我们定义以下`load_data_voc`函数来下载并读取Pascal VOC2012语义分割数据集。它返回训练集和测试集的数据迭代器。

```python
#@save
def load_data_voc(batch_size, crop_size):
    """加载VOC语义分割数据集"""
    voc_dir = d2l.download_extract('voc2012', os.path.join(
        'VOCdevkit', 'VOC2012'))  # 下载并解压VOC2012数据集到voc_dir目录
    num_workers = d2l.get_dataloader_workers()  # 获取数据加载器的子进程数量
    train_iter = torch.utils.data.DataLoader(
        VOCSegDataset(True, crop_size, voc_dir), batch_size,
        shuffle=True, drop_last=True, num_workers=num_workers)  # 创建训练数据加载器，用于训练集
    test_iter = torch.utils.data.DataLoader(
        VOCSegDataset(False, crop_size, voc_dir), batch_size,
        drop_last=True, num_workers=num_workers)  # 创建测试数据加载器，用于测试集
    return train_iter, test_iter  # 返回训练和测试数据加载器
```

# 转置卷积

到目前为止，我们所见到的卷积神经网络层，例如卷积层和汇聚层，通常会减少下采样输入图像的空间维度（高和宽）。然而如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将会很方便。例如，输出像素所处的通道维可以保有输入像素在同一位置上的分类结果。

为了实现这一点，尤其是在空间维度被卷积神经网络层缩小后，我们可以使用另一种类型的卷积神经网络层，它可以增加上采样中间层特征图的空间维度。
本节将介绍*转置卷积*（transposed convolution）用于逆转下采样导致的空间尺寸减小。

```python
import torch
from torch import nn
from d2l import torch as d2l
```

## 基本操作
我们可以对输入矩阵`X`和卷积核矩阵`K`(**实现基本的转置卷积运算**)`trans_conv`。

```python
def trans_conv(X, K):
    h, w = K.shape  # 获取卷积核的高度和宽度
    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))  # 创建输出的零张量，大小为输入的大小加上卷积核的大小减去1
    for i in range(X.shape[0]):  # 遍历输入的行
        for j in range(X.shape[1]):  # 遍历输入的列
            Y[i: i + h, j: j + w] += X[i, j] * K  # 在输出的相应位置添加卷积运算的结果
    return Y  # 返回卷积结果的张量
```

与通过卷积核“减少”输入元素的常规卷积相比，转置卷积通过卷积核“广播”输入元素，从而产生大于输入的输出。我们可以通过 构建输入张量`X`和卷积核张量`K`从而[**验证上述实现输出**]。此实现是基本的二维转置卷积运算。

```python
X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])  # 定义输入张量X，大小为2x2
K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])  # 定义卷积核张量K，大小为2x2
trans_conv(X, K)  # 调用trans_conv函数进行转置卷积计算
```

或者，当输入`X`和卷积核`K`都是四维张量时，我们可以[**使用高级API获得相同的结果**]。

```python
X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)  # 将输入张量X和卷积核张量K reshape为形状为(1, 1, 2, 2)的张量
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)  # 创建一个2x2的反卷积层，无偏置
tconv.weight.data = K  # 将反卷积层的权重设置为卷积核K的权重
tconv(X)  # 对输入张量X进行反卷积操作
```

## **填充、步幅和多通道**

与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。

```python
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)  # 创建一个2x2的反卷积层，填充为1，无偏置
tconv.weight.data = K  # 将反卷积层的权重设置为卷积核K的权重
tconv(X)  # 对输入张量X进行反卷积操作
```

在转置卷积中，步幅被指定为中间结果（输出），而不是输入。以下代码可以验证步幅为2的转置卷积的输出。

```python
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)  # 创建一个2x2的反卷积层，步长为2，无偏置
tconv.weight.data = K  # 将反卷积层的权重设置为卷积核K的权重
tconv(X)  # 对输入张量X进行反卷积操作
```

对于多个输入和输出通道，转置卷积与常规卷积以相同方式运作。假设输入有$c_i$个通道，且转置卷积为每个输入通道分配了一个$k_h\times k_w$的卷积核张量。当指定多个输出通道时，每个输出通道将有一个$c_i\times k_h\times k_w$的卷积核。

同样，如果我们将$\mathsf{X}$代入卷积层$f$来输出$\mathsf{Y}=f(\mathsf{X})$，并创建一个与$f$具有相同的超参数、但输出通道数量是$\mathsf{X}$中通道数的转置卷积层$g$，那么$g(Y)$的形状将与$\mathsf{X}$相同。下面的示例可以解释这一点。

```python
X = torch.rand(size=(1, 10, 16, 16))  # 创建一个大小为(1, 10, 16, 16)的随机张量X
conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)  # 创建一个2D卷积层，输入通道数为10，输出通道数为20，卷积核大小为5x5，填充为2，步长为3
tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)  # 创建一个2D反卷积层，输入通道数为20，输出通道数为10，卷积核大小为5x5，填充为2，步长为3
output_shape = tconv(conv(X)).shape  # 对X先进行卷积conv，然后将结果作为tconv的输入进行反卷积，获取输出张量的形状
output_shape == X.shape  # 检查输出张量的形状是否与X的形状相同
```

## **与矩阵变换的联系**

转置卷积为何以矩阵变换命名呢？让我们首先看看如何使用矩阵乘法来实现卷积。在下面的示例中，我们定义了一个$3\times 3$的输入`X`和$2\times 2$卷积核`K`，然后使用`corr2d`函数计算卷积输出`Y`。

```python
X = torch.arange(9.0).reshape(3, 3)  # 创建一个3x3的Tensor X，包含从0到8的连续数列
K = torch.tensor([[1.0, 2.0], [3.0, 4.0]])  # 创建一个2x2的卷积核K
Y = d2l.corr2d(X, K)  # 对输入张量X使用卷积核K进行二维相关运算，得到输出张量Y
Y  # 输出结果Y
```

接下来，我们将卷积核`K`重写为包含大量0的稀疏权重矩阵`W`。权重矩阵的形状是（$4$，$9$），其中非0元素来自卷积核`K`。

```python
def kernel2matrix(K):
    k, W = torch.zeros(5), torch.zeros((4, 9))
    k[:2], k[3:5] = K[0, :], K[1, :]  # 从卷积核K的第一行和第二行分别取前两个和后两个元素，赋值给k
    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k  # 将k的不同部分复制到输出矩阵W的不同行和列
    return W

W = kernel2matrix(K)  # 调用函数kernel2matrix，将卷积核K转换为矩阵W
W  # 输出结果W
```

逐行连结输入`X`，获得了一个长度为9的矢量。然后，`W`的矩阵乘法和向量化的`X`给出了一个长度为4的向量。重塑它之后，可以获得与上面的原始卷积操作所得相同的结果`Y`：我们刚刚使用矩阵乘法实现了卷积。

```python
Y == torch.matmul(W, X.reshape(-1)).reshape(2, 2)  # 检查是否矩阵Y与通过W和X的扁平化版本计算的乘积相等
```

同样，我们可以使用矩阵乘法来实现转置卷积。在下面的示例中，我们将上面的常规卷积$2 \times 2$的输出`Y`作为转置卷积的输入。想要通过矩阵相乘来实现它，我们只需要将权重矩阵`W`的形状转置为$(9, 4)$。

```python
Z = trans_conv(Y, K)  # 使用转置卷积函数trans_conv对Y和卷积核K进行操作，得到输出张量Z
Z == torch.matmul(W.T, Y.reshape(-1)).reshape(3, 3)  # 检查是否矩阵Z与通过W的转置乘以Y的扁平化版本计算的乘积相等
```

# 全卷积网络

语义分割是对图像中的每个像素分类。*全卷积网络*（fully convolutional network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换 与我们之前在图像分类或目标检测部分介绍的卷积神经网络不同，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过*转置卷积*（transposed convolution）实现的。因此，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测。

```python
%matplotlib inline
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l
```

## 构造模型
下面我们了解一下全卷积网络模型最基本的设计。
全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\times 1$卷积层将通道数变换为类别个数，最后在通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。因此，模型输出与输入图像的高和宽相同，且最终输出通道包含了该空间位置像素的类别预测。

下面，我们[**使用在ImageNet数据集上预训练的ResNet-18模型来提取图像特征**]，并将该网络记为`pretrained_net`。ResNet-18模型的最后几层包括全局平均汇聚层和全连接层，然而全卷积网络中不需要它们。

```python
pretrained_net = torchvision.models.resnet18(pretrained=True)  # 加载预训练的ResNet-18模型
list(pretrained_net.children())[-3:]  # 获取模型的最后三层子模块的列表
```

接下来，我们[**创建一个全卷积网络`net`**]。它复制了ResNet-18中大部分的预训练层，除了最后的全局平均汇聚层和最接近输出的全连接层。

```python
net = nn.Sequential(*list(pretrained_net.children())[:-2])  # 创建一个新的神经网络模型net，通过从预训练的ResNet-18模型中移除最后两层来定义
```

给定高度为320和宽度为480的输入，`net`的前向传播将输入的高和宽减小至原来的$1/32$，即10和15。

```python
X = torch.rand(size=(1, 3, 320, 480))    # 创建一个大小为(1, 3, 320, 480)的随机张量X，表示输入图像
net(X).shape   # 使用创建的神经网络模型net对X进行前向传播，并获取输出张量的形状
```

接下来[**使用$1\times1$卷积层将输出通道数转换为Pascal VOC2012数据集的类数（21类）。**]最后需要(**将特征图的高度和宽度增加32倍**)，从而将其变回输入图像的高和宽。回想一下卷积层输出形状的计算方法：由于$(320-64+16\times2+32)/32=10$且$(480-64+16\times2+32)/32=15$，我们构造一个步幅为$32$的转置卷积层，并将卷积核的高和宽设为$64$，填充为$16$。我们可以看到如果步幅为$s$，填充为$s/2$（假设$s/2$是整数）且卷积核的高和宽为$2s$，转置卷积核会将输入的高和宽分别放大$s$倍。

```python
num_classes = 21  # 设置类别数为21，用于最终的卷积和转置卷积层的输出通道数设定
net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))   # 添加一个1x1卷积层作为最终卷积层，输入通道数为512，输出通道数为num_classes

# 添加一个转置卷积层，用于上采样，输入和输出通道数都为num_classes，核大小为64x64，填充为16，步长为32
net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,
                                    kernel_size=64, padding=16, stride=32))
```

## **初始化转置卷积层**

在图像处理中，我们有时需要将图像放大，即*上采样*（upsampling）。*双线性插值*（bilinear interpolation）是常用的上采样方法之一，它也经常用于初始化转置卷积层。

双线性插值的上采样可以通过转置卷积层实现，内核由以下`bilinear_kernel`函数构造。限于篇幅，我们只给出`bilinear_kernel`函数的实现，不讨论算法的原理。

```python
def bilinear_kernel(in_channels, out_channels, kernel_size):
    # 计算中心点位置和卷积核的权重
    factor = (kernel_size + 1) // 2
    if kernel_size % 2 == 1:
        center = factor - 1
    else:
        center = factor - 0.5
    
    # 创建原始的权重矩阵
    og = (torch.arange(kernel_size).reshape(-1, 1),
          torch.arange(kernel_size).reshape(1, -1))
    
    # 计算双线性插值核
    filt = (1 - torch.abs(og[0] - center) / factor) * \
           (1 - torch.abs(og[1] - center) / factor)
    
    # 创建权重张量
    weight = torch.zeros((in_channels, out_channels,
                          kernel_size, kernel_size))
    
    # 将双线性插值核应用于权重张量的对角线元素
    weight[range(in_channels), range(out_channels), :, :] = filt
    
    return weight
```

让我们用[**双线性插值的上采样实验**]它由转置卷积层实现。我们构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用`bilinear_kernel`函数初始化。

```python
conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,
                                bias=False)  # 创建一个3通道到3通道的转置卷积层，核大小为4x4，填充为1，步长为2，无偏置

# 将双线性插值核赋值给转置卷积层的权重
conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4));
```

读取图像`X`，将上采样的结果记作`Y`。为了打印图像，我们需要调整通道维的位置。

```python
img = torchvision.transforms.ToTensor()(d2l.Image.open('../img/catdog.jpg'))   # 加载图像并转换为张量
X = img.unsqueeze(0)   # 增加批处理维度
Y = conv_trans(X)   # 使用转置卷积层进行图像上采样
out_img = Y[0].permute(1, 2, 0).detach()   # 将输出张量转换为图像格式，并从计算图中分离出来
```

可以看到，转置卷积层将图像的高和宽分别放大了2倍。除了坐标刻度不同，双线性插值放大的图像打印出的原图看上去没什么两样。

```python
d2l.set_figsize()  # 设置图形尺寸
print('input image shape:', img.permute(1, 2, 0).shape)  # 打印输入图像的形状
d2l.plt.imshow(img.permute(1, 2, 0));  # 显示输入图像
print('output image shape:', out_img.shape)  # 打印输出图像的形状
d2l.plt.imshow(out_img);  # 显示输出图像
```

全卷积网络[**用双线性插值的上采样初始化转置卷积层。对于$1\times 1$卷积层，我们使用Xavier初始化参数。**]

```python
W = bilinear_kernel(num_classes, num_classes, 64)  # 使用 bilinear_kernel 函数生成一个大小为 num_classes x num_classes x 64 的张量 W
net.transpose_conv.weight.data.copy_(W);  # 将张量 W 复制到网络 net 的转置卷积层的权重数据中
```

## **读取数据集**

我们用语义分割读取数据集。指定随机裁剪的输出图像的形状为$320\times 480$：高和宽都可以被$32$整除。

```python
batch_size, crop_size = 32, (320, 480)  # 定义批量大小和裁剪尺寸
train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)  # 使用 d2l.load_data_voc 函数加载训练集和测试集迭代器
```

## **训练**

现在我们可以训练全卷积网络了。这里的损失函数和准确率计算与图像分类中的并没有本质上的不同，因为我们使用转置卷积层的通道来预测像素的类别，所以需要在损失计算中指定通道维。此外，模型基于每个像素的预测类别是否正确来计算准确率。

```python
def loss(inputs, targets):
    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)  # 计算输入和目标之间的交叉熵损失

num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()  # 设置训练的 epochs 数量、学习率、权重衰减和设备列表
trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)  # 使用 SGD 优化器来训练网络的参数
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)  # 调用 d2l.train_ch13 函数进行训练
```

## **预测**

在预测时，我们需要将输入图像在各个通道做标准化，并转成卷积神经网络所需要的四维输入格式。

```python
def predict(img):
    X = test_iter.dataset.normalize_image(img).unsqueeze(0)  # 对输入图像进行归一化并增加批量维度
    pred = net(X.to(devices[0])).argmax(dim=1)  # 使用网络 net 进行预测，并取预测结果的最大值所在的索引
    return pred.reshape(pred.shape[1], pred.shape[2])  # 将预测结果重新形状为与输入图像尺寸相同的形状
```

为了[**可视化预测的类别**]给每个像素，我们将预测类别映射回它们在数据集中的标注颜色。

```python
def label2image(pred):
    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])  # 将 VOC_COLORMAP 转换为 PyTorch 张量，并移动到指定设备上
    X = pred.long()  # 将预测结果转换为长整型
    return colormap[X, :]  # 使用 colormap 将预测结果转换为对应的颜色值
```

测试数据集中的图像大小和形状各异。由于模型使用了步幅为32的转置卷积层，因此当输入图像的高或宽无法被32整除时，转置卷积层输出的高或宽会与输入图像的尺寸有偏差。为了解决这个问题，我们可以在图像中截取多块高和宽为32的整数倍的矩形区域，并分别对这些区域中的像素做前向传播。请注意，这些区域的并集需要完整覆盖输入图像。当一个像素被多个区域所覆盖时，它在不同区域前向传播中转置卷积层输出的平均值可以作为`softmax`运算的输入，从而预测类别。

为简单起见，我们只读取几张较大的测试图像，并从图像的左上角开始截取形状为$320\times480$的区域用于预测。对于这些测试图像，我们逐一打印它们截取的区域，再打印预测结果，最后打印标注的类别。

```python
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')  # 下载并解压 VOC2012 数据集
test_images, test_labels = d2l.read_voc_images(voc_dir, False)  # 读取 VOC2012 数据集中的测试图像和标签

n, imgs = 4, []  # 设置展示的图像数量和图像列表
for i in range(n):
    crop_rect = (0, 0, 320, 480)  # 设置裁剪的区域大小 (left, top, width, height)
    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)  # 对测试图像进行裁剪
    pred = label2image(predict(X))  # 使用预定义的 predict 函数进行图像分割预测，并将预测结果转换为颜色图像
    imgs += [X.permute(1, 2, 0), pred.cpu(),  # 将裁剪后的原始图像和预测结果添加到图像列表中
             torchvision.transforms.functional.crop(
                 test_labels[i], *crop_rect).permute(1, 2, 0)]  # 将对应的裁剪后的标签图像添加到图像列表中

# 使用 d2l.show_images 函数展示图像列表中的图像
d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2)
```

# 风格迁移
本节将介绍如何使用卷积神经网络，自动将一个图像中的风格应用在另一图像之上，即*风格迁移*（style transfer）。这里我们需要两张输入图像：一张是*内容图像*，另一张是*风格图像*。我们将使用神经网络修改内容图像，使其在风格上接近风格图像。
## 方法
风格迁移常用的损失函数由3部分组成：

1. *内容损失*使合成图像与内容图像在内容特征上接近；
1. *风格损失*使合成图像与风格图像在风格特征上接近；
1. *全变分损失*则有助于减少合成图像中的噪点。

最后，当模型训练结束时，我们输出风格迁移的模型参数，即得到最终的合成图像。
## [**阅读内容和风格图像**]

首先，我们读取内容和风格图像。从打印出的图像坐标轴可以看出，它们的尺寸并不一样。

```python
%matplotlib inline
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

d2l.set_figsize()  # 设置图像显示尺寸
content_img = d2l.Image.open('../img/rainier.jpg')  # 打开并显示指定路径下的内容图像
d2l.plt.imshow(content_img);  # 使用 matplotlib 显示内容图像
```

```python
style_img = d2l.Image.open('../img/autumn-oak.jpg')  # 打开风格图像
d2l.plt.imshow(style_img);  # 显示风格图像
```

## [**预处理和后处理**]

下面，定义图像的预处理函数和后处理函数。预处理函数`preprocess`对输入图像在RGB三个通道分别做标准化，并将结果变换成卷积神经网络接受的输入格式。后处理函数`postprocess`则将输出图像中的像素值还原回标准化之前的值。由于图像打印函数要求每个像素的浮点数值在0～1之间，我们对小于0和大于1的值分别取0和1。

```python
rgb_mean = torch.tensor([0.485, 0.456, 0.406])   # RGB 均值
rgb_std = torch.tensor([0.229, 0.224, 0.225])   # RGB 标准差

# 图像预处理函数
def preprocess(img, image_shape):
    transforms = torchvision.transforms.Compose([
        torchvision.transforms.Resize(image_shape),  # 调整图像大小
        torchvision.transforms.ToTensor(),  # 将图像转换为Tensor
        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])  # 标准化图像像素值
    return transforms(img).unsqueeze(0)  # 对图像应用预处理转换，并在0维度上增加一个维度

# 图像后处理函数
def postprocess(img):
    img = img[0].to(rgb_std.device)  # 将图像Tensor移到与标准差张量相同的设备上
    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)  # 反归一化图像像素值
    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))  # 将Tensor转换回PIL Image格式
```

## [**抽取图像特征**]

我们使用基于ImageNet数据集预训练的VGG-19模型来抽取图像特征 :cite:`Gatys.Ecker.Bethge.2016`。

```python
pretrained_net = torchvision.models.vgg19(pretrained=True)  # 加载预训练的VGG19模型
```

为了抽取图像的内容特征和风格特征，我们可以选择VGG网络中某些层的输出。一般来说，越靠近输入层，越容易抽取图像的细节信息；反之，则越容易抽取图像的全局信息。为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层，即*内容层*，来输出图像的内容特征。我们还从VGG中选择不同层的输出来匹配局部和全局的风格，这些图层也称为*风格层*。正如所介绍的，VGG网络使用了5个卷积块。实验中，我们选择第四卷积块的最后一个卷积层作为内容层，选择每个卷积块的第一个卷积层作为风格层。这些层的索引可以通过打印`pretrained_net`实例获取。

```python
style_layers, content_layers = [0, 5, 10, 19, 28], [25]  # 定义风格层和内容层的索引
```

使用VGG层抽取特征时，我们只需要用到从输入层到最靠近输出层的内容层或风格层之间的所有层。下面构建一个新的网络`net`，它只保留需要用到的VGG的所有层。

```python
net = nn.Sequential(*[pretrained_net.features[i] for i in
                      range(max(content_layers + style_layers) + 1)])  # 构建包含指定层的神经网络模型
```

给定输入`X`，如果我们简单地调用前向传播`net(X)`，只能获得最后一层的输出。由于我们还需要中间层的输出，因此这里我们逐层计算，并保留内容层和风格层的输出。

```python
def extract_features(X, content_layers, style_layers):
    contents = []  # 存储内容特征
    styles = []  # 存储风格特征
    for i in range(len(net)):
        X = net[i](X)  # 对输入X应用神经网络中的第i层
        if i in style_layers:
            styles.append(X)  # 如果是风格层的索引，将该层输出添加到styles中
        if i in content_layers:
            contents.append(X)  # 如果是内容层的索引，将该层输出添加到contents中
    return contents, styles  # 返回提取的内容特征和风格特征
```

下面定义两个函数：`get_contents`函数对内容图像抽取内容特征；`get_styles`函数对风格图像抽取风格特征。因为在训练时无须改变预训练的VGG的模型参数，所以我们可以在训练开始之前就提取出内容特征和风格特征。由于合成图像是风格迁移所需迭代的模型参数，我们只能在训练过程中通过调用`extract_features`函数来抽取合成图像的内容特征和风格特征。

```python
def get_contents(image_shape, device):
    content_X = preprocess(content_img, image_shape).to(device)  # 预处理内容图像并移动到指定设备
    contents_Y, _ = extract_features(content_X, content_layers, style_layers)  # 提取内容特征
    return content_X, contents_Y  # 返回处理后的内容图像和内容特征列表

def get_styles(image_shape, device):
    style_X = preprocess(style_img, image_shape).to(device)  # 预处理风格图像并移动到指定设备
    _, styles_Y = extract_features(style_X, content_layers, style_layers)  # 提取风格特征
    return style_X, styles_Y  # 返回处理后的风格图像和风格特征列表
```

## [**定义损失函数**]

下面我们来描述风格迁移的损失函数。它由内容损失、风格损失和全变分损失3部分组成。
### 内容损失
与线性回归中的损失函数类似，内容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。平方误差函数的两个输入均为`extract_features`函数计算所得到的内容层的输出

```python
def content_loss(Y_hat, Y):
    # 我们从动态计算梯度的树中分离目标：
    # 这是一个规定的值，而不是一个变量。
    return torch.square(Y_hat - Y.detach()).mean()   # 计算内容损失，使用均方误差，Y.detach()用于断开梯度流
```

### 风格损失

风格损失也通过平方误差函数衡量合成图像与风格图像在风格上的差异。为了表达风格层输出的风格，我们先通过`extract_features`函数计算风格层的输出。假设该输出的样本数为1，通道数为$c$，高和宽分别为$h$和$w$，我们可以将此输出转换为矩阵$\mathbf{X}$，其有$c$行和$hw$列。这个矩阵可以被看作由$c$个长度为$hw$的向量$\mathbf{x}_1, \ldots, \mathbf{x}_c$组合而成的。其中向量$\mathbf{x}_i$代表了通道$i$上的风格特征。

在这些向量的*格拉姆矩阵*$\mathbf{X}\mathbf{X}^\top \in \mathbb{R}^{c \times c}$中，$i$行$j$列的元素$x_{ij}$即向量$\mathbf{x}_i$和$\mathbf{x}_j$的内积。它表达了通道$i$和通道$j$上风格特征的相关性。我们用这样的格拉姆矩阵来表达风格层输出的风格。需要注意的是，当$hw$的值较大时，格拉姆矩阵中的元素容易出现较大的值。此外，格拉姆矩阵的高和宽皆为通道数$c$。为了让风格损失不受这些值的大小影响，下面定义的`gram`函数将格拉姆矩阵除以了矩阵中元素的个数，即$chw$。

```python
def gram(X):
    num_channels, n = X.shape[1], X.numel() // X.shape[1]
    X = X.reshape((num_channels, n))  # 重塑张量形状为 (num_channels, n)，其中 n = height * width
    return torch.matmul(X, X.T) / (num_channels * n)  # 计算 Gram 矩阵，使用公式 X * X^T / (num_channels * n) 进行归一化
```

自然地，风格损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与风格图像的风格层输出。这里假设基于风格图像的格拉姆矩阵`gram_Y`已经预先计算好了。

```python
def style_loss(Y_hat, gram_Y):
    gram_Y_hat = gram(Y_hat)   # 计算预测图像的 Gram 矩阵
    return torch.square(gram_Y_hat - gram_Y.detach()).mean()   # 计算风格损失，使用均方误差，gram_Y.detach() 用于断开梯度流
```

### 全变分损失

有时候，我们学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。一种常见的去噪方法是*全变分去噪*（total variation denoising）：假设$x_{i, j}$表示坐标$(i, j)$处的像素值，降低全变分损失
$$\sum_{i, j} \left|x_{i, j} - x_{i+1, j}\right| + \left|x_{i, j} - x_{i, j+1}\right|$$
能够尽可能使邻近的像素值相似。

```python
def tv_loss(Y_hat):
    # 计算总变差损失，即图像 Y_hat 在水平和垂直方向上相邻像素之间的绝对差的均值
    return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +
                  torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())
```

### 损失函数

[**风格转移的损失函数是内容损失、风格损失和总变化损失的加权和**]。通过调节这些权重超参数，我们可以权衡合成图像在保留内容、迁移风格以及去噪三方面的相对重要性。

```python
content_weight, style_weight, tv_weight = 1, 1e3, 10   # 内容权重、风格权重、总变化损失权重

def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):
    # 分别计算内容损失、风格损失和全变分损失
    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(
        contents_Y_hat, contents_Y)]  # 计算内容损失
    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(
        styles_Y_hat, styles_Y_gram)]  # 计算风格损失
    tv_l = tv_loss(X) * tv_weight  # 计算全变分损失
    # 对所有损失求和
    l = sum(10 * styles_l + contents_l + [tv_l])  # 总损失为风格损失和内容损失加权和再加全变分损失
    return contents_l, styles_l, tv_l, l
```

## [**初始化合成图像**]

在风格迁移中，合成的图像是训练期间唯一需要更新的变量。因此，我们可以定义一个简单的模型`SynthesizedImage`，并将合成的图像视为模型参数。模型的前向传播只需返回模型参数即可。

```python
class SynthesizedImage(nn.Module):
    def __init__(self, img_shape, **kwargs):
        super(SynthesizedImage, self).__init__(**kwargs)
        self.weight = nn.Parameter(torch.rand(*img_shape))  # 初始化权重参数，随机生成与图像形状相同的张量

    def forward(self):
        return self.weight  # 返回权重参数作为合成图像
```

下面，我们定义`get_inits`函数。该函数创建了合成图像的模型实例，并将其初始化为图像`X`。风格图像在各个风格层的格拉姆矩阵`styles_Y_gram`将在训练前预先计算好。

```python
def get_inits(X, device, lr, styles_Y):
    gen_img = SynthesizedImage(X.shape).to(device)  # 创建合成图像对象，并将其移动到指定设备
    gen_img.weight.data.copy_(X.data)  # 将输入数据复制到合成图像的权重中
    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)  # 使用Adam优化器来训练合成图像的参数，学习率为lr
    styles_Y_gram = [gram(Y) for Y in styles_Y]  # 计算每个风格图像的格拉姆矩阵
    return gen_img(), styles_Y_gram, trainer  # 返回合成图像对象、风格图像的格拉姆矩阵列表和训练器对象
```

## [**训练模型**]

在训练模型进行风格迁移时，我们不断抽取合成图像的内容特征和风格特征，然后计算损失函数。下面定义了训练循环。

```python
def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):
    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)  # 初始化合成图像、风格图像的格拉姆矩阵和训练器
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)  # 设置学习率衰减策略
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',  # 创建动画对象用于可视化训练过程
                            xlim=[10, num_epochs],
                            legend=['content', 'style', 'TV'],
                            ncols=2, figsize=(7, 2.5))
    for epoch in range(num_epochs):
        trainer.zero_grad()  # 梯度清零
        contents_Y_hat, styles_Y_hat = extract_features(
            X, content_layers, style_layers)  # 提取当前合成图像的内容特征和风格特征
        contents_l, styles_l, tv_l, l = compute_loss(
            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)  # 计算内容损失、风格损失、总变差损失和总损失
        l.backward()  # 反向传播计算梯度
        trainer.step()  # 更新参数
        scheduler.step()  # 更新学习率
        if (epoch + 1) % 10 == 0:
            animator.axes[1].imshow(postprocess(X))  # 在动画中显示处理后的合成图像
            animator.add(epoch + 1, [float(sum(contents_l)),  # 记录损失到动画对象中
                                     float(sum(styles_l)), float(tv_l)])
    return X  # 返回训练后的合成图像
```

现在我们[**训练模型**]：首先将内容图像和风格图像的高和宽分别调整为300和450像素，用内容图像来初始化合成图像。

```python
device, image_shape = d2l.try_gpu(), (300, 450)  # 尝试获取GPU设备并设置图像尺寸
net = net.to(device)  # 将网络模型移动到指定的设备上
content_X, contents_Y = get_contents(image_shape, device)  # 获取内容图像及其对应的内容特征
_, styles_Y = get_styles(image_shape, device)  # 获取风格图像及其对应的风格特征
output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)  # 训练函数，生成合成图像
```

我们可以看到，合成图像保留了内容图像的风景和物体，并同时迁移了风格图像的色彩。例如，合成图像具有与风格图像中一样的色彩块，其中一些甚至具有画笔笔触的细微纹理。

